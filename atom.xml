<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>江东的笔记</title>
  
  <subtitle>Be overcome difficulties is victory</subtitle>
  <link href="https://du2279664786.github.io/atom.xml" rel="self"/>
  
  <link href="https://du2279664786.github.io/"/>
  <updated>2023-06-26T02:22:13.772Z</updated>
  <id>https://du2279664786.github.io/</id>
  
  <author>
    <name>江东</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>大三下学期操作系统复习</title>
    <link href="https://du2279664786.github.io/posts/88687dc5.html"/>
    <id>https://du2279664786.github.io/posts/88687dc5.html</id>
    <published>2023-06-23T14:55:10.000Z</published>
    <updated>2023-06-26T02:22:13.772Z</updated>
    
    <content type="html"><![CDATA[<p>知识点总结</p><span id="more"></span><h1 id="选择题"><a href="#选择题" class="headerlink" title="选择题"></a>选择题</h1><p>一 </p><p>1 并发与并行：并发是一段时间间隔内多个程序运行，并行是指两个或多个程序或事件在同一时刻发生，单处理机系统只能做到并发</p><p>2 如果题目未告诉单处理机还是多处理机，则默认是单处理机，则只能并发</p><p>3 系统调用目的：为了让系统提供服务</p><p>4 单道批处理、分时、实时操作提醒各有什么特点</p><p>单道批处理：</p><p>分时操作系统：交互</p><p>实时操系统：可靠、及时、工业控制系统 </p><p>5 PCB块：程序控制块</p><ul><li>操作系统管理进程的块</li><li>操作系统负责感知进程存在的唯一标志</li></ul><p>操作系统用来管理进程的数据结构是PCB</p><p>6 进程的三种基本状态：阻塞，就绪，执行</p><p>阻塞：等待某个事件发生，如IO</p><p>就绪：万事俱备，只欠CPU</p><p>执行：正在执行</p><p>阻塞、就绪、执行这三种状态下：程序段、数据段都在内存，之后无状态的情况下才在外村</p><p>7 处于就绪状态的进程数目可以是0，总共有N个进程，最多有N-1个处于就绪状态</p><p>8 三状态图：阻塞、就绪、执行</p><p>9 进程通信</p><ul><li>共享存储器</li><li>消息传递系统</li><li>管道通信：<ul><li>管道通信传递文件的大小-不-受磁盘大小的控制</li><li>管道通信是单向的，当一个进程正在读写时，另一个进程必须等待</li></ul></li></ul><p>10 生产者与消费者：既有同步又有互斥</p><p>11 哲学家就餐问题：避免死锁</p><ul><li>只放四个凳子，拿到凳子才能就餐</li><li></li></ul><p>12 管程：管程名、变量   P121</p><p>13 线程：有了线程，进程扔可以调度</p><p>线程是基本调度的单位，进程是资源分配的单位</p><p>线程的切换不一定引起进程的切换。只有线程切换系统调用，才会引起进程切换</p><p>14 信号量初值为5，当前值是1，等待的进程有0个，可用的资源为1。若当前值为-1，则等待进程有1，可用的有0个</p><h3 id="14-综合应用题：处理机调度-10分——死锁31分"><a href="#14-综合应用题：处理机调度-10分——死锁31分" class="headerlink" title="14 综合应用题：处理机调度    10分——死锁31分"></a>14 综合应用题：处理机调度    10分——死锁31分</h3><p>15 死锁：</p><p>产生的必要条件：</p><ul><li>互斥条件：如果进程请求一个已经被其他进程占用的资源，请求进程必须等待直到该资源被释放</li><li>请求并保持</li><li>不可抢占条件</li><li>循环等待条件</li></ul><p>处理死锁：</p><ul><li>预防死锁</li><li>避免死锁</li><li>检测死锁</li><li>解除死锁</li></ul><p>16 银行家算法：不考</p><p>17 资源分配图：</p><ol><li>进程节点（Process Node）：用圆圈或方框表示，代表系统中的进程或线程。每个进程节点通常标有唯一的标识符。</li><li>资源节点（Resource Node）：用长方形或椭圆形表示，代表系统中的资源，例如磁盘、打印机、内存等。每个资源节点通常标有唯一的标识符。</li><li>请求边（Request Edge）：用箭头表示，从进程节点指向资源节点。表示进程请求获得该资源。</li><li>占有边（Hold Edge）：用实线表示，从进程节点指向资源节点。表示进程已经获得并占有该资源。</li></ol><p>18 根据资源分配图判断是否死锁：判断图是否是可简化的，将图简化为游离的结点</p><p>19 银行家算法的安全状态和不安全状态</p><p>20 IO设备：磁带是顺序存储设备，磁盘是随机存储设备。Spooling技术通常使用磁盘</p><p>21 IO系统的层次结构</p><p>用户层软件(最上层)–&gt;与设备无关的IO软件–&gt;设备驱动程序–&gt;硬件(最下层)</p><p>设备无关性软件把代码参数转化为命令</p><h1 id="简答题"><a href="#简答题" class="headerlink" title="简答题"></a>简答题</h1><p>一 第一章三个大问题：并发与并行的区别、系统调用、三个操作系统的作用</p><p>操作系统有什么特征？ 进程的特征决定了操作系统的特征：并发共享虚拟异步，并发共享互为前提条件，没有并发就做不到共享，没有共享就做不到并发，并发和共享决定了虚拟，</p><p>文件管理：Linux和Unix非常巧妙，普通的文件控制块包含文件名和文件描述信息，而在linux中的FCB只有文件名，把文件描述信息分离出去单独放在“索引节点”里面</p><p>这样做的优点：1 提高了文件检索速度， 只读入了文件名，减少了读入磁盘块的数目       2  便于文件共享，在索引结点中增加一个共享计数，共享计数为3，说明有三个共享(文件共享有两种方式：软共享和硬共享，都使用索引节点)</p><p>赵老师课件里面有死锁问题的解决</p><h1 id="计算题"><a href="#计算题" class="headerlink" title="计算题"></a>计算题</h1><p>一</p><p> 页式的逻辑地址专户为逻辑地址，”请求分页”，已知条件：1 页的大小      2 页表</p><p>页大小：4K，页表逻辑地址5555，首先整除5555&#x2F;4*1024&#x3D;5555&#x2F;4096，整数部分1(页号)，余数1459(偏移量)</p><p>若（）在外存：1 发出缺页中断。2 请求调页程序，如果内存够用，则直接调入内存，若内存不够用，则调出一页换页换入内存。3 根据调入内存的物理块号计算结果</p><p>二 磁盘调度</p><p>先来先服务，最短寻道优先，电梯调度</p><p>磁盘有199个磁道，则磁道号为：0-198</p><p>三           页面置换算法：</p><p>OPT FIFO LRU</p><p>画页面置换框图</p><p>看好问题中问了几问</p><p>如果题目告诉初始为空，则填满前都是缺页</p><h1 id="综合题"><a href="#综合题" class="headerlink" title="综合题"></a>综合题</h1><p>一        处理机调度</p><p>处理机调度：采用进程调度算法 一般计算周转时间、平均周转时间。。。。。先画表</p><p>周转时间&#x3D;结束时间-到达时间</p><p>平均周转时间&#x3D;平均周转时间</p><p>短进程优先的枪战</p><p>二 虚拟内存管理–综合题</p><p>默认的虚拟内存在C盘，画出一个空间当内存使</p><p>虚拟内存的大小理论上等于&#x3D;内存+外存(1 受地址结构的限制   2 受硬盘大小的影响，硬盘可用内存变小，则会影响虚存)</p><p>C盘内存变小，则电脑会变卡，虚拟内存不够</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;知识点总结&lt;/p&gt;</summary>
    
    
    
    <category term="期末复习" scheme="https://du2279664786.github.io/categories/%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/"/>
    
    
    <category term="操作系统" scheme="https://du2279664786.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>《Python自然语言处理入门与实践》作业</title>
    <link href="https://du2279664786.github.io/posts/dd750040.html"/>
    <id>https://du2279664786.github.io/posts/dd750040.html</id>
    <published>2023-06-21T14:55:10.000Z</published>
    <updated>2023-06-21T07:55:29.959Z</updated>
    
    <content type="html"><![CDATA[<p>5.17日作业：文本向量化，TF-IDF关键词提取，情感分类</p><span id="more"></span><h1 id="《Python自然语言处理入门与实践》作业"><a href="#《Python自然语言处理入门与实践》作业" class="headerlink" title="《Python自然语言处理入门与实践》作业"></a>《Python自然语言处理入门与实践》作业</h1><h2 id="作业一"><a href="#作业一" class="headerlink" title="作业一"></a>作业一</h2><p>1、将以下内容转为txt文本：</p><p>I could imagine his giving a friend a little pinch of the latest vegetable alkaloid,not out of malevolence,you understand,but simply out of a spirit of inquiry in order to have an accurate idea of the effects.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">content = <span class="string">&quot;I could imagine his giving a friend a little pinch of the latest vegetable alkaloid, not out of malevolence, you understand, but simply out of a spirit of inquiry in order to have an accurate idea of the effects.&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 将内容写入txt文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;output.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(content)</span><br></pre></td></tr></table></figure><p>2、对内容进行分词，统计文本中的所有词语，进行独热编码，得到每个词的one-hot向量表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">&#x27;I could imagine his giving a friend a little pinch of the latest vegetable alkaloid,not out of malevolence,you understand,but simply out of a spirit of inquiry in order to have an accurate idea of the effects.&#x27;</span></span><br><span class="line">lis = jieba.lcut(text)</span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line">label_encoded = label_encoder.fit_transform(lis)</span><br><span class="line">one_hot_encoder = OneHotEncoder()</span><br><span class="line">one_hot_encoded = one_hot_encoder.fit_transform(label_encoded.reshape(-<span class="number">1</span>, <span class="number">1</span>)).toarray()</span><br></pre></td></tr></table></figure><p>3、将句子进行文本向量化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># author:杜培博</span></span><br><span class="line"><span class="comment"># datetime:2023/5/17 20:00</span></span><br><span class="line"><span class="comment"># software: PyCharm</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line">model_name = <span class="string">&quot;bert-base-cased&quot;</span>  <span class="comment"># 请替换为您的模型名称</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">text = <span class="string">&quot;I could imagine his giving a friend a little pinch of the latest vegetable alkaloid,not out of malevolence,you understand,but simply out of a spirit of inquiry in order to have an accurate idea of the effects.&quot;</span></span><br><span class="line"><span class="comment"># 对文本进行分词并添加特殊标记(例如[CLS]和[SEP])</span></span><br><span class="line">input_ids = tokenizer.encode(text, add_special_tokens=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 将输入ID序列转换为BERT模型的输入</span></span><br><span class="line">input_tensor = torch.tensor([input_ids])</span><br><span class="line"><span class="comment"># 加载预训练的BERT模型</span></span><br><span class="line">model = BertModel.from_pretrained(model_name)</span><br><span class="line"><span class="comment"># 将输入序列传递给BERT模型并获取输出(1个句子的向量表示)</span></span><br><span class="line">outputs = model(input_tensor)</span><br><span class="line"><span class="comment"># 获取第一个输出(即整个句子的向量表示)</span></span><br><span class="line">last_hidden_state = outputs[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 将输出张量展平以获取单个句子向量</span></span><br><span class="line">sentence_vector = last_hidden_state.squeeze(<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sentence vector:&quot;</span>, sentence_vector)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="作业二"><a href="#作业二" class="headerlink" title="作业二"></a>作业二</h2><p><strong>使用TF-IDF算法完成以下内容实现关键词的提取：</strong></p><p>展望2035年，我国将基本实现社会主义现代化。经济实体、科技实力、综合国力将大幅跃升，经济总量和城乡居民人均收入将再迈上新的大台阶，关键核心技术实现重大突破，进入创新型国家前列。基本实现新型工业化、信息化、城镇化、农业现代化，建成现代化经济体系。基本实现国家治理体系和治理能力现代化，人民平等参与、平等发展，权利得到充分保障，基本建成法治国家、法治政府、法治社会。建成文化强国、教育强国、人才强国、体育强国、健康中国，国民素质和社会文明程序达到新高度，国家文化软实力显著增强。广泛形成绿色生产生活方式，碳排放达峰后稳中有降，生态环境根本好转，美丽中国建设目标基本实现。形成对外开放新格局，参与国际经济合作和竞争新优势明显增强。人均国内生产总值达到中等发达国家水平，中等收入群体显著扩大，基本公共服务实现均等化，城乡区域发展差距和居民生活水平差距显著缩小。平安中国建设达到更高水平，基本实现国防和军队现代化。人民生活更加美好，人的全面发展、全体人民共同富裕取得更为明显的实质性进展。经济发展取得新成效，发展是解决我国一切问题的基础和关键，发展必须坚持新民展理念，在质量效益明显提升的基础上实现经济持续健康发展，增长潜力充分发挥，国内生产总值年均增长保持在合理区间、各年度视情提出，全员劳动生产率增长高于国内生产总值增长，国内市场更加强大，经济结构更加优化，创新能力显著提升，全社会研发经费投入年增长7%以上、力争投入强度高于“十三五”时期实际，产业基础高级化、产业链现代化水平明显提高，农业基础更加稳固，城乡区域发展协调性明显增强，常住人中城镇化率提高到65%，现代化经济体系建设取得重大进展。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># -*- coding:utf-8 -*-</span><br><span class="line"># author:杜培博</span><br><span class="line"># datetime:2023/5/17 19:47</span><br><span class="line"># software: PyCharm</span><br><span class="line"></span><br><span class="line">import jieba</span><br><span class="line">from jieba import analyse</span><br><span class="line">import pandas as pd</span><br><span class="line">text = &#x27;展望2035年，我国将基本实现社会主义现代化。经济实体、科技实力、综合国力将大幅跃升，经济总量和城乡居民人均收入将再迈上新的大台阶，关键核心技术实现重大突破，进入创新型国家前列。基本实现新型工业化、信息化、城镇化、农业现代化，建成现代化经济体系。基本实现国家治理体系和治理能力现代化，人民平等参与、平等发展，权利得到充分保障，基本建成法治国家、法治政府、法治社会。建成文化强国、教育强国、人才强国、体育强国、健康中国，国民素质和社会文明程序达到新高度，国家文化软实力显著增强。广泛形成绿色生产生活方式，碳排放达峰后稳中有降，生态环境根本好转，美丽中国建设目标基本实现。形成对外开放新格局，参与国际经济合作和竞争新优势明显增强。人均国内生产总值达到中等发达国家水平，中等收入群体显著扩大，基本公共服务实现均等化，城乡区域发展差距和居民生活水平差距显著缩小。平安中国建设达到更高水平，基本实现国防和军队现代化。人民生活更加美好，人的全面发展、全体人民共同富裕取得更为明显的实质性进展。经济发展取得新成效，发展是解决我国一切问题的基础和关键，发展必须坚持新民展理念，在质量效益明显提升的基础上实现经济持续健康发展，增长潜力充分发挥，国内生产总值年均增长保持在合理区间、各年度视情提出，全员劳动生产率增长高于国内生产总值增长，国内市场更加强大，经济结构更加优化，创新能力显著提升，全社会研发经费投入年增长7%以上、力争投入强度高于“十三五”时期实际，产业基础高级化、产业链现代化水平明显提高，农业基础更加稳固，城乡区域发展协调性明显增强，常住人中城镇化率提高到65%，现代化经济体系建设取得重大进展。&#x27;</span><br><span class="line">lis = jieba.analyse.extract_tags(text, withWeight = True, topK=11)   # 要求返回权重值</span><br><span class="line"></span><br><span class="line">score = pd.DataFrame(columns=[&#x27;名词&#x27;,&#x27;重要性&#x27;])</span><br><span class="line"></span><br><span class="line">score[&#x27;名词&#x27;] = [i[0] for i in lis]</span><br><span class="line">score[&#x27;重要性&#x27;] = [i[1] for i in lis]</span><br><span class="line">score = score[1:]</span><br><span class="line"></span><br><span class="line">from matplotlib import pyplot as plt</span><br><span class="line">plt.rcParams[&#x27;font.sans-serif&#x27;] = [&#x27;SimHei&#x27;]  # 显示中文黑体</span><br><span class="line"># plt.rcParams[&#x27;axes.unicode_minus&#x27;] = False # 负值显示</span><br><span class="line">plt.barh(score[&#x27;名词&#x27;], score[&#x27;重要性&#x27;], height=0.7, color=&quot;c&quot;,hatch=&quot;/&quot;,edgecolor=&#x27;#005344&#x27;) # 更多颜色可参见颜色大全</span><br><span class="line"># plt.xlabel(&#x27;feature importance&#x27;) # x 轴</span><br><span class="line"># plt.ylabel(&#x27;features&#x27;) # y轴</span><br><span class="line">plt.title(&#x27;TFIDF权重&#x27;) # 标题</span><br><span class="line">for a,b in zip( score[&#x27;重要性&#x27;],score[&#x27;名词&#x27;]): # 添加数字标签</span><br><span class="line">   print(a,b)</span><br><span class="line">   plt.text(a+0.001, b,&#x27;%.3f&#x27;%float(a)) # a+0.001代表标签位置在柱形图上方0.001处</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="作业三"><a href="#作业三" class="headerlink" title="作业三"></a>作业三</h2><p>自定义函数sdwu_get_content()，分别读取data文件夹下的neg和pos文件夹中的文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># author:杜培博</span></span><br><span class="line"><span class="comment"># datetime:2023/5/17 20:24</span></span><br><span class="line"><span class="comment"># software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sdwu_get_content</span>(<span class="params">folder_name</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    读取指定文件夹中的文件内容并返回一个列表。</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">    folder_name (str): 文件夹名称。</span></span><br><span class="line"><span class="string">    返回值：</span></span><br><span class="line"><span class="string">    list: 包含文件夹中所有文件内容的列表。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    content = []</span><br><span class="line">    <span class="comment"># 遍历文件夹中的所有文件</span></span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(folder_name):</span><br><span class="line">        <span class="comment"># 如果文件是文本文件，则读取文件内容并添加到列表中</span></span><br><span class="line">        <span class="keyword">if</span> filename.endswith(<span class="string">&quot;.txt&quot;</span>):</span><br><span class="line">            file_path = os.path.join(folder_name, filename)</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                content.append(f.read())</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用自定义函数并打印结果</span></span><br><span class="line">neg_folder = <span class="string">&quot;data/neg&quot;</span></span><br><span class="line">pos_folder = <span class="string">&quot;data/pos&quot;</span></span><br><span class="line">neg_content = sdwu_get_content(neg_folder)</span><br><span class="line">pos_content = sdwu_get_content(pos_folder)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;neg_content:&quot;</span>, neg_content)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;pos_content:&quot;</span>, pos_content)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="作业四"><a href="#作业四" class="headerlink" title="作业四"></a>作业四</h2><p>考虑以下任务：给定一段英文文本，设计一个NLP应用程序，实现文本情感分析。程序应该能够分析文本中的情感并给出正面、负面或中性的情感标签。请描述你的解决方案的主要步骤，包括数据预处理、特征提取、模型选择和评估。</p><ol><li>数据预处理：<ul><li>对评论文本进行长度分析，确定合适的评论长度。</li><li>将数据集分割为训练集、验证集和测试集。</li><li>进行数据预处理操作，如文本清洗、分词等。</li></ul></li><li>模型选择和搭建：<ul><li>选择适用于英文情感分类的预训练模型，例如BERT、RoBERTa、GPT等。</li><li>在选择预训练模型时，确保模型经过情感分类任务的微调或在英文情感分类数据上进行预训练。</li><li>根据选定的预训练模型，在其之上添加适当的分类层。</li></ul></li><li>特征提取：<ul><li>使用预训练模型对文本进行编码，获取文本的表示向量。</li><li>可以选择只使用预训练模型的最后一层输出作为特征表示，或者结合多层输出进行特征提取。</li></ul></li><li>模型训练和评估：<ul><li>选择适当的优化器和损失函数，例如AdamW优化器和交叉熵损失函数。</li><li>使用训练集对模型进行训练，使用验证集评估模型的性能。</li><li>监控模型的准确率、损失值等指标，可视化训练过程。</li></ul></li><li>模型评估指标分析：<ul><li>根据验证集或测试集的真实标签和模型预测结果计算评估指标，如准确率、精确率、召回率和F1-score等。</li></ul></li><li>对测试数据进行预测：<ul><li>使用训练好的模型对测试数据进行预测，并获取预测结果。</li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;5.17日作业：文本向量化，TF-IDF关键词提取，情感分类&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Bert" scheme="https://du2279664786.github.io/tags/Bert/"/>
    
  </entry>
  
  <entry>
    <title>餐饮服务评价情感倾向分析</title>
    <link href="https://du2279664786.github.io/posts/c016d6a.html"/>
    <id>https://du2279664786.github.io/posts/c016d6a.html</id>
    <published>2023-03-08T14:55:10.000Z</published>
    <updated>2023-03-11T06:12:44.016Z</updated>
    
    <content type="html"><![CDATA[<p>使用chinese-roberta-wwm-ext预训练模型进行情感分类</p><span id="more"></span><h1 id="餐饮服务评价情感倾向分析"><a href="#餐饮服务评价情感倾向分析" class="headerlink" title="餐饮服务评价情感倾向分析"></a>餐饮服务评价情感倾向分析</h1><h3 id="任务数据以预测结果-https-github-com-du2279664786-2022-shujufenxi"><a href="#任务数据以预测结果-https-github-com-du2279664786-2022-shujufenxi" class="headerlink" title="任务数据以预测结果 https://github.com/du2279664786/2022_shujufenxi"></a>任务数据以预测结果 <a href="https://github.com/du2279664786/2022_shujufenxi">https://github.com/du2279664786/2022_shujufenxi</a></h3><p><img src="https://s2.loli.net/2023/03/07/HU1IJXdqSVBmw2l.png" alt="题目B：餐饮服务评价情感倾向分析-1.png"></p><h2 id="·-建立餐饮评论情感倾向模型"><a href="#·-建立餐饮评论情感倾向模型" class="headerlink" title="· 建立餐饮评论情感倾向模型"></a>· 建立餐饮评论情感倾向模型</h2><p>​在建立模型前，需对数据进行分析，由于每个商家的评论长度长短不一，所以应该选取一个合适的长度，大于改长度的进行截断，小于该长度的进行填充，评论长度查看如代码所示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">token_lens = []</span><br><span class="line">for txt in train.comment:</span><br><span class="line">tokens = tokenizer.encode(txt, max_length=512)</span><br><span class="line">token_lens.append(len(tokens))</span><br><span class="line"></span><br><span class="line">sns.distplot(token_lens)</span><br><span class="line">plt.xlim([0, 150]);</span><br><span class="line">plt.xlabel(&#x27;Token count&#x27;);</span><br></pre></td></tr></table></figure><p>​</p><p><img src="https://s2.loli.net/2023/03/07/HkzDWwb1OLvstfl.png" alt="Snipaste_2023-03-07_09-37-46.png"></p><p>​通过上图可以看出文本每条评论的长度绝大多数都在 80 以内，所以可以选取 80 为合适的长度，既不会丢失太多信息，也不会填充太多无用的信息。接下来进行数据集的分割，将训练数据分割成训练集和验证集，训练集用来训练模型，验证集用来评估模型的好坏，将数据的 9 份用来训练，1 份用来验证。如代码所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_train, df_test = train_test_split(train, test_size=<span class="number">0.1</span>, random_state=RANDOM_SEED)</span><br><span class="line">df_val, df_test = train_test_split(df_test, test_size=<span class="number">0.5</span>, random_state=RANDOM_SEED)</span><br></pre></td></tr></table></figure><p>由于 comment 内容全为文字内容，无法直接将其输入网络，故应进行数据预处理操作，即文本向量化操作，使用预训练 roberta 对文本进行 embedding，如代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnterpriseDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,texts,labels,tokenizer,max_len</span>):</span><br><span class="line">        self.texts=texts</span><br><span class="line">        self.labels=labels</span><br><span class="line">        self.tokenizer=tokenizer</span><br><span class="line">        self.max_len=max_len</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,item</span>):</span><br><span class="line">        text=<span class="built_in">str</span>(self.texts[item])</span><br><span class="line">        label=self.labels[item]</span><br><span class="line">        encoding=self.tokenizer.encode_plus(</span><br><span class="line">            text,</span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_len,</span><br><span class="line">            return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">            pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">            return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;texts&#x27;</span>:text,</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>:encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten(),</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>:encoding[<span class="string">&#x27;attention_mask&#x27;</span>].flatten(),</span><br><span class="line">            <span class="string">&#x27;labels&#x27;</span>:torch.tensor(label,dtype=torch.long)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​上述编码的结果包含：input_ids和attention_mask，其中input_ids为编码的结果，attention_mask为可以保证模型在做attention时，有效数据不会被mask。</p><p>​接下来是模型的搭建，通过搭建神经网络模型，进行数据的预测，本赛题使用的模型是chinese-roberta-wwn模型，该模型在情感分类任务上较为优越，下面是模型的加载，如代码所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PRE_TRAINED_MODEL_NAME = <span class="string">&#x27;hfl/chinese-roberta-wwm-ext&#x27;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>​在chinese-roberta-wwn模型的后添加全连接层进行二分类处理，即积极情绪和消极情绪的分类，同时，为防止过拟合，选择神经网络的丢弃率为0.3，网络搭建的实现如代码所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnterpriseDangerClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(EnterpriseDangerClassifier, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br><span class="line">        self.drop = nn.Dropout(p=<span class="number">0.3</span>)</span><br><span class="line">        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) <span class="comment"># 两个类别</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        _, pooled_output = self.bert(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            return_dict = <span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        output = self.drop(pooled_output) <span class="comment"># dropout</span></span><br><span class="line">        <span class="keyword">return</span> self.out(output)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>对于神经网络模型，需要选择合适的优化器，以及损失函数的选取。这里选择的优化器为AdamW，其优点是在Adam优化器的基础上加入$L_0$正则化，有效避免了过拟合问题。损失函数选取的是交叉熵损失函数，用于评估分类问题。如代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-3</span>, correct_bias=<span class="literal">False</span>)</span><br><span class="line">scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">  optimizer,</span><br><span class="line">  num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">  num_training_steps=total_steps</span><br><span class="line">)</span><br><span class="line">loss_fn = nn.CrossEntropyLoss().to(device)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="·-二-情感倾向模型的训练和评估"><a href="#·-二-情感倾向模型的训练和评估" class="headerlink" title="· 二 情感倾向模型的训练和评估"></a>· 二 情感倾向模型的训练和评估</h2><p>在神经模型建立完成之后，需要进行模型的训练，如代码所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params"></span></span><br><span class="line"><span class="params">  model, </span></span><br><span class="line"><span class="params">  data_loader, </span></span><br><span class="line"><span class="params">  loss_fn, </span></span><br><span class="line"><span class="params">  optimizer, </span></span><br><span class="line"><span class="params">  device, </span></span><br><span class="line"><span class="params">  scheduler, </span></span><br><span class="line"><span class="params">  n_examples</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    model = model.train() <span class="comment"># train模式</span></span><br><span class="line">    losses = []</span><br><span class="line">    correct_predictions = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data_loader:</span><br><span class="line">        input_ids = d[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">        attention_mask = d[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">        targets = d[<span class="string">&quot;labels&quot;</span>].to(device)</span><br><span class="line">        outputs = model(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask</span><br><span class="line">        )</span><br><span class="line">        _, preds = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line">        correct_predictions += torch.<span class="built_in">sum</span>(preds == targets)</span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> correct_predictions.double() / n_examples, np.mean(losses)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>训练完成后需进行模型的评估，选择所给数据的五分之一用来模型的评估，并计算相关的准确率，进行可视化展示，如代码所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(torch.tensor(history[<span class="string">&#x27;train_acc&#x27;</span>], device=<span class="string">&#x27;cpu&#x27;</span>), label=<span class="string">&#x27;train accuracy&#x27;</span>)</span><br><span class="line">plt.plot(torch.tensor(history[<span class="string">&#x27;val_acc&#x27;</span>], device=<span class="string">&#x27;cpu&#x27;</span>), label=<span class="string">&#x27;validation accuracy&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training history&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/03/07/JfMv1mkYEchtOAC.png" alt="Snipaste_2023-03-07_09-48-31.png"></p><p>从上图可以看出，训练准确率和验证准确率都在增高，最终训练准确率收敛在97.2%左右</p><p>接下来查看一下模型的混淆矩阵，如代码所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_confusion_matrix</span>(<span class="params">confusion_matrix</span>):</span><br><span class="line">    hmap = sns.heatmap(confusion_matrix, annot=<span class="literal">True</span>, fmt=<span class="string">&quot;d&quot;</span>, cmap=<span class="string">&quot;Blues&quot;</span>)</span><br><span class="line">    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=<span class="number">0</span>, ha=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line">    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=<span class="number">30</span>, ha=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True label&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Predicted label&#x27;</span>);</span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line">df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)</span><br><span class="line">show_confusion_matrix(df_cm)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2023/03/07/q65SirGDfY82gE3.png" alt="Snipaste_2023-03-07_09-50-32.png"></p><p>从以上混淆矩阵可以观察出，TF为409，TP为444，在验证数据中，共有853条样本预测正确，45条样本是预测错误的，准确率在97%左右。接下来看一下模型各方面的评估。如代码所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred, target_names=[<span class="built_in">str</span>(label) <span class="keyword">for</span> label <span class="keyword">in</span> class_names]))</span><br></pre></td></tr></table></figure><table><thead><tr><th></th><th>Precision</th><th>Recall</th><th>F1-score</th><th>Support</th></tr></thead><tbody><tr><td>0</td><td>0.94</td><td>0.95</td><td>0.95</td><td>429</td></tr><tr><td>1</td><td>0.96</td><td>0.95</td><td>0.95</td><td>469</td></tr><tr><td>Weighted   avg</td><td>0.95</td><td>0.95</td><td>0.95</td><td>898</td></tr><tr><td>Maacro   avg</td><td>0.95</td><td>0.95</td><td>0.95</td><td>898</td></tr></tbody></table><h2 id="·-三-对附件test-xlsx进行预测"><a href="#·-三-对附件test-xlsx进行预测" class="headerlink" title="· 三 对附件test.xlsx进行预测"></a>· 三 对附件test.xlsx进行预测</h2><p>首先对测试集进行读取，发现测试集共含有1500条样本，由于已经完成了模型的训练和评估步骤，接下来可以进行模型的预测，并将结果补充到文件的第一列。如代码所示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">con</span>(<span class="params">sample_text</span>):</span><br><span class="line">    encoded_text = tokenizer.encode_plus(</span><br><span class="line">    sample_text,</span><br><span class="line">    max_length=MAX_LEN,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">    return_token_type_ids=<span class="literal">False</span>,</span><br><span class="line">    pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">    return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">    return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">    )</span><br><span class="line">    input_ids = encoded_text[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">    attention_mask = encoded_text[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line">    output = model(input_ids, attention_mask)</span><br><span class="line">    _, prediction = torch.<span class="built_in">max</span>(output, dim=<span class="number">1</span>)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">return</span> class_names[prediction]</span><br><span class="line">test[<span class="string">&#x27;target&#x27;</span>] = test[<span class="string">&#x27;comment&#x27;</span>].apply(<span class="keyword">lambda</span> x:con(x))</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;使用chinese-roberta-wwm-ext预训练模型进行情感分类&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="文本分类" scheme="https://du2279664786.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Web攻击检测与分类识别思路</title>
    <link href="https://du2279664786.github.io/posts/d0147289.html"/>
    <id>https://du2279664786.github.io/posts/d0147289.html</id>
    <published>2022-12-05T14:55:10.000Z</published>
    <updated>2022-12-05T04:20:40.266Z</updated>
    
    <content type="html"><![CDATA[<p>CCF BDCI Web攻击检测与分类识别 Top8思路</p><span id="more"></span><h1 id="CCF-BDCI-Web攻击检测与分类识别-Top8思路"><a href="#CCF-BDCI-Web攻击检测与分类识别-Top8思路" class="headerlink" title="CCF BDCI Web攻击检测与分类识别 Top8思路"></a>CCF BDCI Web攻击检测与分类识别 Top8思路</h1><p><strong>赛题地址</strong>：<a href="https://www.datafountain.cn/competitions/596">https://www.datafountain.cn/competitions/596</a></p><p><img src="https://s2.loli.net/2022/12/05/FHOr3m67MXeTv4w.png" alt="Snipaste_2022-12-05_12-05-48.png"></p><h2 id="赛题背景："><a href="#赛题背景：" class="headerlink" title="赛题背景："></a>赛题背景：</h2><p>某业务平台平均每月捕获到Web攻击数量超过2亿，涉及常见注入攻击，代码执行等类型。传统威胁检测手段通过分析已知攻击特征进行规则匹配，无法检测未知漏洞或攻击手法。如何快速准确地识别未知威胁攻击并且将不同攻击正确分类，对提升Web攻击检测能力至关重要。利用机器学习和深度学习技术对攻击报文进行识别和分类已经成为解决该问题的创新思路，有利于推动AI技术在威胁检测分析场景的研究与应用。</p><h2 id="赛题任务："><a href="#赛题任务：" class="headerlink" title="赛题任务："></a>赛题任务：</h2><p>参赛团队需要对前期提供的训练集进行分析，通过特征工程、机器学习和深度学习等方法构建AI模型，实现对每一条样本正确且快速分类，不断提高模型精确率和召回率。待模型优化稳定后，通过无标签测试集评估各参赛团队模型分类效果，以正确率评估各参赛团队模型质量。</p><h2 id="决赛答辩："><a href="#决赛答辩：" class="headerlink" title="决赛答辩："></a>决赛答辩：</h2><p>决赛答辩中，评审专家将根据答辩作品的创新性、可用性等进行打分；最终成绩将综合考虑初赛成绩、创新性、可用性等方面确定最终排名，最终成绩 &#x3D; 初赛复现成绩 * 80% + 决赛成绩 * 20%。<br>注意，答辩着重考察以下方面：<br>(1) 模型发现未知攻击类型的能力<br>(2) 模型的时间复杂度<br>(3) 其他创新</p><h2 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h2><p>赛题训练集分为6种不同标签，共计约3.5万条数据。训练数据集字段内容主要包括：<br>●　ID：样本编号<br>●　label：攻击类型编号<br>●　其他：HTTP协议内容</p><h2 id="评测标准"><a href="#评测标准" class="headerlink" title="评测标准"></a>评测标准</h2><p>评比期间将提供无标签测试集，参赛团队需提交对该测试集每条数据的模型分类结果，即每条数据中增加一个predict字段（模型分类结果），与训练集label字段含义保持一致。<br>评估程序将模型预测结果predict与标准答案label对比，统计精确率、召回率和F1，最终以F1为准。</p><table><thead><tr><th align="center">标签</th><th align="center">分类为正标签</th><th align="center">分类为负标签</th></tr></thead><tbody><tr><td align="center">正标签</td><td align="center">TP</td><td align="center">FN</td></tr><tr><td align="center">负标签</td><td align="center">FP</td><td align="center">TN</td></tr></tbody></table><p>精确率计算公式：Precision &#x3D; TP&#x2F;(TP + FP)<br>召回率计算公式：Recall &#x3D; TP&#x2F;(TP + FN)<br>F1计算公式：F1 &#x3D; 2 * Precision * Recall&#x2F;(Precision + Recall)<br>注：该F1为 macro F1</p><h1 id="代码如下"><a href="#代码如下" class="headerlink" title="代码如下"></a>代码如下</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lgb删掉tfidf+refer：线下0.9644，线上0.95997</span></span><br><span class="line"><span class="comment"># lgb未删tfidf+refer：线下0.96432，线上0.95828</span></span><br><span class="line"><span class="comment"># xgb:线下0.96494</span></span><br><span class="line"><span class="comment"># cat:线下9650，线上0.9613</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> early_stopping</span><br><span class="line"><span class="keyword">from</span> lightgbm <span class="keyword">import</span> log_evaluation</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> TruncatedSVD</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score,f1_score</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> user_agents <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostClassifier</span><br><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote, unquote, urlparse</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> glob  </span><br><span class="line"></span><br><span class="line">pd.set_option(<span class="string">&#x27;display.max_columns&#x27;</span>, <span class="literal">None</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iPhone的UserAgent</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_ua</span>(<span class="params">row</span>):</span><br><span class="line">    user_agent = parse(row[<span class="string">&#x27;user_agent&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    browser_family=<span class="built_in">str</span>(user_agent.browser.family)</span><br><span class="line">    os_family=<span class="built_in">str</span>(user_agent.os.family)</span><br><span class="line">    device_family=<span class="built_in">str</span>(user_agent.device.family)</span><br><span class="line">    device_brand=<span class="built_in">str</span>(user_agent.device.brand)</span><br><span class="line">    device_model=<span class="built_in">str</span>(user_agent.device.model)</span><br><span class="line">    <span class="keyword">return</span> browser_family,os_family,device_family,device_brand,device_model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">prob = np.load(<span class="string">&#x27;E://data//DF//Web攻击检测与分类识别//large/deberta-v3-large_probs.npy&#x27;</span>)</span><br><span class="line">prob.shape</span><br></pre></td></tr></table></figure><pre><code>(4000, 6)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">train=pd.read_pickle(<span class="string">&#x27;E:\\data\\DF\\Web攻击检测与分类识别/large/oof_df.pkl&#x27;</span>)     <span class="comment"># 33037 rows × 15 columns</span></span><br><span class="line">sub = pd.read_csv(<span class="string">&#x27;E:\\data\\DF\\Web攻击检测与分类识别\\submit_example (10).csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test=pd.read_csv(<span class="string">&#x27;E:\\data\\DF\\Web攻击检测与分类识别/test.csv&#x27;</span>)</span><br><span class="line">train.head()</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>method</th>      <th>user_agent</th>      <th>url</th>      <th>refer</th>      <th>body</th>      <th>label</th>      <th>text</th>      <th>fold</th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>13429</td>      <td>GET</td>      <td>'||(select 1 from (select pg_sleep(8))x)||'</td>      <td>/kelev/scripts/?C=M%3BO%3DA</td>      <td>NAN</td>      <td>GET /kelev/scripts/?C=M%3BO%3DA HTTP/1.1 Accep...</td>      <td>1</td>      <td>method:GET[SEP]user_agent:'||(select 1 from (s...</td>      <td>0</td>      <td>0.000238</td>      <td>0.999110</td>      <td>0.000445</td>      <td>0.000058</td>      <td>0.000040</td>      <td>0.000110</td>    </tr>    <tr>      <th>1</th>      <td>18125</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 11; M2102K1C B...</td>      <td>/livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...</td>      <td>NAN</td>      <td>GET /livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;...</td>      <td>1</td>      <td>method:GET[SEP]user_agent:Dalvik/2.1.0 (Linux;...</td>      <td>0</td>      <td>0.006598</td>      <td>0.992451</td>      <td>0.000763</td>      <td>0.000086</td>      <td>0.000067</td>      <td>0.000035</td>    </tr>    <tr>      <th>2</th>      <td>14538</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 11; M2011K2C B...</td>      <td>/livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=0&amp;openudid=d2...</td>      <td>NAN</td>      <td>GET /livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=0&amp;openudi...</td>      <td>1</td>      <td>method:GET[SEP]user_agent:Dalvik/2.1.0 (Linux;...</td>      <td>0</td>      <td>0.000783</td>      <td>0.999017</td>      <td>0.000138</td>      <td>0.000031</td>      <td>0.000017</td>      <td>0.000013</td>    </tr>    <tr>      <th>3</th>      <td>7127</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 10; MI 9 MIUI/...</td>      <td>/livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...</td>      <td>NAN</td>      <td>NAN</td>      <td>1</td>      <td>method:GET[SEP]user_agent:Dalvik/2.1.0 (Linux;...</td>      <td>0</td>      <td>0.007603</td>      <td>0.991491</td>      <td>0.000725</td>      <td>0.000087</td>      <td>0.000062</td>      <td>0.000033</td>    </tr>    <tr>      <th>4</th>      <td>7</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 10; ELS-AN00 B...</td>      <td>/livemsg?sdtfrom=v5004&amp;ad_type=WL_WK&amp;oadid=&amp;ty...</td>      <td>NAN</td>      <td>GET /livemsg?sdtfrom=v5004&amp;ad_type=WL_WK&amp;oadid...</td>      <td>1</td>      <td>method:GET[SEP]user_agent:Dalvik/2.1.0 (Linux;...</td>      <td>0</td>      <td>0.000529</td>      <td>0.999257</td>      <td>0.000153</td>      <td>0.000020</td>      <td>0.000021</td>      <td>0.000019</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>method</th>      <th>user_agent</th>      <th>url</th>      <th>refer</th>      <th>body</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>GET</td>      <td>Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl...</td>      <td>/demo/aisec/upload.php?act='%7C%7C(select+1+fr...</td>      <td>http://demo.aisec.cn/demo/aisec/upload.php?t=0...</td>      <td>GET /demo/aisec/upload.php?act='%7C%7C(select+...</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 11; M2102J2SC ...</td>      <td>/livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=1&amp;openudid=5f...</td>      <td>NaN</td>      <td>GET /livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=1&amp;openudi...</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>GET</td>      <td>Mozilla/5.0 (Windows NT 10.0; rv:78.0) Gecko/2...</td>      <td>/create_user/?username=%3Cscript%3Ealert(docum...</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>GET</td>      <td>NaN</td>      <td>/mmsns/WeDwicXmkOl4kjKsBycicI0H3q41r6syFFvu46h...</td>      <td>NaN</td>      <td>NaN</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>PUT</td>      <td>Mozilla/5.0 (Windows NT 10.0; rv:78.0) Gecko/2...</td>      <td>/naizau.jsp/</td>      <td>NaN</td>      <td>GET /login HTTP/1.1 Host: 111.160.211.18:8088 ...</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><pre><code>id            0method        0user_agent    0url           0refer         0body          0label         0text          0fold          00             01             02             03             04             05             0dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.columns</span><br></pre></td></tr></table></figure><pre><code>Index([&#39;id&#39;, &#39;method&#39;, &#39;user_agent&#39;, &#39;url&#39;, &#39;refer&#39;, &#39;body&#39;, &#39;label&#39;, &#39;text&#39;,       &#39;fold&#39;, &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;],      dtype=&#39;object&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test[[<span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="string">&#x27;2&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;5&#x27;</span>]]=prob</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># test[&#x27;label&#x27;]=pd.read_csv(&#x27;models/v4/lgb.csv&#x27;)[&#x27;predict&#x27;]</span></span><br><span class="line">train=train.drop(<span class="string">&#x27;text&#x27;</span>,axis=<span class="number">1</span>)</span><br><span class="line">train=train.drop(<span class="string">&#x27;fold&#x27;</span>,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train=train[[<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;method&#x27;</span>, <span class="string">&#x27;user_agent&#x27;</span>, <span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;refer&#x27;</span>, <span class="string">&#x27;body&#x27;</span>, <span class="string">&#x27;0&#x27;</span>, <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">       <span class="string">&#x27;2&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="string">&#x27;5&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train.shape&quot;</span>,train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;test.shape&quot;</span>,test.shape)</span><br></pre></td></tr></table></figure><pre><code>train.shape (33037, 13)test.shape (4000, 12)</code></pre><h1 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h1><p>赛题训练集分为6种不同标签，共计约3.5万条数据。训练数据集字段内容主要包括：<br>●　lable：攻击类型编号<br>●　其他：HTTP协议内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看训练集的字段</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.columns</span><br></pre></td></tr></table></figure><pre><code>Index([&#39;id&#39;, &#39;method&#39;, &#39;user_agent&#39;, &#39;url&#39;, &#39;refer&#39;, &#39;body&#39;, &#39;0&#39;, &#39;1&#39;, &#39;2&#39;,       &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;label&#39;],      dtype=&#39;object&#39;)</code></pre><p>‘lable’看着很别扭，重新rename一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train=train.rename(columns=&#123;<span class="string">&#x27;lable&#x27;</span>:<span class="string">&#x27;label&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.dtypes</span><br></pre></td></tr></table></figure><pre><code>id              int64method         objectuser_agent     objecturl            objectrefer          objectbody           object0             float321             float322             float323             float324             float325             float32label           int64dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标签个数统计</span></span><br><span class="line">train[<span class="string">&#x27;label&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>1    140382     99390     64893     12154      6975      659Name: label, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;label&#x27;</span>].value_counts().plot(kind=<span class="string">&#x27;bar&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/12/05/2x7AWXbvuoigw8n.png" alt="Snipaste_2022-12-05_12-13-30.png"></p><p>​    </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data=pd.concat([train,test],axis=<span class="number">0</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">data.nunique()</span><br></pre></td></tr></table></figure><pre><code>id            19497method           21user_agent     1087url           36613refer           941body          223800             359151             316182             360613             369614             369655             36959label             6dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 缺失值处理：</span></span><br><span class="line">data[<span class="string">&#x27;user_agent&#x27;</span>]=data[<span class="string">&#x27;user_agent&#x27;</span>].fillna(<span class="string">&#x27;NAN&#x27;</span>)</span><br><span class="line">data[<span class="string">&#x27;refer&#x27;</span>]=data[<span class="string">&#x27;refer&#x27;</span>].fillna(<span class="string">&#x27;NAN&#x27;</span>)</span><br><span class="line">data[<span class="string">&#x27;body&#x27;</span>]=data[<span class="string">&#x27;body&#x27;</span>].fillna(<span class="string">&#x27;NAN&#x27;</span>)</span><br><span class="line">data[<span class="string">&#x27;url&#x27;</span>]=data[<span class="string">&#x27;url&#x27;</span>].fillna(<span class="string">&#x27;NAN&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提取user_agent特征</span></span><br><span class="line">ua_cols=[<span class="string">&#x27;browser_family&#x27;</span>, <span class="string">&#x27;os_family&#x27;</span>, <span class="string">&#x27;device_family&#x27;</span>,<span class="string">&#x27;device_brand&#x27;</span>,<span class="string">&#x27;device_model&#x27;</span>]</span><br><span class="line">data[ua_cols] = data.apply(get_ua, axis=<span class="number">1</span>, result_type=<span class="string">&quot;expand&quot;</span>)</span><br><span class="line">data.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>method</th>      <th>user_agent</th>      <th>url</th>      <th>refer</th>      <th>body</th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>      <th>label</th>      <th>browser_family</th>      <th>os_family</th>      <th>device_family</th>      <th>device_brand</th>      <th>device_model</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>13429</td>      <td>GET</td>      <td>'||(select 1 from (select pg_sleep(8))x)||'</td>      <td>/kelev/scripts/?C=M%3BO%3DA</td>      <td>NAN</td>      <td>GET /kelev/scripts/?C=M%3BO%3DA HTTP/1.1 Accep...</td>      <td>0.000238</td>      <td>0.999110</td>      <td>0.000445</td>      <td>0.000058</td>      <td>0.000040</td>      <td>0.000110</td>      <td>1.0</td>      <td>Other</td>      <td>Other</td>      <td>Other</td>      <td>None</td>      <td>None</td>    </tr>    <tr>      <th>1</th>      <td>18125</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 11; M2102K1C B...</td>      <td>/livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...</td>      <td>NAN</td>      <td>GET /livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;...</td>      <td>0.006598</td>      <td>0.992451</td>      <td>0.000763</td>      <td>0.000086</td>      <td>0.000067</td>      <td>0.000035</td>      <td>1.0</td>      <td>Android</td>      <td>Android</td>      <td>M2102K1C</td>      <td>Generic_Android</td>      <td>M2102K1C</td>    </tr>    <tr>      <th>2</th>      <td>14538</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 11; M2011K2C B...</td>      <td>/livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=0&amp;openudid=d2...</td>      <td>NAN</td>      <td>GET /livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=0&amp;openudi...</td>      <td>0.000783</td>      <td>0.999017</td>      <td>0.000138</td>      <td>0.000031</td>      <td>0.000017</td>      <td>0.000013</td>      <td>1.0</td>      <td>Android</td>      <td>Android</td>      <td>M2011K2C</td>      <td>Generic_Android</td>      <td>M2011K2C</td>    </tr>    <tr>      <th>3</th>      <td>7127</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 10; MI 9 MIUI/...</td>      <td>/livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...</td>      <td>NAN</td>      <td>NAN</td>      <td>0.007603</td>      <td>0.991491</td>      <td>0.000725</td>      <td>0.000087</td>      <td>0.000062</td>      <td>0.000033</td>      <td>1.0</td>      <td>Android</td>      <td>Android</td>      <td>XiaoMi MI 9</td>      <td>XiaoMi</td>      <td>MI 9</td>    </tr>    <tr>      <th>4</th>      <td>7</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 10; ELS-AN00 B...</td>      <td>/livemsg?sdtfrom=v5004&amp;ad_type=WL_WK&amp;oadid=&amp;ty...</td>      <td>NAN</td>      <td>GET /livemsg?sdtfrom=v5004&amp;ad_type=WL_WK&amp;oadid...</td>      <td>0.000529</td>      <td>0.999257</td>      <td>0.000153</td>      <td>0.000020</td>      <td>0.000021</td>      <td>0.000019</td>      <td>1.0</td>      <td>Android</td>      <td>Android</td>      <td>ELS-AN00</td>      <td>Huawei</td>      <td>ELS-AN00</td>    </tr>  </tbody></table></div><h1 id="基础特征"><a href="#基础特征" class="headerlink" title="基础特征"></a>基础特征</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;user_agent_len&#x27;</span>]=data[<span class="string">&#x27;user_agent&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x))</span><br><span class="line">data[<span class="string">&#x27;url_len&#x27;</span>]=data[<span class="string">&#x27;url&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x))</span><br><span class="line">data[<span class="string">&#x27;refer_len&#x27;</span>]=data[<span class="string">&#x27;refer&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x))</span><br><span class="line">data[<span class="string">&#x27;body_len&#x27;</span>]=data[<span class="string">&#x27;body&#x27;</span>].apply(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x))</span><br><span class="line">data[<span class="string">&#x27;body_user_agent_len_diff&#x27;</span>]=data[<span class="string">&#x27;body_len&#x27;</span>]-data[<span class="string">&#x27;user_agent_len&#x27;</span>]</span><br><span class="line">data[<span class="string">&#x27;body_url_len_diff&#x27;</span>]=data[<span class="string">&#x27;body_len&#x27;</span>]-data[<span class="string">&#x27;url_len&#x27;</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将user_agent列进行tfidf特征提取，再SVD变成16维度</span></span><br><span class="line">texts=data[<span class="string">&#x27;user_agent&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line">n_components = <span class="number">16</span>      <span class="comment"># 期望维数 </span></span><br><span class="line">tf = TfidfVectorizer(min_df= <span class="number">1</span>, max_df=<span class="number">0.5</span>,analyzer = <span class="string">&#x27;char_wb&#x27;</span>, ngram_range = (<span class="number">1</span>,<span class="number">3</span>))     <span class="comment"># ngram_range = (2,5)</span></span><br><span class="line">X = tf.fit_transform(texts)</span><br><span class="line">svd = TruncatedSVD(n_components=n_components,</span><br><span class="line">                   random_state=<span class="number">42</span>)</span><br><span class="line">X_svd = svd.fit_transform(X)</span><br><span class="line">df_tfidf = pd.DataFrame(X_svd)</span><br><span class="line">df_tfidf.columns = [<span class="string">f&#x27;user_agent_name_tfidf_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_components)]</span><br><span class="line">data=pd.concat([data,df_tfidf],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">texts=data[<span class="string">&#x27;url&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line">n_components = <span class="number">16</span></span><br><span class="line">tf = TfidfVectorizer(min_df= <span class="number">1</span>, max_df=<span class="number">0.5</span>,analyzer = <span class="string">&#x27;char_wb&#x27;</span>, ngram_range = (<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">X = tf.fit_transform(texts)</span><br><span class="line">svd = TruncatedSVD(n_components=n_components,</span><br><span class="line">                   random_state=<span class="number">42</span>)</span><br><span class="line">X_svd = svd.fit_transform(X)</span><br><span class="line">df_tfidf = pd.DataFrame(X_svd)</span><br><span class="line">df_tfidf.columns = [<span class="string">f&#x27;url_name_tfidf_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_components)]</span><br><span class="line">data=pd.concat([data,df_tfidf],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># texts=data[&#x27;refer&#x27;].values.tolist()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># n_components = 16</span></span><br><span class="line"><span class="comment"># tf = TfidfVectorizer(min_df= 1, max_df=0.5,analyzer = &#x27;char_wb&#x27;, ngram_range = (1,3))</span></span><br><span class="line"><span class="comment"># X = tf.fit_transform(texts)</span></span><br><span class="line"><span class="comment"># svd = TruncatedSVD(n_components=n_components,</span></span><br><span class="line"><span class="comment">#                    random_state=42)</span></span><br><span class="line"><span class="comment"># X_svd = svd.fit_transform(X)</span></span><br><span class="line"><span class="comment"># df_tfidf = pd.DataFrame(X_svd)</span></span><br><span class="line"><span class="comment"># df_tfidf.columns = [f&#x27;refer_tfidf_&#123;i&#125;&#x27; for i in range(n_components)]</span></span><br><span class="line"><span class="comment"># data=pd.concat([data,df_tfidf],axis=1)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">texts=data[<span class="string">&#x27;body&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line">n_components = <span class="number">32</span></span><br><span class="line">tf = TfidfVectorizer(min_df= <span class="number">1</span>, max_df=<span class="number">0.5</span>,analyzer = <span class="string">&#x27;char_wb&#x27;</span>, ngram_range = (<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">X = tf.fit_transform(texts)</span><br><span class="line">svd = TruncatedSVD(n_components=n_components,</span><br><span class="line">                   random_state=<span class="number">42</span>)</span><br><span class="line">X_svd = svd.fit_transform(X)</span><br><span class="line">df_tfidf = pd.DataFrame(X_svd)</span><br><span class="line">df_tfidf.columns = [<span class="string">f&#x27;body_tfidf_<span class="subst">&#123;i&#125;</span>&#x27;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_components)]</span><br><span class="line">data=pd.concat([data,df_tfidf],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">&#x27;method&#x27;</span>, <span class="string">&#x27;url&#x27;</span>,<span class="string">&#x27;refer&#x27;</span>, <span class="string">&#x27;body&#x27;</span>,<span class="string">&#x27;browser_family&#x27;</span>,<span class="string">&#x27;os_family&#x27;</span>,<span class="string">&#x27;device_family&#x27;</span>,<span class="string">&#x27;device_brand&#x27;</span>,<span class="string">&#x27;device_model&#x27;</span>]:    <span class="comment"># refer</span></span><br><span class="line">    data[<span class="string">f&#x27;id_<span class="subst">&#123;f&#125;</span>_nunique&#x27;</span>] = data.groupby([<span class="string">&#x27;id&#x27;</span>])[f].transform(<span class="string">&#x27;nunique&#x27;</span>)</span><br><span class="line">    data[<span class="string">f&#x27;id_<span class="subst">&#123;f&#125;</span>_count&#x27;</span>] = data.groupby([<span class="string">&#x27;id&#x27;</span>])[f].transform(<span class="string">&#x27;count&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.split(<span class="string">&#x27;[=&amp;]&#x27;</span>, urlparse(data[<span class="string">&#x27;url&#x27;</span>][<span class="number">0</span>])[<span class="number">4</span>])</span><br></pre></td></tr></table></figure><pre><code>[&#39;C&#39;, &#39;M%3BO%3DA&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_url_query</span>(<span class="params">s</span>):</span><br><span class="line">    li = re.split(<span class="string">&#x27;[=&amp;]&#x27;</span>, urlparse(s)[<span class="number">4</span>])</span><br><span class="line">    <span class="keyword">return</span> [li[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(li)) <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_max_str_length</span>(<span class="params">x</span>):</span><br><span class="line">    max_ = <span class="number">0</span></span><br><span class="line">    li = [<span class="built_in">len</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(li) <span class="keyword">if</span> <span class="built_in">len</span>(li) &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_str_length_std</span>(<span class="params">x</span>):</span><br><span class="line">    max_ = <span class="number">0</span></span><br><span class="line">    li = [<span class="built_in">len</span>(i) <span class="keyword">for</span> i <span class="keyword">in</span> x]</span><br><span class="line">    <span class="keyword">return</span> np.std(li) <span class="keyword">if</span> <span class="built_in">len</span>(li) &gt; <span class="number">0</span> <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;url_unquote&#x27;</span>] = data[<span class="string">&#x27;url&#x27;</span>].apply(unquote)</span><br><span class="line">data[<span class="string">&#x27;url_query&#x27;</span>] = data[<span class="string">&#x27;url_unquote&#x27;</span>].apply(<span class="keyword">lambda</span> x: get_url_query(x))</span><br><span class="line">data[<span class="string">&#x27;url_query_num&#x27;</span>] = data[<span class="string">&#x27;url_query&#x27;</span>].apply(<span class="built_in">len</span>)</span><br><span class="line">data[<span class="string">&#x27;url_query_max_len&#x27;</span>] = data[<span class="string">&#x27;url_query&#x27;</span>].apply(find_max_str_length)</span><br><span class="line">data[<span class="string">&#x27;url_query_len_std&#x27;</span>] = data[<span class="string">&#x27;url_query&#x27;</span>].apply(find_str_length_std)</span><br><span class="line">data[<span class="string">&#x27;url&#x27;</span>].apply(unquote)</span><br></pre></td></tr></table></figure><pre><code>0                                  /kelev/scripts/?C=M;O=A1        /livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...2        /livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=0&amp;openudid=d2...3        /livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...4        /livemsg?sdtfrom=v5004&amp;ad_type=WL_WK&amp;oadid=&amp;ty...                               ...                        37032    /livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=1&amp;openudid=64...37033                                          /runtime.js37034                                     /query?49352181237035              /stats.php?rand=JtmT4wBtrpNy5RJnNX9wCUo37036    /api/gateway.do?method=qihoo.sdk.user.mobile.l...Name: url, Length: 37037, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>method</th>      <th>user_agent</th>      <th>url</th>      <th>refer</th>      <th>body</th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>      <th>label</th>      <th>browser_family</th>      <th>os_family</th>      <th>device_family</th>      <th>device_brand</th>      <th>device_model</th>      <th>user_agent_len</th>      <th>url_len</th>      <th>refer_len</th>      <th>body_len</th>      <th>body_user_agent_len_diff</th>      <th>body_url_len_diff</th>      <th>user_agent_name_tfidf_0</th>      <th>user_agent_name_tfidf_1</th>      <th>user_agent_name_tfidf_2</th>      <th>user_agent_name_tfidf_3</th>      <th>user_agent_name_tfidf_4</th>      <th>user_agent_name_tfidf_5</th>      <th>user_agent_name_tfidf_6</th>      <th>user_agent_name_tfidf_7</th>      <th>user_agent_name_tfidf_8</th>      <th>user_agent_name_tfidf_9</th>      <th>user_agent_name_tfidf_10</th>      <th>user_agent_name_tfidf_11</th>      <th>user_agent_name_tfidf_12</th>      <th>user_agent_name_tfidf_13</th>      <th>user_agent_name_tfidf_14</th>      <th>user_agent_name_tfidf_15</th>      <th>url_name_tfidf_0</th>      <th>url_name_tfidf_1</th>      <th>url_name_tfidf_2</th>      <th>url_name_tfidf_3</th>      <th>url_name_tfidf_4</th>      <th>url_name_tfidf_5</th>      <th>url_name_tfidf_6</th>      <th>url_name_tfidf_7</th>      <th>url_name_tfidf_8</th>      <th>url_name_tfidf_9</th>      <th>url_name_tfidf_10</th>      <th>url_name_tfidf_11</th>      <th>url_name_tfidf_12</th>      <th>url_name_tfidf_13</th>      <th>url_name_tfidf_14</th>      <th>url_name_tfidf_15</th>      <th>body_tfidf_0</th>      <th>body_tfidf_1</th>      <th>body_tfidf_2</th>      <th>body_tfidf_3</th>      <th>body_tfidf_4</th>      <th>body_tfidf_5</th>      <th>body_tfidf_6</th>      <th>body_tfidf_7</th>      <th>body_tfidf_8</th>      <th>body_tfidf_9</th>      <th>body_tfidf_10</th>      <th>body_tfidf_11</th>      <th>body_tfidf_12</th>      <th>body_tfidf_13</th>      <th>body_tfidf_14</th>      <th>body_tfidf_15</th>      <th>body_tfidf_16</th>      <th>body_tfidf_17</th>      <th>body_tfidf_18</th>      <th>body_tfidf_19</th>      <th>body_tfidf_20</th>      <th>body_tfidf_21</th>      <th>body_tfidf_22</th>      <th>body_tfidf_23</th>      <th>body_tfidf_24</th>      <th>body_tfidf_25</th>      <th>body_tfidf_26</th>      <th>body_tfidf_27</th>      <th>body_tfidf_28</th>      <th>body_tfidf_29</th>      <th>body_tfidf_30</th>      <th>body_tfidf_31</th>      <th>id_method_nunique</th>      <th>id_method_count</th>      <th>id_url_nunique</th>      <th>id_url_count</th>      <th>id_refer_nunique</th>      <th>id_refer_count</th>      <th>id_body_nunique</th>      <th>id_body_count</th>      <th>id_browser_family_nunique</th>      <th>id_browser_family_count</th>      <th>id_os_family_nunique</th>      <th>id_os_family_count</th>      <th>id_device_family_nunique</th>      <th>id_device_family_count</th>      <th>id_device_brand_nunique</th>      <th>id_device_brand_count</th>      <th>id_device_model_nunique</th>      <th>id_device_model_count</th>      <th>url_unquote</th>      <th>url_query</th>      <th>url_query_num</th>      <th>url_query_max_len</th>      <th>url_query_len_std</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>13429</td>      <td>GET</td>      <td>'||(select 1 from (select pg_sleep(8))x)||'</td>      <td>/kelev/scripts/?C=M%3BO%3DA</td>      <td>NAN</td>      <td>GET /kelev/scripts/?C=M%3BO%3DA HTTP/1.1 Accep...</td>      <td>0.000238</td>      <td>0.999110</td>      <td>0.000445</td>      <td>0.000058</td>      <td>0.000040</td>      <td>0.000110</td>      <td>1.0</td>      <td>Other</td>      <td>Other</td>      <td>Other</td>      <td>None</td>      <td>None</td>      <td>43</td>      <td>27</td>      <td>3</td>      <td>212</td>      <td>169</td>      <td>185</td>      <td>0.010070</td>      <td>0.009456</td>      <td>0.001205</td>      <td>0.003217</td>      <td>0.021082</td>      <td>0.000999</td>      <td>-0.000847</td>      <td>-0.002107</td>      <td>0.008443</td>      <td>0.002747</td>      <td>0.023997</td>      <td>-0.003526</td>      <td>-0.001894</td>      <td>-0.013000</td>      <td>-0.005918</td>      <td>0.009153</td>      <td>0.066298</td>      <td>0.059683</td>      <td>-0.057310</td>      <td>-0.006595</td>      <td>-0.001159</td>      <td>0.094755</td>      <td>-0.021858</td>      <td>0.023071</td>      <td>-0.001968</td>      <td>0.043890</td>      <td>0.022147</td>      <td>-0.006037</td>      <td>-0.005375</td>      <td>0.014239</td>      <td>0.077494</td>      <td>0.081818</td>      <td>0.000054</td>      <td>0.115960</td>      <td>0.105404</td>      <td>-0.027789</td>      <td>-0.002577</td>      <td>0.064009</td>      <td>-0.039324</td>      <td>0.006577</td>      <td>0.023000</td>      <td>-0.064038</td>      <td>0.021134</td>      <td>-0.058787</td>      <td>0.011353</td>      <td>0.066560</td>      <td>0.010658</td>      <td>0.189591</td>      <td>-0.067039</td>      <td>-0.116538</td>      <td>-0.014128</td>      <td>-0.029329</td>      <td>0.047018</td>      <td>-0.023777</td>      <td>-0.035825</td>      <td>0.005829</td>      <td>0.013030</td>      <td>-0.022777</td>      <td>-0.007482</td>      <td>0.039357</td>      <td>0.046385</td>      <td>-0.007902</td>      <td>-0.029089</td>      <td>-0.051989</td>      <td>1</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>1</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>1</td>      <td>2</td>      <td>1</td>      <td>2</td>      <td>1</td>      <td>2</td>      <td>/kelev/scripts/?C=M;O=A</td>      <td>[M;O]</td>      <td>1</td>      <td>3</td>      <td>0.000000</td>    </tr>    <tr>      <th>1</th>      <td>18125</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 11; M2102K1C B...</td>      <td>/livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...</td>      <td>NAN</td>      <td>GET /livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;...</td>      <td>0.006598</td>      <td>0.992451</td>      <td>0.000763</td>      <td>0.000086</td>      <td>0.000067</td>      <td>0.000035</td>      <td>1.0</td>      <td>Android</td>      <td>Android</td>      <td>M2102K1C</td>      <td>Generic_Android</td>      <td>M2102K1C</td>      <td>67</td>      <td>1747</td>      <td>3</td>      <td>2016</td>      <td>1949</td>      <td>269</td>      <td>0.035096</td>      <td>0.120188</td>      <td>0.270092</td>      <td>0.655747</td>      <td>-0.042591</td>      <td>-0.024738</td>      <td>-0.019824</td>      <td>-0.000642</td>      <td>-0.015951</td>      <td>-0.183311</td>      <td>-0.026321</td>      <td>-0.061993</td>      <td>-0.018288</td>      <td>0.027496</td>      <td>-0.012776</td>      <td>-0.002250</td>      <td>0.617250</td>      <td>-0.130247</td>      <td>0.079094</td>      <td>-0.023195</td>      <td>0.003702</td>      <td>-0.030486</td>      <td>-0.013201</td>      <td>-0.021136</td>      <td>0.012303</td>      <td>-0.006278</td>      <td>-0.023727</td>      <td>-0.003599</td>      <td>0.027036</td>      <td>0.006405</td>      <td>-0.005268</td>      <td>0.010234</td>      <td>0.000132</td>      <td>0.530101</td>      <td>-0.277845</td>      <td>0.022171</td>      <td>-0.069403</td>      <td>-0.062703</td>      <td>-0.006813</td>      <td>0.001156</td>      <td>-0.028995</td>      <td>-0.009364</td>      <td>-0.013567</td>      <td>0.015499</td>      <td>-0.009968</td>      <td>-0.032960</td>      <td>-0.000036</td>      <td>-0.008127</td>      <td>-0.000813</td>      <td>0.001447</td>      <td>0.009261</td>      <td>-0.017541</td>      <td>-0.000682</td>      <td>-0.003697</td>      <td>0.007340</td>      <td>-0.010968</td>      <td>0.008710</td>      <td>-0.067023</td>      <td>-0.014870</td>      <td>-0.024112</td>      <td>0.011792</td>      <td>0.004538</td>      <td>0.014397</td>      <td>-0.003550</td>      <td>1</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>1</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>/livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...</td>      <td>[WL_WK, , web, 0, 1, 210810, 116, 1, 8, fa0d30...</td>      <td>23</td>      <td>1324</td>      <td>268.709026</td>    </tr>    <tr>      <th>2</th>      <td>14538</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 11; M2011K2C B...</td>      <td>/livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=0&amp;openudid=d2...</td>      <td>NAN</td>      <td>GET /livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=0&amp;openudi...</td>      <td>0.000783</td>      <td>0.999017</td>      <td>0.000138</td>      <td>0.000031</td>      <td>0.000017</td>      <td>0.000013</td>      <td>1.0</td>      <td>Android</td>      <td>Android</td>      <td>M2011K2C</td>      <td>Generic_Android</td>      <td>M2011K2C</td>      <td>67</td>      <td>1688</td>      <td>3</td>      <td>1986</td>      <td>1919</td>      <td>298</td>      <td>0.034866</td>      <td>0.170330</td>      <td>0.292857</td>      <td>0.713273</td>      <td>-0.047369</td>      <td>-0.025480</td>      <td>-0.014846</td>      <td>-0.002015</td>      <td>-0.095211</td>      <td>-0.199969</td>      <td>0.001352</td>      <td>0.004859</td>      <td>0.026900</td>      <td>-0.000053</td>      <td>-0.045812</td>      <td>-0.003042</td>      <td>0.662307</td>      <td>-0.134895</td>      <td>0.074788</td>      <td>-0.033836</td>      <td>0.004867</td>      <td>-0.012766</td>      <td>-0.014195</td>      <td>-0.019396</td>      <td>0.016613</td>      <td>-0.006143</td>      <td>-0.018700</td>      <td>-0.012640</td>      <td>0.030395</td>      <td>0.004169</td>      <td>-0.005899</td>      <td>0.005060</td>      <td>0.000137</td>      <td>0.557311</td>      <td>-0.298627</td>      <td>0.021799</td>      <td>-0.072395</td>      <td>-0.035369</td>      <td>-0.009561</td>      <td>-0.020158</td>      <td>-0.030540</td>      <td>-0.019107</td>      <td>-0.011824</td>      <td>0.016220</td>      <td>-0.010370</td>      <td>-0.021592</td>      <td>0.002470</td>      <td>0.001666</td>      <td>-0.004027</td>      <td>-0.000666</td>      <td>0.012006</td>      <td>-0.009133</td>      <td>-0.007882</td>      <td>-0.001795</td>      <td>-0.003188</td>      <td>-0.015516</td>      <td>0.010797</td>      <td>-0.083144</td>      <td>-0.021120</td>      <td>-0.029922</td>      <td>0.020777</td>      <td>0.001687</td>      <td>0.006579</td>      <td>-0.001808</td>      <td>1</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>1</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>2</td>      <td>/livemsg?ad_type=WL_WK&amp;ty=web&amp;pu=0&amp;openudid=d2...</td>      <td>[WL_WK, web, 0, d24c93f6c8de719a00f1676f3a9a53...</td>      <td>29</td>      <td>1154</td>      <td>209.374211</td>    </tr>    <tr>      <th>3</th>      <td>7127</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 10; MI 9 MIUI/...</td>      <td>/livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...</td>      <td>NAN</td>      <td>NAN</td>      <td>0.007603</td>      <td>0.991491</td>      <td>0.000725</td>      <td>0.000087</td>      <td>0.000062</td>      <td>0.000033</td>      <td>1.0</td>      <td>Android</td>      <td>Android</td>      <td>XiaoMi MI 9</td>      <td>XiaoMi</td>      <td>MI 9</td>      <td>64</td>      <td>1613</td>      <td>3</td>      <td>3</td>      <td>-61</td>      <td>-1610</td>      <td>0.038503</td>      <td>0.058521</td>      <td>0.184916</td>      <td>0.434441</td>      <td>0.026507</td>      <td>-0.024867</td>      <td>-0.016191</td>      <td>0.021308</td>      <td>0.058906</td>      <td>0.048677</td>      <td>-0.015773</td>      <td>-0.017188</td>      <td>0.012508</td>      <td>0.026593</td>      <td>0.009872</td>      <td>0.001220</td>      <td>0.621003</td>      <td>-0.119104</td>      <td>0.071898</td>      <td>-0.014246</td>      <td>-0.005748</td>      <td>-0.025066</td>      <td>-0.015300</td>      <td>-0.006643</td>      <td>0.011277</td>      <td>-0.011099</td>      <td>-0.026949</td>      <td>-0.013011</td>      <td>0.027294</td>      <td>0.006678</td>      <td>0.006156</td>      <td>0.004784</td>      <td>1.000000</td>      <td>-0.000356</td>      <td>-0.000224</td>      <td>-0.000019</td>      <td>0.000078</td>      <td>0.000016</td>      <td>-0.000132</td>      <td>-0.000012</td>      <td>-0.000029</td>      <td>0.000011</td>      <td>-0.000044</td>      <td>-0.000041</td>      <td>-0.000041</td>      <td>0.000027</td>      <td>-0.000017</td>      <td>-0.000027</td>      <td>-0.000017</td>      <td>-0.000096</td>      <td>0.000005</td>      <td>0.000026</td>      <td>0.000013</td>      <td>0.000016</td>      <td>-0.000005</td>      <td>-0.000005</td>      <td>0.000015</td>      <td>-0.000004</td>      <td>0.000011</td>      <td>0.000021</td>      <td>-0.000004</td>      <td>0.000045</td>      <td>0.000019</td>      <td>-0.000019</td>      <td>1</td>      <td>3</td>      <td>3</td>      <td>3</td>      <td>1</td>      <td>3</td>      <td>2</td>      <td>3</td>      <td>2</td>      <td>3</td>      <td>2</td>      <td>3</td>      <td>3</td>      <td>3</td>      <td>2</td>      <td>3</td>      <td>3</td>      <td>3</td>      <td>/livemsg?ad_type=WL_WK&amp;oadid=&amp;ty=web&amp;pu=0&amp;adap...</td>      <td>[WL_WK, , web, 0, 1, 201209, 116, 1, 8, bbe035...</td>      <td>24</td>      <td>1186</td>      <td>235.820461</td>    </tr>    <tr>      <th>4</th>      <td>7</td>      <td>GET</td>      <td>Dalvik/2.1.0 (Linux; U; Android 10; ELS-AN00 B...</td>      <td>/livemsg?sdtfrom=v5004&amp;ad_type=WL_WK&amp;oadid=&amp;ty...</td>      <td>NAN</td>      <td>GET /livemsg?sdtfrom=v5004&amp;ad_type=WL_WK&amp;oadid...</td>      <td>0.000529</td>      <td>0.999257</td>      <td>0.000153</td>      <td>0.000020</td>      <td>0.000021</td>      <td>0.000019</td>      <td>1.0</td>      <td>Android</td>      <td>Android</td>      <td>ELS-AN00</td>      <td>Huawei</td>      <td>ELS-AN00</td>      <td>66</td>      <td>1467</td>      <td>3</td>      <td>1704</td>      <td>1638</td>      <td>237</td>      <td>0.023016</td>      <td>0.076053</td>      <td>0.211935</td>      <td>0.431614</td>      <td>-0.013015</td>      <td>-0.028123</td>      <td>0.016032</td>      <td>-0.039645</td>      <td>0.129501</td>      <td>0.388460</td>      <td>-0.012173</td>      <td>-0.016553</td>      <td>-0.012854</td>      <td>-0.049835</td>      <td>-0.037182</td>      <td>-0.024569</td>      <td>0.615644</td>      <td>-0.116622</td>      <td>0.066164</td>      <td>-0.019455</td>      <td>-0.007366</td>      <td>-0.022974</td>      <td>-0.016564</td>      <td>-0.006819</td>      <td>-0.000959</td>      <td>-0.011903</td>      <td>-0.022060</td>      <td>-0.011351</td>      <td>0.002437</td>      <td>-0.008505</td>      <td>-0.008668</td>      <td>-0.004333</td>      <td>0.000129</td>      <td>0.535128</td>      <td>-0.294560</td>      <td>0.024614</td>      <td>-0.084538</td>      <td>-0.049889</td>      <td>-0.019040</td>      <td>-0.000086</td>      <td>-0.029467</td>      <td>-0.026259</td>      <td>-0.010166</td>      <td>0.009809</td>      <td>-0.017834</td>      <td>-0.018439</td>      <td>0.007596</td>      <td>-0.017013</td>      <td>-0.005269</td>      <td>0.002957</td>      <td>0.006733</td>      <td>-0.010658</td>      <td>-0.006394</td>      <td>-0.005429</td>      <td>0.011300</td>      <td>-0.024379</td>      <td>0.006489</td>      <td>-0.061012</td>      <td>-0.019155</td>      <td>-0.021446</td>      <td>0.021441</td>      <td>-0.001876</td>      <td>0.002968</td>      <td>-0.005974</td>      <td>2</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>5</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>3</td>      <td>5</td>      <td>/livemsg?sdtfrom=v5004&amp;ad_type=WL_WK&amp;oadid=&amp;ty...</td>      <td>[v5004, WL_WK, , web, 0, 20220209V0BT5X00, 1, ...</td>      <td>27</td>      <td>972</td>      <td>182.378599</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">find_url_filetype</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> re.search(<span class="string">r&#x27;\.[a-z]+&#x27;</span>, x).group()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;__NaN__&#x27;</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">data[<span class="string">&#x27;url_path&#x27;</span>] = data[<span class="string">&#x27;url_unquote&#x27;</span>].apply(<span class="keyword">lambda</span> x: urlparse(x)[<span class="number">2</span>])</span><br><span class="line">data[<span class="string">&#x27;url_filetype&#x27;</span>] = data[<span class="string">&#x27;url_path&#x27;</span>].apply(<span class="keyword">lambda</span> x: find_url_filetype(x))</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;url_path_len&#x27;</span>] = data[<span class="string">&#x27;url_path&#x27;</span>].apply(<span class="built_in">len</span>)</span><br><span class="line">data[<span class="string">&#x27;url_path_num&#x27;</span>] = data[<span class="string">&#x27;url_path&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(re.findall(<span class="string">&#x27;/&#x27;</span>,  x)))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;ua_short&#x27;</span>] = data[<span class="string">&#x27;user_agent&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27;/&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">data[<span class="string">&#x27;ua_first&#x27;</span>] = data[<span class="string">&#x27;user_agent&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.split(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># def strs_contains(strs, keyword):</span></span><br><span class="line"><span class="comment">#     return True if re.search(keyword, strs, re.IGNORECASE) else False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_select&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;select&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_select_from&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;select.*from&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_union&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;union&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_where&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;where&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_struts2&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;struts2&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_alert&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;alert&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_sudo&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;sudo&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_etc_passwd&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;etc.*passwd&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_dot_dot&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;%2e%2e%2f&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_dot_dot2&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;\.\./&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_javascript&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;javascript&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_shell&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;shell&#x27;))</span></span><br><span class="line"><span class="comment"># data[&#x27;url_contains_java_lang&#x27;] = data[&#x27;url_unquote&#x27;].apply(lambda x: strs_contains(x, &#x27;java.lang&#x27;))</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm([<span class="string">&#x27;method&#x27;</span>, <span class="string">&#x27;refer&#x27;</span>, <span class="string">&#x27;browser_family&#x27;</span>,<span class="string">&#x27;os_family&#x27;</span>,<span class="string">&#x27;device_family&#x27;</span>, <span class="string">&#x27;device_brand&#x27;</span>, <span class="string">&#x27;device_model&#x27;</span>,<span class="string">&#x27;url_filetype&#x27;</span>,<span class="string">&#x27;ua_short&#x27;</span>,<span class="string">&#x27;ua_first&#x27;</span>]):</span><br><span class="line">    le = LabelEncoder()</span><br><span class="line">    data[col] = le.fit_transform(data[col])</span><br></pre></td></tr></table></figure><pre><code>100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00&lt;00:00, 82.97it/s]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(data.select_dtypes(include=[<span class="string">&#x27;int&#x27;</span>,<span class="string">&#x27;float&#x27;</span>]).columns.tolist())</span><br></pre></td></tr></table></figure><pre><code>109</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">col = data.select_dtypes(include=[<span class="string">&#x27;int&#x27;</span>,<span class="string">&#x27;float&#x27;</span>]).columns.tolist()</span><br><span class="line">data = data[col]</span><br><span class="line">feature_names = [i <span class="keyword">for</span> i <span class="keyword">in</span> col <span class="keyword">if</span> i <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;label&#x27;</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train = data[data[<span class="string">&#x27;label&#x27;</span>].notnull()].reset_index(drop = <span class="literal">True</span>)</span><br><span class="line">test = data[~data[<span class="string">&#x27;label&#x27;</span>].notnull()].reset_index(drop = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_train = train[feature_names]</span><br><span class="line">y_train = train[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">x_test = test[feature_names]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train.shape</span><br></pre></td></tr></table></figure><pre><code>(33037, 107)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lgb_model</span>(<span class="params">train, target, test, k</span>):</span><br><span class="line">    feats = [f <span class="keyword">for</span> f <span class="keyword">in</span> train.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;label&#x27;</span>,  <span class="string">&#x27;url&#x27;</span>, <span class="string">&#x27;url_count&#x27;</span>]]</span><br><span class="line">    <span class="comment">#     feats=import_cols</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Current num of features:&#x27;</span>, <span class="built_in">len</span>(feats))</span><br><span class="line"></span><br><span class="line">    oof_probs = np.zeros((train.shape[<span class="number">0</span>],<span class="number">6</span>))</span><br><span class="line">    output_preds = <span class="number">0</span></span><br><span class="line">    offline_score = []</span><br><span class="line">    feature_importance_df = pd.DataFrame()</span><br><span class="line">    parameters = &#123;</span><br><span class="line">        <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.03</span>,</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;multiclass&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;multi_error&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;num_class&#x27;</span>: <span class="number">6</span>,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">31</span>,</span><br><span class="line">        <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.6</span>,</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.8</span>,</span><br><span class="line">        <span class="string">&#x27;min_data_in_leaf&#x27;</span>: <span class="number">15</span>,</span><br><span class="line">        <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;nthread&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">        <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">7</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># parameters = &#123;</span></span><br><span class="line">    <span class="comment">#         &#x27;learning_rate&#x27;: 0.1,</span></span><br><span class="line">    <span class="comment">#         &#x27;metric&#x27;: &#x27;multiclass&#x27;,</span></span><br><span class="line">    <span class="comment">#         &#x27;objective&#x27;: &#x27;multiclass&#x27;,</span></span><br><span class="line">    <span class="comment">#         &#x27;num_classes&#x27;: 6,</span></span><br><span class="line">    <span class="comment">#         &#x27;feature_fraction&#x27;: 0.75,</span></span><br><span class="line">    <span class="comment">#         &#x27;bagging_fraction&#x27;: 0.75,</span></span><br><span class="line">    <span class="comment">#         &#x27;bagging_freq&#x27;: 2,</span></span><br><span class="line">    <span class="comment">#         &#x27;n_jobs&#x27;: -1,</span></span><br><span class="line">    <span class="comment">#         &#x27;seed&#x27;: 1029,</span></span><br><span class="line">    <span class="comment">#         &#x27;max_depth&#x27;: 10,</span></span><br><span class="line">    <span class="comment">#         &#x27;num_leaves&#x27;: 100,</span></span><br><span class="line">    <span class="comment">#         &#x27;lambda_l1&#x27;: 0.5,</span></span><br><span class="line">    <span class="comment">#         &#x27;lambda_l2&#x27;: 0.8,</span></span><br><span class="line">    <span class="comment">#         &#x27;verbose&#x27;: -1</span></span><br><span class="line">    <span class="comment">#     &#125;</span></span><br><span class="line"></span><br><span class="line">    seeds = [<span class="number">2020</span>]</span><br><span class="line">    <span class="keyword">for</span> seed <span class="keyword">in</span> seeds:</span><br><span class="line">        folds = StratifiedKFold(n_splits=k, shuffle=<span class="literal">True</span>, random_state=seed)</span><br><span class="line">        <span class="keyword">for</span> i, (train_index, test_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(folds.split(train, target)):</span><br><span class="line">            train_y, test_y = target.iloc[train_index], target.iloc[test_index]</span><br><span class="line">            train_X, test_X = train[feats].iloc[train_index, :], train[feats].iloc[test_index, :]</span><br><span class="line"></span><br><span class="line">            dtrain = lgb.Dataset(train_X,</span><br><span class="line">                                 label=train_y)</span><br><span class="line">            dval = lgb.Dataset(test_X,</span><br><span class="line">                               label=test_y)</span><br><span class="line">            lgb_model = lgb.train(</span><br><span class="line">                parameters,</span><br><span class="line">                dtrain,</span><br><span class="line">                num_boost_round=<span class="number">8000</span>,</span><br><span class="line">                valid_sets=[dval],</span><br><span class="line">                <span class="comment"># feval = evalerror,</span></span><br><span class="line">                callbacks=[early_stopping(<span class="number">100</span>), log_evaluation(<span class="number">100</span>)],</span><br><span class="line">            )</span><br><span class="line">            oof_probs[test_index] = lgb_model.predict(test_X[feats], num_iteration=lgb_model.best_iteration) / <span class="built_in">len</span>(</span><br><span class="line">                seeds)</span><br><span class="line">            offline_score.append(lgb_model.best_score[<span class="string">&#x27;valid_0&#x27;</span>][<span class="string">&#x27;multi_error&#x27;</span>])</span><br><span class="line">            output_preds += lgb_model.predict(test[feats],</span><br><span class="line">                                              num_iteration=lgb_model.best_iteration) / folds.n_splits / <span class="built_in">len</span>(seeds)</span><br><span class="line">            <span class="built_in">print</span>(offline_score)</span><br><span class="line">            <span class="comment"># feature importance</span></span><br><span class="line">            fold_importance_df = pd.DataFrame()</span><br><span class="line">            fold_importance_df[<span class="string">&quot;feature&quot;</span>] = feats</span><br><span class="line">            fold_importance_df[<span class="string">&quot;importance&quot;</span>] = lgb_model.feature_importance(importance_type=<span class="string">&#x27;gain&#x27;</span>)</span><br><span class="line">            fold_importance_df[<span class="string">&quot;fold&quot;</span>] = i + <span class="number">1</span></span><br><span class="line">            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=<span class="number">0</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;OOF-MEAN-AUC:%.6f, OOF-STD-AUC:%.6f&#x27;</span> % (np.mean(offline_score), np.std(offline_score)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;feature importance:&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(feature_importance_df.groupby([<span class="string">&#x27;feature&#x27;</span>])[<span class="string">&#x27;importance&#x27;</span>].mean().sort_values(ascending=<span class="literal">False</span>).head(<span class="number">50</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_preds, oof_probs, np.mean(offline_score), feature_importance_df</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># feature_names = list(</span></span><br><span class="line"><span class="comment">#     filter(</span></span><br><span class="line"><span class="comment">#         lambda x: x not in [&#x27;id&#x27;,&#x27;label&#x27;,&#x27;url&#x27;, &#x27;url_count&#x27;,&#x27;url_query&#x27;],</span></span><br><span class="line"><span class="comment">#         train.columns))</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;开始模型训练train&#x27;</span>)</span><br><span class="line">lgb_preds, lgb_oof, lgb_score, feature_importance_df = lgb_model(train=train[feature_names],</span><br><span class="line">                                                                 target=train[<span class="string">&#x27;label&#x27;</span>],</span><br><span class="line">                                                                 test=test[feature_names], k=<span class="number">5</span>)</span><br></pre></td></tr></table></figure><pre><code>开始模型训练trainCurrent num of features: 107Training until validation scores don&#39;t improve for 100 rounds[100]valid_0&#39;s multi_error: 0.0145278[200]valid_0&#39;s multi_error: 0.0136199[300]valid_0&#39;s multi_error: 0.0134685Early stopping, best iteration is:[203]valid_0&#39;s multi_error: 0.0133172[0.013317191283292978]Training until validation scores don&#39;t improve for 100 rounds[100]valid_0&#39;s multi_error: 0.0119552Early stopping, best iteration is:[63]valid_0&#39;s multi_error: 0.0116525[0.013317191283292978, 0.011652542372881356]Training until validation scores don&#39;t improve for 100 rounds[100]valid_0&#39;s multi_error: 0.011503Early stopping, best iteration is:[92]valid_0&#39;s multi_error: 0.011503[0.013317191283292978, 0.011652542372881356, 0.011502951415165732]Training until validation scores don&#39;t improve for 100 rounds[100]valid_0&#39;s multi_error: 0.011503Early stopping, best iteration is:[55]valid_0&#39;s multi_error: 0.0107462[0.013317191283292978, 0.011652542372881356, 0.011502951415165732, 0.010746178295746934]Training until validation scores don&#39;t improve for 100 rounds[100]valid_0&#39;s multi_error: 0.011503Early stopping, best iteration is:[21]valid_0&#39;s multi_error: 0.0112002[0.013317191283292978, 0.011652542372881356, 0.011502951415165732, 0.010746178295746934, 0.011200242167398214]OOF-MEAN-AUC:0.011684, OOF-STD-AUC:0.000873feature importance:feature2                           222576.4399841                           211736.6668760                           147032.1741784                           108800.5989453                            86021.756831browser_family               62270.5018395                            39814.736140body_tfidf_0                 27317.752312body_tfidf_1                 23303.336642url_name_tfidf_2             16427.787706url_name_tfidf_14            15618.236285url_name_tfidf_3             14344.577968user_agent_len                9418.270741body_user_agent_len_diff      9103.063841user_agent_name_tfidf_2       8463.647873url_query_max_len             6510.758108user_agent_name_tfidf_5       4018.171339body_tfidf_3                  3672.676081body_tfidf_23                 3355.861338user_agent_name_tfidf_4       3142.178290url_name_tfidf_4              3127.492671url_name_tfidf_13             2886.949569url_name_tfidf_1              2849.448233url_name_tfidf_5              2676.723486user_agent_name_tfidf_7       2330.854639user_agent_name_tfidf_14      2310.199494refer_len                     2145.159362user_agent_name_tfidf_3       2103.610035body_tfidf_10                 1986.768004url_name_tfidf_6              1866.483835body_url_len_diff             1849.085455body_tfidf_6                  1846.585181body_len                      1821.060478user_agent_name_tfidf_0       1743.131918id_method_count               1659.460621user_agent_name_tfidf_13      1627.554725body_tfidf_5                  1539.584926url_name_tfidf_8              1485.817275url_name_tfidf_9              1449.335372url_name_tfidf_12             1429.778973url_name_tfidf_10             1413.196746url_name_tfidf_11             1296.268733user_agent_name_tfidf_9       1290.671890url_len                       1284.274063body_tfidf_12                 1279.712368body_tfidf_9                  1209.730761body_tfidf_8                  1161.935181url_name_tfidf_15             1158.114828url_name_tfidf_7               998.550187url_name_tfidf_0               998.193206Name: importance, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># xgb_params = &#123;&#x27;n_estimators&#x27;: 10000,</span></span><br><span class="line"><span class="comment">#               &#x27;learning_rate&#x27;: 0.03689407512484644,</span></span><br><span class="line"><span class="comment">#               &#x27;max_depth&#x27;: 8,</span></span><br><span class="line"><span class="comment">#               &#x27;objective&#x27;: &#x27;multi:softproba&#x27;,</span></span><br><span class="line"><span class="comment">#               &#x27;colsample_bytree&#x27;: 0.3723914688159835,</span></span><br><span class="line"><span class="comment">#               &#x27;subsample&#x27;: 0.780714581166012,</span></span><br><span class="line"><span class="comment">#               &#x27;eval_metric&#x27;: &#x27;mlogloss&#x27;,</span></span><br><span class="line"><span class="comment">#               &#x27;gamma&#x27;: 0,</span></span><br><span class="line"><span class="comment">#               &#x27;nthread&#x27;: 1,</span></span><br><span class="line"><span class="comment">#               &#x27;reg_lambda&#x27;: 50.0,</span></span><br><span class="line"><span class="comment">#               &#x27;random_state&#x27;: 42&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat_params = &#123;&#x27;iterations&#x27;: 8000,</span></span><br><span class="line"><span class="comment">#               &#x27;learning_rate&#x27;: 0.03429054860458741,</span></span><br><span class="line"><span class="comment">#               &#x27;reg_lambda&#x27;: 0.3242286463210283,</span></span><br><span class="line"><span class="comment">#               &#x27;subsample&#x27;: 0.9433911589913944,</span></span><br><span class="line"><span class="comment">#               &#x27;random_strength&#x27;: 22.4849972385133,</span></span><br><span class="line"><span class="comment">#               &#x27;depth&#x27;: 8,</span></span><br><span class="line"><span class="comment">#               &#x27;thread_count&#x27;: 1,</span></span><br><span class="line"><span class="comment">#               #         &#x27;min_data_in_leaf&#x27;: 4,</span></span><br><span class="line"><span class="comment">#               &#x27;leaf_estimation_iterations&#x27;: 8,</span></span><br><span class="line"><span class="comment">#               &#x27;task_type&#x27;: &quot;CPU&quot;,</span></span><br><span class="line"><span class="comment">#               &#x27;bootstrap_type&#x27;: &#x27;Bernoulli&#x27;,</span></span><br><span class="line"><span class="comment">#               &#x27;verbose&#x27;: 50,</span></span><br><span class="line"><span class="comment">#               &#x27;early_stopping_rounds&#x27;: 50,</span></span><br><span class="line"><span class="comment">#               # &#x27;eval_metric&#x27;: &#x27;AUC&#x27;,</span></span><br><span class="line"><span class="comment">#               &#x27;loss_function&#x27;:&#x27;MultiClass&#x27;</span></span><br><span class="line"><span class="comment">#               &#125;</span></span><br><span class="line"><span class="comment"># # lgb = LGBMClassifier(**lgb_params)</span></span><br><span class="line"><span class="comment"># xgb = XGBClassifier(**xgb_params)</span></span><br><span class="line"><span class="comment"># cat = CatBoostClassifier(**cat_params)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # In[18]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># def get_oof(feats, target, test, kfold, clf):</span></span><br><span class="line"><span class="comment">#     oof_preds = np.zeros((feats.shape[0],6))</span></span><br><span class="line"><span class="comment">#     sub_preds = np.zeros((test.shape[0],6))</span></span><br><span class="line"><span class="comment">#     for i, (train_idx, valid_idx) in enumerate(kfold.split(feats, target)):</span></span><br><span class="line"><span class="comment">#         train_X, train_y = feats.loc[train_idx], target.loc[train_idx]</span></span><br><span class="line"><span class="comment">#         valid_X, valid_y = feats.loc[valid_idx], target.loc[valid_idx]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         clf.fit(train_X, train_y, eval_set=[(valid_X, valid_y)], verbose=100, early_stopping_rounds=50, )</span></span><br><span class="line"><span class="comment">#         oof_preds[valid_idx] = clf.predict_proba(valid_X)</span></span><br><span class="line"><span class="comment">#         sub_preds += clf.predict_proba(test)</span></span><br><span class="line"><span class="comment">#         del train_X, train_y, valid_X, valid_y</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     evalution_result = accuracy_score(target, np.argmax(oof_preds,axis=1))</span></span><br><span class="line"><span class="comment">#     print(&#x27;*&#x27; * 10)</span></span><br><span class="line"><span class="comment">#     print(&#x27;roc auc score:&#x27;, evalution_result)</span></span><br><span class="line"><span class="comment">#     print(&#x27;*&#x27; * 20)</span></span><br><span class="line"><span class="comment">#     sub_preds_result = sub_preds / kfold.n_splits</span></span><br><span class="line"><span class="comment">#     return oof_preds, sub_preds_result</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)</span></span><br><span class="line"><span class="comment"># # 45开始</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># oof_preds_2, sub_preds_2 = get_oof(train[feature_names], train[&#x27;label&#x27;], test[feature_names], kfold, xgb)i</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># oof_preds_3, sub_preds_3 = get_oof(train[feature_names], train[&#x27;label&#x27;], test[feature_names], kfold, cat)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sub[<span class="string">&#x27;predict&#x27;</span>]=np.argmax(lgb_preds,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sub[<span class="string">&#x27;predict&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>2    8551    8280    8043    6664    4475    400Name: predict, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sub.to_csv(&#x27;E:/data/DF/Web攻击检测与分类识别/res/9-8-1.csv&#x27;)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy_score(train[<span class="string">&#x27;label&#x27;</span>],np.argmax(lgb_oof,axis=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><pre><code>0.9883161303992494</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lgb</span></span><br><span class="line">f1_score(np.argmax(lgb_oof,axis=<span class="number">1</span>),train[<span class="string">&#x27;label&#x27;</span>],average= <span class="string">&#x27;macro&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xgb</span></span><br><span class="line"><span class="comment"># f1_score(np.argmax(oof_preds_2,axis=1),train[&#x27;label&#x27;],average= &#x27;macro&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat</span></span><br><span class="line"><span class="comment"># f1_score(np.argmax(lgb_oof,axis=1),train[&#x27;label&#x27;],average= &#x27;macro&#x27;)</span></span><br></pre></td></tr></table></figure><pre><code>0.9650199644033531</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(train[<span class="string">&#x27;label&#x27;</span>],np.argmax(lgb_oof,axis=<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support         0.0       0.99      1.00      1.00      6489         1.0       0.99      0.99      0.99     14038         2.0       0.99      0.99      0.99      9939         3.0       0.94      0.93      0.94      1215         4.0       0.94      0.87      0.90       697         5.0       0.97      0.98      0.98       659    accuracy                           0.99     33037   macro avg       0.97      0.96      0.97     33037weighted avg       0.99      0.99      0.99     33037</code></pre><p>​    </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;CCF BDCI Web攻击检测与分类识别 Top8思路&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="数据挖掘" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    <category term="文本挖掘" scheme="https://du2279664786.github.io/tags/%E6%96%87%E6%9C%AC%E6%8C%96%E6%8E%98/"/>
    
  </entry>
  
  <entry>
    <title>BERT梳理</title>
    <link href="https://du2279664786.github.io/posts/31537f8b.html"/>
    <id>https://du2279664786.github.io/posts/31537f8b.html</id>
    <published>2022-11-28T14:55:10.000Z</published>
    <updated>2022-11-28T02:42:11.697Z</updated>
    
    <content type="html"><![CDATA[<p>BERT的总结与梳理</p><span id="more"></span><p>在学习BERT之前，我们需要回顾一下Transformer，看一下这篇文章：<a href="https://du2279664786.github.io/posts/55106.html">Transformer总结和梳理</a></p><p>而BERT只包括Transformer中的Encoder结构</p><h1 id="总体概览"><a href="#总体概览" class="headerlink" title="总体概览"></a>总体概览</h1><p>本文通过以下几部分来梳理BERT</p><ul><li><p>BERT的输入</p></li><li><p>BERT的结构</p></li><li><p>BERT所做的任务</p></li></ul><h1 id="BERT的输入"><a href="#BERT的输入" class="headerlink" title="BERT的输入"></a>BERT的输入</h1><p>先放一张图片来看一下BERT的输入结构:</p><p><img src="https://s2.loli.net/2022/11/27/se3KlBPxuyZzhMJ.png" alt="Snipaste_2022-11-27_17-11-50.png"></p><p>可以看出BERT的输入包括三部分:Token Embedding+Segment Embedding+Position Embedding组成</p><h2 id="Token-Embedding"><a href="#Token-Embedding" class="headerlink" title="Token Embedding"></a>Token Embedding</h2><p>Token Embedding为将原始文本转化为embedding后的结果，其shape为[N,d_model]，N为seq_lenght，d_model词向量的维度</p><h2 id="Segment-Embedding"><a href="#Segment-Embedding" class="headerlink" title="Segment Embedding"></a>Segment Embedding</h2><p>Segment Embedding为句子分割嵌入，主要用来区分前后两句话，其shape为[N,d_model]，N为seq_lenght，d_model句子嵌入的维度</p><h2 id="Position-Embedding"><a href="#Position-Embedding" class="headerlink" title="Position Embedding"></a>Position Embedding</h2><p>Position Embedding为位置嵌入，用来区分token的位置，其shape为[N,d_model],N为seq_length，d_model为位置嵌入的维度,详细的Position Embedding<a href="https://du2279664786.github.io/posts/7459.html">请点击此处查看</a></p><p>可以看出，三种Embedding之后shape是一样的，由上图可以看出，input是将三个Embedding进行相加求和，即为输入的向量</p><p><img src="https://s2.loli.net/2022/11/27/boAzwiyY7S14DOB.png" alt="Snipaste_2022-11-27_19-15-47.png"></p><p>在输入BERT前还需进行修改，添加特殊token，[CLS]表示一句话的开头，s_m和p_n分别为两句话，输⼊序列⾸标记[SEP]⽤作分类任务表示；特殊标记[SEP]⽤作区分句⼦对各⼦句。</p><h1 id="BERT的结构"><a href="#BERT的结构" class="headerlink" title="BERT的结构"></a>BERT的结构</h1><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p><img src="https://s2.loli.net/2022/11/27/MCDvQJ7wetBnl8d.jpg" alt="bert-模型结构.jpg"></p><p>由上图可以看出：BERT是由多个Trm组成，Trm为Transformer的Encoder结构，即BERT是由多个Transformer的Encoder堆叠而成</p><p>根据堆叠的层数不同，将BERT分为两类</p><pre><code>+ BERT-base：12层，768维度，12头，110百万参数+ BERT-large：24层，1024维度，16头，340百万参数</code></pre><h2 id="单个Block"><a href="#单个Block" class="headerlink" title="单个Block"></a>单个Block</h2><p>单个Block就是Transformer的Encoder,结构如下:</p><p><img src="https://s2.loli.net/2022/11/27/mfy98lKF7jYNhGu.png" alt="Snipaste_2022-11-27_20-40-47.png"></p><p>关于input输入,篇幅刚开始已经做详细介绍</p><h3 id="多头注意力与缩放点积"><a href="#多头注意力与缩放点积" class="headerlink" title="多头注意力与缩放点积"></a>多头注意力与缩放点积</h3><p><img src="https://s2.loli.net/2022/11/27/QvYcZlz8H29AmxV.png" alt="Snipaste_2022-11-27_20-44-36.png"></p><p>这里的多头注意力和缩放点积均和Transformer中的相同,缩放点积打分函数如下:</p><p><img src="https://s2.loli.net/2022/11/27/x4KVYBuPs8CaQUf.png" alt="Snipaste_2022-11-27_20-53-16.png"></p><p>具体详细解析可以<a href="https://du2279664786.github.io/posts/17366.html">点击此处查看</a></p><h3 id="ADD-amp-Norm"><a href="#ADD-amp-Norm" class="headerlink" title="ADD &amp; Norm"></a>ADD &amp; Norm</h3><p>关于ADD &amp; Norm可以<a href="https://du2279664786.github.io/posts/34541.html">点击此处</a>查看解释</p><h3 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h3><p>FeedForward是Multi-Head Attention的输出做了残差连接和Norm之后得数据，然后FeedForward做了两次线性线性变换，为的是更加深入的提取特征。<br>在每次线性变换都引入了非线性激活函数Relu，在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强，FeedForward的计算公式如下：max相当于Relu<br><img src="https://s2.loli.net/2022/11/27/BgoDL1sen7GU6Fx.png" alt="Snipaste_2022-11-27_20-57-27.png"></p><p>所以FeedForward的作用是：通过线性变换，先将数据映射到高纬度的空间再映射到低纬度的空间，提取了更深层次的特征</p><h3 id="PAD掩码"><a href="#PAD掩码" class="headerlink" title="PAD掩码"></a>PAD掩码</h3><p>对于Transformer而言，每次的输入为：[batch_size,seq_length,d_module]结构，由于句子一般是长短不一的，而输入的数据需要是固定的格式，所以要对句子进行处理。<br>通常会把每个句子按照最大长度进行补齐，所以当句子不够长时，需要进行补0操作，以保证输入数据结构的完整性<br>但是在计算注意力机制时的Softmax函数时，就会出现问题，Padding数值为0的话，仍然会影响到Softmax的计算结果，即无效数据参加了运算。<br>为了不让Padding数据产生影响，通常会将Padding数据变为负无穷，这样的话就不会影响Softmax函数了</p><h1 id="BERT的两个任务"><a href="#BERT的两个任务" class="headerlink" title="BERT的两个任务"></a>BERT的两个任务</h1><h2 id="遮蔽语言模型-MLM-训练任务"><a href="#遮蔽语言模型-MLM-训练任务" class="headerlink" title="遮蔽语言模型(MLM)训练任务"></a>遮蔽语言模型(MLM)训练任务</h2><p>遮蔽语⾔模型可描述为给定单词上下⽂序列后，当前单词出现的条件概率的乘积：</p><p><img src="https://s2.loli.net/2022/11/27/dx6AvlVQ7PD1jJK.png" alt="Snipaste_2022-11-27_21-23-41.png"></p><p>其中, $W_t$是第t个单词，$W_i^j$&#x3D;（$W_i$,$W_{i+1}$,………..$W_{j+1}$,$W_{j}$）是从第i个单词到第j个单词的子序列。</p><p>具体的表现形式如下：</p><p>在输入Embedding中会选择15%的单词进行MASK，再将其预测出来，MASK操作只会发生在Pre-training中，而在Fine-turning中不会出现，为了减少MASK对微调的影响，采用以下策略：</p><pre><code>+ 80%的词会被真正[MASK]+ 10%的次会被随机替换成其它Token+ 10%的词保持不变</code></pre><h2 id="预测下个句子-NSP-任务"><a href="#预测下个句子-NSP-任务" class="headerlink" title="预测下个句子(NSP)任务"></a>预测下个句子(NSP)任务</h2><p>从语料库中⽣成⼆值化的下⼀句句⼦预测任务。</p><p>具体的，当为每个预训练选择句⼦A和B时，B的50％的时间是跟随A的实际下⼀个句⼦，⽽50％的时间是来⾃语料库的<br>随机句⼦。</p><ul><li>input &#x3D; [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] label &#x3D; IsNext</li><li>input &#x3D; [CLS] the man [MASK] to the store [SEP] penguin [MASK] are filght ##less birds [SEP] label &#x3D; NotNext</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;BERT的总结与梳理&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
    <category term="BERT" scheme="https://du2279664786.github.io/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>基于ResNet50的口罩检测</title>
    <link href="https://du2279664786.github.io/posts/2b88de7d.html"/>
    <id>https://du2279664786.github.io/posts/2b88de7d.html</id>
    <published>2022-11-26T14:55:10.000Z</published>
    <updated>2022-11-26T12:41:01.411Z</updated>
    
    <content type="html"><![CDATA[<p>基于 ResNet50 模型的口罩佩戴检测</p><span id="more"></span><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>​在公共场所佩戴口罩，是防止新型冠状病毒传染的最主要手段，在必要的场所，每个人都必须佩戴口罩以进行自我保护。在人群相对集中的公共场所，相互之间不可避免地存在遮挡干扰，从而产生了小范围内的复杂干扰识别问题。</p><p>​如果使用单一的卷积神经网络对口罩佩戴进行识别，有可能造成提取关键特征信息时聚焦度欠缺，出现特征提取不足等问题。因此本文提出一种两渠道卷积神经网络的佩戴口罩识别方法。在卷积神经网络的基础上，通过 2 个输入渠道，分别对眼睛区域和眼睛以下的区域，进行特征提取; 最后通过基于决策层的信息融合方法，将 2 个渠道的识别结果加以融合，从而得到最终的识别结果，其平均识别准确率达到了 98.8%。经过实验验证，该方法在佩戴口罩的识别上，取得了较好的识别准确率。</p><p>​    在过去的十年间，人工智能（ArtificialIntelligence，AI）相关技术日新月异，几乎颠覆了传统的计算机视觉领域，随着机器学习、大数据处理、深度学习等重要理论的日益完善，很多十年前听起来像是天方夜谭的任务，例如智能机器人、图像识别、语音识别和个性化推荐系统等，如今已经变得触手可及，密切融入了我们的日常社会生活之中。卷积神经网络（Convolutional NeuralNetworks，CNN）是一类具有层级构造并以卷积运算为主的神经网络，是过去十年里计算机视觉领域的绝对主角。1998年，Le Cun 等人首次提出了用于手写数字图像识别的卷积神经网络 LeNet，通过梯度下降（GradientDescent）算法训练的 LeNet 模型取得了当时世界最先进的准确率，这一奠基性的尝试首次让卷积神经网络走向了全世界，并对后来人工智能技术的发展产生了深远的影响。随着2006年深度学习理论的提出，卷积神经网络对图像数据的学习能力和表达能力得到了非常广泛的关注，并且计算机硬件性能的高速更新迭代也为卷积神经网络的应用推广提供了相当有利的条件[2]。自从 2012 年 AlexNet 的横空出世以来，在现代 GPU 计算集群的支持下，VGGNet、GoogLeNet 和 ResNet 等结构更为复杂的卷积神经网络不停地在 ImageNet 等著名的大型视觉比赛中刷新记录，卷积神经网络在传统的计算机视觉领域掀起了重要的变革。得益于深度学习理论的快速发展，除了较为成熟的图像分类领域之外，图像分割、目标检测和目标追踪等关联领域同样涌现出了很多颠覆性的算法。新冠疫情防控工作中，需要在机场、车站等公共场所对人流的口罩佩戴情况实施监测，这可以被抽象为一种目标检测任务，该任务中需要检测的目标包含了口罩目标<br>（佩戴口罩）和人脸目标（未佩戴口罩），共 2 个类别，如下图所示。</p><p><img src="https://s2.loli.net/2022/11/26/lRUVOz5P8Bsq1ju.png" alt="Snipaste_2022-11-26_20-20-08.png"></p><h1 id="ResNet50-模型的介绍"><a href="#ResNet50-模型的介绍" class="headerlink" title="ResNet50 模型的介绍"></a>ResNet50 模型的介绍</h1><p>神经网络微调是一种深度迁移学习，是将具有通用的特征提取能力的预训练模型迁入到目标网络，根据不同的训练任务对预训练模型网络结构进行调整，在目标数据集上训练微调模型参数，将其转化为目标领域神经网络的一部分。文献中残差网络有效地解决了深度卷积神经网络出现的退化问题。下图为一个 bottleneck 残差单元输入与输出关系。</p><p><img src="https://s2.loli.net/2022/11/26/sRQ9rXKWDZtb6Pw.png" alt="Snipaste_2022-11-26_20-30-35.png"></p><p>​                                                               图 3-1 Ｒesidual network unit</p><p>图 3-1 中:x 表示该残差单元的输入;F(x)表示输入在经过卷积层后的残差值;H(x)表示当前残差单元的输出，其的表达式如下:<br>                        H(x)&#x3D;F(x)+x<br>设卷积神经网络的期望输出为 H’(x)，在网络训练达到较饱和准确率的情况下，接下来的学习相当于一个恒等映射学习，也就是 H(x)&#x3D;x，之后的训练目标就变成使残差 F(x)趋近于 0，随着网络加深，准确率不再下降。Bottleneck 是构成ＲesNet50 的基本单元，每个 Bottleneck 残差块由 3 个卷积层构成，图 3-1 中采用 1×1 卷积降低输入通道，最后再用 1×1 卷积恢复，从而减少计算量。输入数据在经过 3 次卷积后得出的输出数据再与输入相加得到该残差块最终输出。<br>图 3-2 为 ResNet50 预训练模型参数迁移，从 ImageNet 中迁移预训练模型，根据是否佩戴口罩修改全链接(fc)层结构、冻结不同卷积层参数，通过训练，进一步调整模型参数以适应口罩数据集。</p><p>​因为卷积神经网络在训练时参与训练的参数量对训练效率，最终准确率有很大影响。在不迁移参数的情况下，模型的性能是随着冻结参数的增多而下降的，但是训练效率会提升。因此本文将ＲesNet50 网络的部分卷积层参数替换为预训练模型参数并锁定，其余卷积层参与对口罩数据集的训练。因此在实验部分重点探究ＲesNet50 网络不同参数迁移量对识别准确率，训练耗时的影响。</p><p><img src="https://s2.loli.net/2022/11/26/wtR2LZzsPX15C7a.png" alt="Snipaste_2022-11-26_20-32-15.png"></p><h2 id="模型的建立"><a href="#模型的建立" class="headerlink" title="模型的建立"></a>模型的建立</h2><p>​模型构建采用的是 TensorFlow 的序列模型框架，首先转换图片数据作为模型的输入，之后再加载 Resnet50 网络去做微调。之后再添加两个 dense 层，两个 dence 层的 activation 分别使用的 relu 和 sigmoid。另外，在模型训练过程中，采用 dropout 以防止过拟合。编译过程采用梯度下降算法进行权重的更新迭代。</p><h2 id="模型的评估"><a href="#模型的评估" class="headerlink" title="模型的评估"></a>模型的评估</h2><p>损失函数 LOSS 如下图所示：</p><p><img src="https://s2.loli.net/2022/11/26/r8Ey3vgQxTScp9w.png" alt="Snipaste_2022-11-26_20-34-24.png"></p><p>准确率 Accuracy 如下图所示：</p><p><img src="https://s2.loli.net/2022/11/26/hcVTUzAbpwQd98s.png" alt="Snipaste_2022-11-26_20-35-20.png"></p><h1 id="最终实现的效果"><a href="#最终实现的效果" class="headerlink" title="最终实现的效果"></a>最终实现的效果</h1><p><img src="https://s2.loli.net/2022/11/26/UYL4PohJz3rWF1A.png" alt="Snipaste_2022-11-26_20-37-28.png"></p><p><img src="https://s2.loli.net/2022/11/26/31WTuCJe7jvHP68.png" alt="Snipaste_2022-11-26_20-38-04.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;基于 ResNet50 模型的口罩佩戴检测&lt;/p&gt;</summary>
    
    
    
    <category term="机器视觉" scheme="https://du2279664786.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="CV" scheme="https://du2279664786.github.io/tags/CV/"/>
    
    <category term="Resnet" scheme="https://du2279664786.github.io/tags/Resnet/"/>
    
    <category term="微调" scheme="https://du2279664786.github.io/tags/%E5%BE%AE%E8%B0%83/"/>
    
  </entry>
  
  <entry>
    <title>机器视觉-手势识别</title>
    <link href="https://du2279664786.github.io/posts/486b0f81.html"/>
    <id>https://du2279664786.github.io/posts/486b0f81.html</id>
    <published>2022-11-25T13:55:10.000Z</published>
    <updated>2022-11-26T12:55:53.786Z</updated>
    
    <content type="html"><![CDATA[<p>使用keras搭建网络进行手势识别，并查看准确率和损失函数曲线</p><span id="more"></span><h1 id="实验要求"><a href="#实验要求" class="headerlink" title="实验要求"></a>实验要求</h1><p><img src="https://s2.loli.net/2022/11/26/4E8hcfkVGP3nRjH.png" alt="实验1手势识别.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">your_name, your_code, work_counter = <span class="string">&quot;人工智能20本杜培博&quot;</span>, <span class="number">200507340136</span>, <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;我是：<span class="subst">&#123;your_name&#125;</span>，学号：<span class="subst">&#123;your_code&#125;</span>，这是我的第 <span class="subst">&#123;work_counter&#125;</span> 次作业。&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;现在是&#x27;</span>, datetime.datetime.now())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;我的机器信息如下：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(platform.uname())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n当前工作目录：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(os.getcwd())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n当前目录的文件信息如下：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(os.listdir())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;\n我安装包的情况:&#x27;</span>)</span><br><span class="line">!pip <span class="built_in">list</span></span><br></pre></td></tr></table></figure><pre><code>我是：人工智能20本杜培博，学号：200507340136，这是我的第 2 次作业。现在是 2022-10-21 16:39:19.008657我的机器信息如下：uname_result(system=&#39;Windows&#39;, node=&#39;LAPTOP-5PHEQ8I3&#39;, release=&#39;10&#39;, version=&#39;10.0.18362&#39;, machine=&#39;AMD64&#39;, processor=&#39;AMD64 Family 23 Model 24 Stepping 1, AuthenticAMD&#39;)当前工作目录：E:\jupyter\curriculum_code\da three\competition\week02当前目录的文件信息如下：[&#39;.ipynb_checkpoints&#39;, &#39;hand_gesture_dataset&#39;, &#39;Untitled.ipynb&#39;]我安装包的情况:Package                Version---------------------- -----------</code></pre><h1 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># for plotting</span></span><br><span class="line"><span class="keyword">import</span> os <span class="comment"># provides a way of using operating system dependent functionality</span></span><br><span class="line"><span class="keyword">import</span> cv2 <span class="comment">#Image handling library</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Callback,ModelCheckpoint</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential,load_model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout</span><br><span class="line"><span class="keyword">from</span> keras.wrappers.scikit_learn <span class="keyword">import</span> KerasClassifier</span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_multilabel_classification</span><br><span class="line"><span class="comment"># Import of keras model and hidden layers for our convolutional network</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Conv2D, Activation, MaxPool2D, Dense, Flatten, Dropout</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><h1 id="读取目录，遍历文件夹"><a href="#读取目录，遍历文件夹" class="headerlink" title="读取目录，遍历文件夹"></a>读取目录，遍历文件夹</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># paths for dataset</span></span><br><span class="line">data_path = <span class="string">&quot;./hand_gesture_dataset&quot;</span></span><br><span class="line">IMG_SIZE = <span class="number">40</span></span><br><span class="line">CATEGORIES = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(data_path):</span><br><span class="line">    CATEGORIES.append(i)</span><br><span class="line">CATEGORIES</span><br></pre></td></tr></table></figure><pre><code>[&#39;EIGHT&#39;, &#39;FIVE&#39;, &#39;FOUR&#39;, &#39;GOOD&#39;, &#39;NINE&#39;, &#39;OK&#39;, &#39;ONE&#39;, &#39;SEVEN&#39;, &#39;SIX&#39;, &#39;TEN&#39;, &#39;THREE&#39;, &#39;TWO&#39;]</code></pre><h1 id="将label进行编码"><a href="#将label进行编码" class="headerlink" title="将label进行编码"></a>将label进行编码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">revice</span>(<span class="params">name</span>):</span><br><span class="line">    <span class="keyword">if</span> name==<span class="string">&#x27;ONE&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;TWO&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;THREE&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;FOUR&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">4</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;FIVE&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">5</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;SIX&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">6</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;SEVEN&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">7</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;EIGHT&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">8</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;NINE&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">9</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;TEN&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">10</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;GOOD&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">11</span></span><br><span class="line">    <span class="keyword">elif</span> name==<span class="string">&#x27;OK&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">12</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="遍历读取图片data"><a href="#遍历读取图片data" class="headerlink" title="遍历读取图片data"></a>遍历读取图片data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">image_data = []</span><br><span class="line"></span><br><span class="line">data = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> category <span class="keyword">in</span> CATEGORIES:</span><br><span class="line">    path = os.path.join(data_path, category)</span><br><span class="line"><span class="comment">#     print(os.listdir(path))</span></span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> os.listdir(path):</span><br><span class="line">        img_arr = cv2.imread(os.path.join(path,img))       <span class="comment"># ,cv2.IMREAD_GRAYSCALE</span></span><br><span class="line">        image_data.append([img_arr,revice(category)])</span><br><span class="line"><span class="comment">#     data[category] =image_data</span></span><br><span class="line"><span class="comment">#     print(data.get(categorys,img_arr))</span></span><br><span class="line"></span><br><span class="line">plt.imshow(img_arr)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x19a7e675e80&gt;</code></pre><p><img src="https://s2.loli.net/2022/11/26/e4lEYjsFpOdNxZS.png" alt="Snipaste_2022-11-26_20-50-10.png"></p><p>​    </p><h1 id="打乱数据"><a href="#打乱数据" class="headerlink" title="打乱数据"></a>打乱数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shuffle the input data</span></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.shuffle(image_data)</span><br></pre></td></tr></table></figure><h1 id="将数据和label分离"><a href="#将数据和label分离" class="headerlink" title="将数据和label分离"></a>将数据和label分离</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">input_data = []</span><br><span class="line">label = []</span><br><span class="line"><span class="keyword">for</span> X, y <span class="keyword">in</span> image_data:</span><br><span class="line">    input_data.append(X)</span><br><span class="line">    label.append(y)</span><br><span class="line"><span class="comment"># input_data[:5]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># label</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="number">17</span>):</span><br><span class="line">    plt.subplot(<span class="number">4</span>,<span class="number">4</span>,i)</span><br><span class="line">    plt.imshow(image_data[i][<span class="number">0</span>], cmap=<span class="string">&#x27;hot&#x27;</span>)</span><br><span class="line">    plt.xticks([])</span><br><span class="line">    plt.yticks([])</span><br><span class="line"><span class="comment">#     plt.title(CATEGORIES[label[i]][3:])</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/11/26/e4lEYjsFpOdNxZS.png" alt="Snipaste_2022-11-26_20-50-10.png"></p><p>​    </p><h1 id="归一化数值"><a href="#归一化数值" class="headerlink" title="归一化数值"></a>归一化数值</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalizing the data</span></span><br><span class="line">input_data = np.array(input_data)</span><br><span class="line">label = np.array(label)</span><br><span class="line">input_data = input_data/<span class="number">255.0</span></span><br><span class="line">input_data.shape</span><br></pre></td></tr></table></figure><pre><code>(96252, 40, 40, 3)</code></pre><h1 id="数据分割"><a href="#数据分割" class="headerlink" title="数据分割"></a>数据分割</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_multilabel_classification</span><br><span class="line">X,y=make_multilabel_classification(n_samples=<span class="number">500</span>,n_features=<span class="number">4</span>,n_classes=<span class="number">2</span>,n_labels=<span class="number">3</span>,random_state=<span class="number">1</span>)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>array([[ 9., 12.,  6., 12.],       [ 5.,  2., 12., 22.],       [15.,  5., 12., 11.],       ...,       [ 5., 10., 15., 28.],       [ 0.,  8., 16., 27.],       [ 6.,  9., 13., 22.]])</code></pre><h1 id="独热编码"><a href="#独热编码" class="headerlink" title="独热编码"></a>独热编码</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line">label = to_categorical(label, num_classes=<span class="number">13</span>,dtype=<span class="string">&#x27;i1&#x27;</span>)</span><br><span class="line">label.shape</span><br></pre></td></tr></table></figure><pre><code>(96252, 13)</code></pre><h1 id="训练集和测试集9-1分割"><a href="#训练集和测试集9-1分割" class="headerlink" title="训练集和测试集9:1分割"></a>训练集和测试集9:1分割</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment">#X_train, X_test, y_train, y_test = train_test_split(input_data, label, test_size = 0.3, random_state=0)</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(input_data, label, test_size = <span class="number">0.10</span>, random_state=<span class="number">0</span>)</span><br><span class="line">X_train.shape,y_train.shape,X_test.shape,y_test.shape</span><br></pre></td></tr></table></figure><pre><code>((86626, 40, 40, 3), (86626, 13), (9626, 40, 40, 3), (9626, 13))</code></pre><h1 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">model = keras.models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters = <span class="number">32</span>, kernel_size = (<span class="number">3</span>,<span class="number">3</span>), input_shape = (IMG_SIZE, IMG_SIZE, <span class="number">3</span>)))</span><br><span class="line">model.add(Activation(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters = <span class="number">32</span>, kernel_size = (<span class="number">3</span>,<span class="number">3</span>)))</span><br><span class="line">model.add(Activation(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(MaxPool2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters = <span class="number">64</span>, kernel_size = (<span class="number">3</span>,<span class="number">3</span>)))</span><br><span class="line">model.add(Activation(<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(MaxPool2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>)))</span><br><span class="line">model.add(Dropout(<span class="number">0.3</span>))</span><br><span class="line"></span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(Dense(<span class="number">13</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,</span><br><span class="line">         optimizer = <span class="string">&#x27;rmsprop&#x27;</span>,</span><br><span class="line">         metrics = [<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    </span><br></pre></td></tr></table></figure><h1 id="训练模型-32batch-size-7epoch"><a href="#训练模型-32batch-size-7epoch" class="headerlink" title="训练模型:32batch_size,7epoch"></a>训练模型:32batch_size,7epoch</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, y_train, epochs = <span class="number">7</span>, batch_size=<span class="number">32</span>, validation_data=(X_test, y_test))</span><br></pre></td></tr></table></figure><pre><code>Train on 86626 samples, validate on 9626 samplesEpoch 1/786626/86626 [==============================] - 224s 3ms/step - loss: 0.5109 - accuracy: 0.8228 - val_loss: 0.2274 - val_accuracy: 0.9259Epoch 2/786626/86626 [==============================] - 234s 3ms/step - loss: 0.1164 - accuracy: 0.9630 - val_loss: 0.0445 - val_accuracy: 0.9876Epoch 3/786626/86626 [==============================] - 230s 3ms/step - loss: 0.0731 - accuracy: 0.9773 - val_loss: 0.0430 - val_accuracy: 0.9889Epoch 4/786626/86626 [==============================] - 230s 3ms/step - loss: 0.0574 - accuracy: 0.9825 - val_loss: 0.0309 - val_accuracy: 0.9913Epoch 5/786626/86626 [==============================] - 236s 3ms/step - loss: 0.0508 - accuracy: 0.9847 - val_loss: 0.0329 - val_accuracy: 0.9909Epoch 6/786626/86626 [==============================] - 238s 3ms/step - loss: 0.0468 - accuracy: 0.9863 - val_loss: 0.0197 - val_accuracy: 0.9947Epoch 7/786626/86626 [==============================] - 239s 3ms/step - loss: 0.0464 - accuracy: 0.9870 - val_loss: 0.0219 - val_accuracy: 0.9936&lt;keras.callbacks.callbacks.History at 0x19a1ead3b38&gt;</code></pre><h1 id="查看模型的网络结构"><a href="#查看模型的网络结构" class="headerlink" title="查看模型的网络结构"></a>查看模型的网络结构</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>Model: &quot;sequential_1&quot;_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 38, 38, 32)        896       _________________________________________________________________activation_1 (Activation)    (None, 38, 38, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 36, 36, 32)        9248      _________________________________________________________________activation_2 (Activation)    (None, 36, 36, 32)        0         _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 18, 18, 32)        0         _________________________________________________________________dropout_1 (Dropout)          (None, 18, 18, 32)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     _________________________________________________________________activation_3 (Activation)    (None, 16, 16, 64)        0         _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         _________________________________________________________________dropout_2 (Dropout)          (None, 8, 8, 64)          0         _________________________________________________________________flatten_1 (Flatten)          (None, 4096)              0         _________________________________________________________________dense_1 (Dense)              (None, 256)               1048832   _________________________________________________________________dense_2 (Dense)              (None, 13)                3341      =================================================================Total params: 1,080,813Trainable params: 1,080,813Non-trainable params: 0_________________________________________________________________</code></pre><h1 id="查看损失"><a href="#查看损失" class="headerlink" title="查看损失"></a>查看损失</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(model.history.history[<span class="string">&#x27;loss&#x27;</span>])</span><br><span class="line">plt.plot(model.history.history[<span class="string">&#x27;val_loss&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Model Loss&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/11/26/a1T5MzBxKvRoOrw.png" alt="output_30_0.png"></p><h1 id="查看准确率"><a href="#查看准确率" class="headerlink" title="查看准确率"></a>查看准确率</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(model.history.history[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">plt.plot(model.history.history[<span class="string">&#x27;val_accuracy&#x27;</span>])</span><br><span class="line">plt.title(<span class="string">&#x27;Model Accuracy&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://s2.loli.net/2022/11/26/rmNMpLe549qxHv7.png" alt="output_32_0.png"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;使用keras搭建网络进行手势识别，并查看准确率和损失函数曲线&lt;/p&gt;</summary>
    
    
    
    <category term="机器视觉" scheme="https://du2279664786.github.io/categories/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"/>
    
    
    <category term="CV" scheme="https://du2279664786.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>中文语义病句识别挑战</title>
    <link href="https://du2279664786.github.io/posts/3652056.html"/>
    <id>https://du2279664786.github.io/posts/3652056.html</id>
    <published>2022-11-24T14:55:10.000Z</published>
    <updated>2022-11-24T13:29:30.399Z</updated>
    
    <content type="html"><![CDATA[<p>预测句子是否是语义病句,语义错误和拼写错误、语法错误</p><span id="more"></span><h1 id="中文语义病句识别挑战赛"><a href="#中文语义病句识别挑战赛" class="headerlink" title="中文语义病句识别挑战赛"></a>中文语义病句识别挑战赛</h1><h1 id="一、赛事背景"><a href="#一、赛事背景" class="headerlink" title="一、赛事背景"></a>一、赛事背景</h1><p>近年来随着自媒体热潮的掀起，人人都是信息的生产者，互联网上文本错误的内容暴增，如何避免这些文本错误，成为了人们迫切关注的问题。因此，各大有关文本校对的比赛蜂拥而至。然而，过往的文本错误主要针对拼写错误和语法错误，这些错误对于人类来说相对简单，往往是由外国语言学习者和中文母语写作者的疏忽而产生的。对于出版、教育等一些对深层次的中文语义错误识别有需求的行业，中文语义病句的识别将会有更大的帮助。语义病句经常出现在初高中的语文考试题目中，用来衡量学生对语文知识的掌握程度，这类语义病句对于学生来说是比较困难的，对于研究也有重大意义。</p><h1 id="二、赛事任务"><a href="#二、赛事任务" class="headerlink" title="二、赛事任务"></a>二、赛事任务</h1><p>中文语义病句识别是一个二分类的问题，预测句子是否是语义病句。语义错误和拼写错误、语法错误不同，语义错误更加关注句子语义层面的合法性，语义病句例子如下表所示。</p><table><thead><tr><th>病句</th><th>解析</th></tr></thead><tbody><tr><td>英法联军烧毁并洗劫了北京圆明园。</td><td>应该先“洗劫”，再“烧毁”</td></tr><tr><td>山上的水宝贵，我们把它留给晚上来的人喝。</td><td>歧义，“晚上&#x2F;来”“晚&#x2F;上来”</td></tr><tr><td>国内彩电市场严重滞销。</td><td>“市场”不能“滞销”</td></tr></tbody></table><h1 id="三、评审规则"><a href="#三、评审规则" class="headerlink" title="三、评审规则"></a>三、评审规则</h1><h2 id="1-数据说明"><a href="#1-数据说明" class="headerlink" title="1.数据说明"></a>1.数据说明</h2><p>本次比赛使用的数据一部分来自网络上的中小学病句题库，一部分来自人工标注。每条数据包括句子id、句子标签（0：正确句子&#x2F;1：病句）、句子，以上三个字段用制表符分隔。数据格式示例如下表所示：</p><table><thead><tr><th>id</th><th>标签</th><th>句子</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>英法联军烧毁并洗劫了北京圆明园。</td></tr><tr><td>2</td><td>1</td><td>山上的水宝贵，我们把它留给晚上来的人喝。</td></tr><tr><td>3</td><td>0</td><td>国内彩电严重滞销。</td></tr></tbody></table><p>本次大赛由哈工大讯飞联合实验室提供的数据作为训练样本。训练集中病句和正确句子的比例大致7：3，要求参赛者使用且仅能使用组织方提供的训练集进行训练，不允许使用额外的人工标注的数据进行训练、更不允许将测试集的数据用于训练。此次比赛分为初赛和复赛两个阶段，两个阶段所使用的训练集相同。</p><h2 id="2-评估指标"><a href="#2-评估指标" class="headerlink" title="2.评估指标"></a>2.评估指标</h2><p>本模型依据提交的结果文件，采用针对语义病句的F1-score进行评价。</p><h1 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h1><h2 id="导入所需包"><a href="#导入所需包" class="headerlink" title="导入所需包"></a>导入所需包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> sklearn.utils.class_weight <span class="keyword">import</span> compute_class_weight</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle</span><br><span class="line"><span class="keyword">import</span> paddle <span class="keyword">as</span> P</span><br><span class="line"><span class="keyword">import</span> paddle.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> paddlenlp <span class="keyword">as</span> ppnlp</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> paddle.io <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> paddlenlp.data <span class="keyword">import</span> Stack, <span class="type">Tuple</span>, Pad</span><br><span class="line"><span class="keyword">from</span> paddlenlp.datasets <span class="keyword">import</span> MapDataset</span><br><span class="line"><span class="keyword">from</span> paddlenlp.transformers <span class="keyword">import</span> LinearDecayWithWarmup</span><br><span class="line"><span class="keyword">from</span> paddlenlp.transformers <span class="keyword">import</span> ErnieGramModel</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> paddle.fluid <span class="keyword">as</span> fluid</span><br><span class="line"><span class="keyword">import</span> paddle.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br></pre></td></tr></table></figure><h2 id="初始化所有需要的用到的参数"><a href="#初始化所有需要的用到的参数" class="headerlink" title="初始化所有需要的用到的参数"></a>初始化所有需要的用到的参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># =============================== 初始化 ========================</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Config</span>:</span><br><span class="line">    text_col = <span class="string">&#x27;text&#x27;</span></span><br><span class="line">    target_col = <span class="string">&#x27;label&#x27;</span></span><br><span class="line">    <span class="comment"># 最大长度大小</span></span><br><span class="line">    max_len = <span class="number">90</span></span><br><span class="line">    <span class="comment"># 模型运行批处理大小</span></span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    target_size = <span class="number">2</span></span><br><span class="line">    seed = <span class="number">71</span></span><br><span class="line">    n_fold = <span class="number">5</span></span><br><span class="line">    <span class="comment"># 训练过程中的最大学习率</span></span><br><span class="line">    learning_rate = <span class="number">5e-5</span></span><br><span class="line">    <span class="comment"># 训练轮次</span></span><br><span class="line">    epochs = <span class="number">5</span>  <span class="comment"># 3</span></span><br><span class="line">    <span class="comment"># 学习率预热比例</span></span><br><span class="line">    warmup_proportion = <span class="number">0.1</span></span><br><span class="line">    <span class="comment"># 权重衰减系数，类似模型正则项策略，避免模型过拟合</span></span><br><span class="line">    weight_decay = <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># model_name = &quot;ernie-gram-zh&quot;</span></span><br><span class="line">    <span class="comment"># model_name = &quot;ernie-doc-base-zh&quot;</span></span><br><span class="line">    model_name = <span class="string">&quot;ernie-1.0&quot;</span></span><br><span class="line">    print_freq = <span class="number">100</span></span><br></pre></td></tr></table></figure><h2 id="使用FGM-Fast-Gradient-Method-对抗训练过程"><a href="#使用FGM-Fast-Gradient-Method-对抗训练过程" class="headerlink" title="使用FGM(Fast Gradient Method)对抗训练过程"></a>使用FGM(Fast Gradient Method)对抗训练过程</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FGM</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;针对embedding层梯度上升干扰的对抗训练方法,Fast Gradient Method（FGM）&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model</span>):</span><br><span class="line">        self.model = model</span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attack</span>(<span class="params">self, epsilon=<span class="number">0.15</span>, emb_name=<span class="string">&#x27;embeddings&#x27;</span></span>):</span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> param.stop_gradient <span class="keyword">and</span> emb_name <span class="keyword">in</span> name:  <span class="comment"># 检验参数是否可训练及范围</span></span><br><span class="line">                self.backup[name] = param.numpy()  <span class="comment"># 备份原有参数值</span></span><br><span class="line">                grad_tensor = paddle.to_tensor(param.grad)  <span class="comment"># param.grad是个numpy对象</span></span><br><span class="line">                norm = paddle.norm(grad_tensor)  <span class="comment"># norm化</span></span><br><span class="line">                <span class="keyword">if</span> norm != <span class="number">0</span>:</span><br><span class="line">                    r_at = epsilon * grad_tensor / norm</span><br><span class="line">                    param.add(r_at)  <span class="comment"># 在原有embed值上添加向上梯度干扰</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">restore</span>(<span class="params">self, emb_name=<span class="string">&#x27;embeddings&#x27;</span></span>):</span><br><span class="line">        <span class="comment"># emb_name这个参数要换成你模型中embedding的参数名</span></span><br><span class="line">        <span class="keyword">for</span> name, param <span class="keyword">in</span> self.model.named_parameters():</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> param.stop_gradient <span class="keyword">and</span> emb_name <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">assert</span> name <span class="keyword">in</span> self.backup</span><br><span class="line">                param.set_value(self.backup[name])  <span class="comment"># 将原有embed参数还原</span></span><br><span class="line">        self.backup = &#123;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="设置随机种子，保证结果复现"><a href="#设置随机种子，保证结果复现" class="headerlink" title="设置随机种子，保证结果复现"></a>设置随机种子，保证结果复现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">seed_torch</span>(<span class="params">seed=<span class="number">42</span></span>):</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    os.environ[<span class="string">&#x27;PYTHONHASHSEED&#x27;</span>] = <span class="built_in">str</span>(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">&#x27;/home/aistudio/data/data176811/&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#39;data.xlsx&#39;, &#39;test1.csv&#39;, &#39;提交示例.csv&#39;]</code></pre><h2 id="数据的读取"><a href="#数据的读取" class="headerlink" title="数据的读取"></a>数据的读取</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CFG = Config()</span><br><span class="line">seed_torch(seed=CFG.seed)</span><br><span class="line">train = pd.read_excel(<span class="string">&#x27;data/data176811/data.xlsx&#x27;</span>)</span><br><span class="line">test = pd.read_table(<span class="string">&#x27;data/data176811/test1.csv&#x27;</span>)</span><br><span class="line"><span class="comment"># train[&#x27;text&#x27;] = train.apply(lambda row: concat_text(row), axis=1)</span></span><br><span class="line"><span class="comment"># test[&#x27;text&#x27;] = test.apply(lambda row: concat_text(row), axis=1)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train</span><br></pre></td></tr></table></figure><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>label</th>      <th>text</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>1</td>      <td>通过大力发展社区教育，使我省全民终身学习的教育体系已深入人心。</td>    </tr>    <tr>      <th>1</th>      <td>2</td>      <td>1</td>      <td>再次投入巨资的英超劲旅曼城队能否在2010-2011年度的英超联赛中夺得英超冠军，曼联、切尔...</td>    </tr>    <tr>      <th>2</th>      <td>3</td>      <td>1</td>      <td>广西居民纸质图书的阅读率偏低，手机阅读将成为了广西居民极倾向的阅读方式。</td>    </tr>    <tr>      <th>3</th>      <td>4</td>      <td>1</td>      <td>文字书写时代即将结束，预示着人与字之间最亲密的一种关系已经终结。与此同时，屏幕文化造就了另一...</td>    </tr>    <tr>      <th>4</th>      <td>5</td>      <td>1</td>      <td>安徽合力公司2006年叉车销售强劲，销售收入涨幅很有可能将超过40%以上。公司预计2006年...</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>45242</th>      <td>45244</td>      <td>0</td>      <td>进入5月以来，全国新增人感染H7N9禽流感病例呈明显下降趋势。</td>    </tr>    <tr>      <th>45243</th>      <td>45245</td>      <td>1</td>      <td>建设中国新一代天气雷达监测网，能够明显改善对热带气旋或台风登陆位置及强度预报的准确性，尤其对...</td>    </tr>    <tr>      <th>45244</th>      <td>45246</td>      <td>1</td>      <td>每当回忆起和他朝夕相处的一段生活，他那循循善诱的教导和那和蔼可亲的音容笑貌，又重新出现在我的面前。</td>    </tr>    <tr>      <th>45245</th>      <td>45247</td>      <td>1</td>      <td>8月，延安市公开拍卖35辆超编超标公务车。在拍卖过程中，多辆年份较新、行驶里程较少的公务车竞...</td>    </tr>    <tr>      <th>45246</th>      <td>45248</td>      <td>1</td>      <td>清华大学联合剑桥大学、麻省理工学院，成立低碳能源大学联盟未来交通研究中心，他们试图寻找解决北...</td>    </tr>  </tbody></table><p>45247 rows × 3 columns</p><h2 id="定义5折交叉验证"><a href="#定义5折交叉验证" class="headerlink" title="定义5折交叉验证"></a>定义5折交叉验证</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CV split</span></span><br><span class="line">folds = train.copy()</span><br><span class="line">Fold = StratifiedKFold(n_splits=CFG.n_fold, shuffle=<span class="literal">True</span>, random_state=CFG.seed)</span><br><span class="line"><span class="keyword">for</span> n, (train_index, val_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(Fold.split(folds, folds[CFG.target_col])):</span><br><span class="line">    folds.loc[val_index, <span class="string">&#x27;fold&#x27;</span>] = <span class="built_in">int</span>(n)</span><br><span class="line">folds[<span class="string">&#x27;fold&#x27;</span>] = folds[<span class="string">&#x27;fold&#x27;</span>].astype(<span class="built_in">int</span>)</span><br></pre></td></tr></table></figure><h2 id="数据预处理操作"><a href="#数据预处理操作" class="headerlink" title="数据预处理操作"></a>数据预处理操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ====================================== 数据集以及转换函数==============================</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, df</span>):</span><br><span class="line">        self.data = df.values.tolist()</span><br><span class="line">        self.texts = df[CFG.text_col]</span><br><span class="line">        self.labels = df[CFG.target_col]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        索引数据</span></span><br><span class="line"><span class="string">        :param idx:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        text = <span class="built_in">str</span>(self.texts[idx])</span><br><span class="line">        label = self.labels[idx]</span><br><span class="line">        example = &#123;<span class="string">&#x27;text&#x27;</span>: text, <span class="string">&#x27;label&#x27;</span>: label&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> example</span><br></pre></td></tr></table></figure><h2 id="将data进行Embedding"><a href="#将data进行Embedding" class="headerlink" title="将data进行Embedding"></a>将data进行Embedding</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_example</span>(<span class="params">example, tokenizer, max_seq_length=<span class="number">512</span>, is_test=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    创建Bert输入</span></span><br><span class="line"><span class="string">    ::</span></span><br><span class="line"><span class="string">        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1</span></span><br><span class="line"><span class="string">        | first sequence    | second sequence |</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        input_ids(obj:`list[int]`): The list of token ids.</span></span><br><span class="line"><span class="string">        token_type_ids(obj: `list[int]`): List of sequence pair mask.</span></span><br><span class="line"><span class="string">        label(obj:`numpy.array`, data type of int64, optional): The input label if not is_test.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    encoded_inputs = tokenizer(text=example[<span class="string">&quot;text&quot;</span>], max_seq_len=max_seq_length)</span><br><span class="line">    input_ids = encoded_inputs[<span class="string">&quot;input_ids&quot;</span>]</span><br><span class="line">    <span class="comment"># print(input_ids)</span></span><br><span class="line">    token_type_ids = encoded_inputs[<span class="string">&quot;token_type_ids&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_test:</span><br><span class="line">        label = np.array([example[<span class="string">&quot;label&quot;</span>]], dtype=<span class="string">&quot;int64&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> input_ids, token_type_ids, label</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> input_ids, token_type_ids</span><br></pre></td></tr></table></figure><h2 id="创建DataLoader"><a href="#创建DataLoader" class="headerlink" title="创建DataLoader"></a>创建DataLoader</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloader</span>(<span class="params">dataset,</span></span><br><span class="line"><span class="params">                      mode=<span class="string">&#x27;train&#x27;</span>,</span></span><br><span class="line"><span class="params">                      batch_size=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                      batchify_fn=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                      trans_fn=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> trans_fn:</span><br><span class="line">        dataset = dataset.<span class="built_in">map</span>(trans_fn)</span><br><span class="line"></span><br><span class="line">    shuffle = <span class="literal">True</span> <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span> <span class="keyword">else</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">&#x27;train&#x27;</span>:</span><br><span class="line">        batch_sampler = paddle.io.DistributedBatchSampler(</span><br><span class="line">            dataset, batch_size=batch_size, shuffle=shuffle)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        batch_sampler = paddle.io.BatchSampler(</span><br><span class="line">            dataset, batch_size=batch_size, shuffle=shuffle)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> paddle.io.DataLoader(</span><br><span class="line">        dataset=dataset,</span><br><span class="line">        batch_sampler=batch_sampler,</span><br><span class="line">        collate_fn=batchify_fn,</span><br><span class="line">        return_list=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(CFG.model_name)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.ErnieGramTokenizer.from_pretrained(CFG.model_name)</span></span><br><span class="line"><span class="comment"># tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(CFG.model_name)</span></span><br><span class="line"><span class="keyword">if</span> CFG.model_name == <span class="string">&#x27;ernie-1.0&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(<span class="string">&#x27;ernie-1.0&#x27;</span>)</span><br><span class="line"><span class="keyword">elif</span> CFG.model_name == <span class="string">&#x27;ernie-doc-base-zh&#x27;</span>:</span><br><span class="line">    tokenizer = ppnlp.transformers.ErnieDocTokenizer.from_pretrained(<span class="string">&#x27;ernie-doc-base-zh&#x27;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    tokenizer = ppnlp.transformers.ErnieGramTokenizer.from_pretrained(CFG.model_name)</span><br><span class="line">trans_func = partial(</span><br><span class="line">    convert_example,</span><br><span class="line">    tokenizer=tokenizer,</span><br><span class="line">    max_seq_length=CFG.max_len)</span><br><span class="line">batchify_fn = <span class="keyword">lambda</span> samples, fn=<span class="type">Tuple</span>(</span><br><span class="line">    Pad(axis=<span class="number">0</span>, pad_val=tokenizer.pad_token_id),  <span class="comment"># input</span></span><br><span class="line">    Pad(axis=<span class="number">0</span>, pad_val=tokenizer.pad_token_type_id),  <span class="comment"># segment</span></span><br><span class="line">    Stack(dtype=<span class="string">&quot;int64&quot;</span>)  <span class="comment"># label</span></span><br><span class="line">): [data <span class="keyword">for</span> data <span class="keyword">in</span> fn(samples)]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="评估模型"><a href="#评估模型" class="headerlink" title="评估模型"></a>评估模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ====================================== 验证与预测函数 ==============================</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@paddle.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model, criterion, metric, data_loader</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    验证函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    metric.reset()</span><br><span class="line">    losses = []</span><br><span class="line">    preds_list = []</span><br><span class="line">    labels_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> data_loader:</span><br><span class="line">        input_ids, token_type_ids, labels = batch</span><br><span class="line">        logits = model(input_ids, token_type_ids)</span><br><span class="line">        preds_list.append(np.argmax(logits.numpy(), axis=<span class="number">1</span>))</span><br><span class="line">        labels_list.append(labels)</span><br><span class="line">        loss = criterion(logits, labels)</span><br><span class="line">        losses.append(loss.numpy())</span><br><span class="line">        correct = metric.compute(logits, labels)</span><br><span class="line">        metric.update(correct)</span><br><span class="line">        accu = metric.accumulate()</span><br><span class="line">    f1_macro = f1_score(np.concatenate(preds_list, axis=<span class="number">0</span>), np.concatenate(labels_list, axis=<span class="number">0</span>), average=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;eval loss: %.5f, accu: %.5f&quot;</span> % (np.mean(losses), accu))</span><br><span class="line">    model.train()</span><br><span class="line">    metric.reset()</span><br><span class="line">    <span class="keyword">return</span> accu, f1_macro</span><br></pre></td></tr></table></figure><h2 id="模型进行预测"><a href="#模型进行预测" class="headerlink" title="模型进行预测"></a>模型进行预测</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">model, data, tokenizer, batch_size=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    预测函数</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    examples = []</span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> data:</span><br><span class="line">        input_ids, segment_ids = convert_example(</span><br><span class="line">            text,</span><br><span class="line">            tokenizer,</span><br><span class="line">            max_seq_length=CFG.max_len,</span><br><span class="line">            is_test=<span class="literal">True</span>)</span><br><span class="line">        examples.append((input_ids, segment_ids))</span><br><span class="line"></span><br><span class="line">    batchify_fn = <span class="keyword">lambda</span> samples, fn=<span class="type">Tuple</span>(</span><br><span class="line">        Pad(axis=<span class="number">0</span>, pad_val=tokenizer.pad_token_id),  <span class="comment"># input id</span></span><br><span class="line">        Pad(axis=<span class="number">0</span>, pad_val=tokenizer.pad_token_id),  <span class="comment"># segment id</span></span><br><span class="line">    ): fn(samples)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Seperates data into some batches.</span></span><br><span class="line">    batches = []</span><br><span class="line">    one_batch = []</span><br><span class="line">    <span class="keyword">for</span> example <span class="keyword">in</span> examples:</span><br><span class="line">        one_batch.append(example)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(one_batch) == batch_size:</span><br><span class="line">            batches.append(one_batch)</span><br><span class="line">            one_batch = []</span><br><span class="line">    <span class="keyword">if</span> one_batch:</span><br><span class="line">        <span class="comment"># The last batch whose size is less than the config batch_size setting.</span></span><br><span class="line">        batches.append(one_batch)</span><br><span class="line"></span><br><span class="line">    results = []</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(batches):</span><br><span class="line">        input_ids, segment_ids = batchify_fn(batch)</span><br><span class="line">        input_ids = paddle.to_tensor(input_ids)</span><br><span class="line">        segment_ids = paddle.to_tensor(segment_ids)</span><br><span class="line">        logits = model(input_ids, segment_ids)</span><br><span class="line">        probs = F.softmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">        results.append(probs.numpy())</span><br><span class="line">    <span class="keyword">return</span> np.vstack(results)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="预测数据"><a href="#预测数据" class="headerlink" title="预测数据"></a>预测数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inference</span>():</span><br><span class="line">    model_paths = [</span><br><span class="line">        <span class="string">f&#x27;<span class="subst">&#123;CFG.model_name&#125;</span>_fold0.bin&#x27;</span>,</span><br><span class="line">        <span class="string">f&#x27;<span class="subst">&#123;CFG.model_name&#125;</span>_fold1.bin&#x27;</span>,</span><br><span class="line">        <span class="string">f&#x27;<span class="subst">&#123;CFG.model_name&#125;</span>_fold2.bin&#x27;</span>,</span><br><span class="line">        <span class="string">f&#x27;<span class="subst">&#123;CFG.model_name&#125;</span>_fold3.bin&#x27;</span>,</span><br><span class="line">        <span class="string">f&#x27;<span class="subst">&#123;CFG.model_name&#125;</span>_fold4.bin&#x27;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> CFG.model_name == <span class="string">&#x27;ernie-1.0&#x27;</span>:</span><br><span class="line">        model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(CFG.model_name,</span><br><span class="line">                                                                                  num_classes=CFG.target_size)</span><br><span class="line">    <span class="keyword">elif</span> CFG.model_name == <span class="string">&#x27;ernie-doc-base-zh&#x27;</span>:</span><br><span class="line">        model = ppnlp.transformers.ErnieDocForSequenceClassification.from_pretrained(<span class="string">&#x27;ernie-doc-base-zh&#x27;</span>,</span><br><span class="line">                                                                                     num_classes=CFG.target_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">        model = ppnlp.transformers.ErnieGramForSequenceClassification.from_pretrained(CFG.model_name,</span><br><span class="line">                                                                                      num_classes=CFG.target_size)</span><br><span class="line">    <span class="comment"># model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(CFG.model_name,</span></span><br><span class="line">    <span class="comment">#                                                                           num_classes=25)</span></span><br><span class="line">    <span class="comment"># model = ppnlp.transformers.ErnieGramForSequenceClassification.from_pretrained(CFG.model_name,</span></span><br><span class="line">    <span class="comment">#                                                                               num_classes=25)</span></span><br><span class="line">    fold_preds = []</span><br><span class="line">    <span class="keyword">for</span> model_path <span class="keyword">in</span> model_paths:</span><br><span class="line">        model.load_dict(P.load(model_path))</span><br><span class="line">        pred = predict(model, test.to_dict(orient=<span class="string">&#x27;records&#x27;</span>), tokenizer, <span class="number">16</span>)</span><br><span class="line">        fold_preds.append(pred)</span><br><span class="line">    preds = np.mean(fold_preds, axis=<span class="number">0</span>)</span><br><span class="line">    np.save(<span class="string">&quot;preds.npy&quot;</span>, preds)</span><br><span class="line">    labels = np.argmax(preds, axis=<span class="number">1</span>)</span><br><span class="line">    test[<span class="string">&#x27;label&#x27;</span>] = labels</span><br><span class="line">    test[[<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;label&#x27;</span>]].to_csv(<span class="string">&#x27;paddle.csv&#x27;</span>,sep=<span class="string">&#x27;\t&#x27;</span>, index=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h2 id="训练函数"><a href="#训练函数" class="headerlink" title="训练函数"></a>训练函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>():</span><br><span class="line">    <span class="comment"># ====================================  交叉验证训练 ==========================</span></span><br><span class="line">    <span class="keyword">for</span> fold <span class="keyword">in</span> <span class="built_in">range</span>(CFG.n_fold):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;===============training fold_nth:<span class="subst">&#123;fold + <span class="number">1</span>&#125;</span>======================&quot;</span>)</span><br><span class="line">        trn_idx = folds[folds[<span class="string">&#x27;fold&#x27;</span>] != fold].index</span><br><span class="line">        val_idx = folds[folds[<span class="string">&#x27;fold&#x27;</span>] == fold].index</span><br><span class="line"></span><br><span class="line">        train_folds = folds.loc[trn_idx].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        valid_folds = folds.loc[val_idx].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 训练集的数据格式转换</span></span><br><span class="line">        train_dataset = CustomDataset(train_folds)</span><br><span class="line">        train_ds = MapDataset(train_dataset)</span><br><span class="line">        <span class="comment"># 验证集的数据转换</span></span><br><span class="line">        dev_dataset = CustomDataset(valid_folds)</span><br><span class="line">        dev_ds = MapDataset(dev_dataset)</span><br><span class="line">        <span class="comment"># print(trans_func)</span></span><br><span class="line">        train_data_loader = create_dataloader(</span><br><span class="line">            train_ds,</span><br><span class="line">            mode=<span class="string">&#x27;train&#x27;</span>,</span><br><span class="line">            batch_size=CFG.batch_size,</span><br><span class="line">            batchify_fn=batchify_fn,</span><br><span class="line">            trans_fn=trans_func)</span><br><span class="line">        dev_data_loader = create_dataloader(</span><br><span class="line">            dev_ds,</span><br><span class="line">            mode=<span class="string">&#x27;dev&#x27;</span>,</span><br><span class="line">            batch_size=CFG.batch_size,</span><br><span class="line">            batchify_fn=batchify_fn,</span><br><span class="line">            trans_fn=trans_func)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># model = ppnlp.transformers.ErnieGramForSequenceClassification.from_pretrained(CFG.model_name,</span></span><br><span class="line">        <span class="comment">#                                                                               num_classes=25)</span></span><br><span class="line">        <span class="comment"># model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(CFG.model_name,</span></span><br><span class="line">        <span class="comment">#                                                                               num_classes=25)</span></span><br><span class="line">        <span class="comment"># 选择模型</span></span><br><span class="line">        <span class="keyword">if</span> CFG.model_name == <span class="string">&#x27;ernie-1.0&#x27;</span>:</span><br><span class="line">            model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(CFG.model_name,</span><br><span class="line">                                                                                      num_classes=CFG.target_size)</span><br><span class="line">        <span class="keyword">elif</span> CFG.model_name == <span class="string">&#x27;ernie-doc-base-zh&#x27;</span>:</span><br><span class="line">            model = ppnlp.transformers.ErnieDocForSequenceClassification.from_pretrained(<span class="string">&#x27;ernie-doc-base-zh&#x27;</span>,</span><br><span class="line">                                                                                         num_classes=CFG.target_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            model = ppnlp.transformers.ErnieGramForSequenceClassification.from_pretrained(CFG.model_name,</span><br><span class="line">                                                                                          num_classes=CFG.target_size)</span><br><span class="line">        <span class="comment"># print(model)</span></span><br><span class="line">        num_training_steps = <span class="built_in">len</span>(train_data_loader) * CFG.epochs</span><br><span class="line">        lr_scheduler = LinearDecayWithWarmup(CFG.learning_rate, num_training_steps, CFG.warmup_proportion)</span><br><span class="line">        <span class="comment"># 构建优化器</span></span><br><span class="line">        optimizer = paddle.optimizer.AdamW(</span><br><span class="line">            learning_rate=lr_scheduler,</span><br><span class="line">            parameters=model.parameters(),</span><br><span class="line">            weight_decay=CFG.weight_decay,    </span><br><span class="line">            apply_decay_param_fun=<span class="keyword">lambda</span> x: x <span class="keyword">in</span> [</span><br><span class="line">                p.name <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">any</span>(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> [<span class="string">&quot;bias&quot;</span>, <span class="string">&quot;norm&quot;</span>])</span><br><span class="line">            ])</span><br><span class="line">        <span class="comment"># 定义交叉熵函数</span></span><br><span class="line">        criterion = paddle.nn.loss.CrossEntropyLoss()</span><br><span class="line">        <span class="comment"># criterion = paddle.nn.loss.CrossEntropyLoss(weight=paddle.to_tensor(np.array(weight)))</span></span><br><span class="line">        <span class="comment"># criterion = FocalLoss()</span></span><br><span class="line"></span><br><span class="line">        metric = paddle.metric.Accuracy()</span><br><span class="line"></span><br><span class="line">        global_step = <span class="number">0</span></span><br><span class="line">        best_val_acc = <span class="number">0</span></span><br><span class="line">        best_val_f1 = <span class="number">0</span></span><br><span class="line">        fgm = FGM(model)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, CFG.epochs + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_data_loader, start=<span class="number">1</span>):</span><br><span class="line">                input_ids, segment_ids, labels = batch</span><br><span class="line">                <span class="comment"># print(segment_ids.shape)</span></span><br><span class="line">                logits = model(input_ids, segment_ids)</span><br><span class="line">                <span class="comment"># probs_ = paddle.to_tensor(logits, dtype=&quot;float64&quot;)</span></span><br><span class="line">                loss = criterion(logits, labels)</span><br><span class="line">                probs = F.softmax(logits, axis=<span class="number">1</span>)</span><br><span class="line">                correct = metric.compute(probs, labels)</span><br><span class="line">                metric.update(correct)</span><br><span class="line">                acc = metric.accumulate()</span><br><span class="line">                f1_macro = f1_score(np.argmax(probs.numpy(), axis=<span class="number">1</span>), labels, average=<span class="string">&#x27;binary&#x27;</span>)</span><br><span class="line">                global_step += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> global_step % CFG.print_freq == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f,f1_macro: %.5f&quot;</span> % (</span><br><span class="line">                        global_step, epoch, step, loss, acc, f1_macro))</span><br><span class="line">                loss.backward()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 对抗训练</span></span><br><span class="line">                fgm.attack()  <span class="comment"># 在embedding上添加对抗扰动</span></span><br><span class="line">                logits_adv = model(input_ids, segment_ids)</span><br><span class="line">                loss_adv = criterion(logits_adv, labels)</span><br><span class="line">                loss_adv.backward()  <span class="comment"># 反向传播，并在正常的grad基础上，累加对抗训练的梯度</span></span><br><span class="line">                fgm.restore()  <span class="comment"># 恢复embedding参数</span></span><br><span class="line"></span><br><span class="line">                optimizer.step()</span><br><span class="line">                lr_scheduler.step()</span><br><span class="line">                optimizer.clear_grad()</span><br><span class="line">            acc, f1 = evaluate(model, criterion, metric, dev_data_loader)</span><br><span class="line">            <span class="keyword">if</span> acc &gt; best_val_acc:</span><br><span class="line">                best_val_acc = acc</span><br><span class="line">            <span class="comment">#if f1 &gt; best_val_f1:</span></span><br><span class="line">                <span class="comment">#best_val_f1 = f1</span></span><br><span class="line">                P.save(model.state_dict(), <span class="string">f&#x27;<span class="subst">&#123;CFG.model_name&#125;</span>_fold<span class="subst">&#123;fold&#125;</span>.bin&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;Best Val acc %.5f&#x27;</span> % best_val_acc)</span><br><span class="line">        <span class="keyword">del</span> model</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train()</span><br></pre></td></tr></table></figure><pre><code>===============training fold_nth:1======================[2022-11-23 19:03:33,381] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparamsW1123 19:03:33.386370   628 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 11.2W1123 19:03:33.390496   628 gpu_resources.cc:91] device: 0, cuDNN Version: 8.2.global step 100, epoch: 1, batch: 100, loss: 0.46791, acc: 0.73188,f1_macro: 0.89655global step 200, epoch: 1, batch: 200, loss: 0.65628, acc: 0.73625,f1_macro: 0.79245global step 300, epoch: 1, batch: 300, loss: 0.73637, acc: 0.74031,f1_macro: 0.79245global step 400, epoch: 1, batch: 400, loss: 0.57431, acc: 0.74234,f1_macro: 0.83636global step 500, epoch: 1, batch: 500, loss: 0.37923, acc: 0.74138,f1_macro: 0.91525global step 600, epoch: 1, batch: 600, loss: 0.38467, acc: 0.74344,f1_macro: 0.91228global step 700, epoch: 1, batch: 700, loss: 0.51419, acc: 0.74562,f1_macro: 0.78261global step 800, epoch: 1, batch: 800, loss: 0.38103, acc: 0.74750,f1_macro: 0.89286global step 900, epoch: 1, batch: 900, loss: 0.34946, acc: 0.74899,f1_macro: 0.91304global step 1000, epoch: 1, batch: 1000, loss: 0.53628, acc: 0.75106,f1_macro: 0.92308global step 1100, epoch: 1, batch: 1100, loss: 0.42920, acc: 0.75298,f1_macro: 0.91525eval loss: 0.44171, accu: 0.78530Best Val acc 0.78530global step 1200, epoch: 2, batch: 68, loss: 0.22114, acc: 0.83042,f1_macro: 0.91667global step 1300, epoch: 2, batch: 168, loss: 0.29129, acc: 0.83464,f1_macro: 0.90476global step 1400, epoch: 2, batch: 268, loss: 0.58879, acc: 0.83605,f1_macro: 0.80000global step 1500, epoch: 2, batch: 368, loss: 0.42207, acc: 0.83993,f1_macro: 0.85714global step 1600, epoch: 2, batch: 468, loss: 0.18622, acc: 0.83747,f1_macro: 0.95833global step 1700, epoch: 2, batch: 568, loss: 0.26564, acc: 0.83814,f1_macro: 0.93617global step 1800, epoch: 2, batch: 668, loss: 0.38719, acc: 0.84108,f1_macro: 0.86364global step 1900, epoch: 2, batch: 768, loss: 0.35982, acc: 0.84204,f1_macro: 0.88889global step 2000, epoch: 2, batch: 868, loss: 0.33469, acc: 0.84465,f1_macro: 0.89474global step 2100, epoch: 2, batch: 968, loss: 0.52275, acc: 0.84475,f1_macro: 0.81818global step 2200, epoch: 2, batch: 1068, loss: 0.23787, acc: 0.84574,f1_macro: 0.96552eval loss: 0.45888, accu: 0.79613Best Val acc 0.79613global step 2300, epoch: 3, batch: 36, loss: 0.04629, acc: 0.94878,f1_macro: 0.98246global step 2400, epoch: 3, batch: 136, loss: 0.19579, acc: 0.94003,f1_macro: 0.97778global step 2500, epoch: 3, batch: 236, loss: 0.03761, acc: 0.93988,f1_macro: 1.00000global step 2600, epoch: 3, batch: 336, loss: 0.21055, acc: 0.93899,f1_macro: 0.88235global step 2700, epoch: 3, batch: 436, loss: 0.07569, acc: 0.94030,f1_macro: 0.97674global step 2800, epoch: 3, batch: 536, loss: 0.17687, acc: 0.94076,f1_macro: 0.97674global step 2900, epoch: 3, batch: 636, loss: 0.19501, acc: 0.94099,f1_macro: 0.92000global step 3000, epoch: 3, batch: 736, loss: 0.25105, acc: 0.94153,f1_macro: 0.93617global step 3100, epoch: 3, batch: 836, loss: 0.09677, acc: 0.94109,f1_macro: 0.95833global step 3200, epoch: 3, batch: 936, loss: 0.12877, acc: 0.94071,f1_macro: 0.96000global step 3300, epoch: 3, batch: 1036, loss: 0.12475, acc: 0.94058,f1_macro: 0.96154eval loss: 0.50610, accu: 0.82055Best Val acc 0.82055global step 3400, epoch: 4, batch: 4, loss: 0.03767, acc: 0.98438,f1_macro: 1.00000global step 3500, epoch: 4, batch: 104, loss: 0.00521, acc: 0.98257,f1_macro: 1.00000global step 3600, epoch: 4, batch: 204, loss: 0.01782, acc: 0.98100,f1_macro: 1.00000global step 3700, epoch: 4, batch: 304, loss: 0.03877, acc: 0.97995,f1_macro: 1.00000global step 3800, epoch: 4, batch: 404, loss: 0.03308, acc: 0.98028,f1_macro: 1.00000global step 3900, epoch: 4, batch: 504, loss: 0.02905, acc: 0.98065,f1_macro: 1.00000global step 4000, epoch: 4, batch: 604, loss: 0.00859, acc: 0.97987,f1_macro: 1.00000global step 4100, epoch: 4, batch: 704, loss: 0.00564, acc: 0.97954,f1_macro: 1.00000global step 4200, epoch: 4, batch: 804, loss: 0.02505, acc: 0.97905,f1_macro: 1.00000global step 4300, epoch: 4, batch: 904, loss: 0.01213, acc: 0.97905,f1_macro: 1.00000global step 4400, epoch: 4, batch: 1004, loss: 0.30969, acc: 0.97883,f1_macro: 0.92683global step 4500, epoch: 4, batch: 1104, loss: 0.11206, acc: 0.97905,f1_macro: 0.98039eval loss: 0.65418, accu: 0.82221Best Val acc 0.82221global step 4600, epoch: 5, batch: 72, loss: 0.04641, acc: 0.99045,f1_macro: 0.98182global step 4700, epoch: 5, batch: 172, loss: 0.00841, acc: 0.99001,f1_macro: 1.00000global step 4800, epoch: 5, batch: 272, loss: 0.00417, acc: 0.99081,f1_macro: 1.00000global step 4900, epoch: 5, batch: 372, loss: 0.00382, acc: 0.98975,f1_macro: 1.00000global step 5000, epoch: 5, batch: 472, loss: 0.00420, acc: 0.99060,f1_macro: 1.00000global step 5100, epoch: 5, batch: 572, loss: 0.01085, acc: 0.99088,f1_macro: 1.00000global step 5200, epoch: 5, batch: 672, loss: 0.02734, acc: 0.99056,f1_macro: 0.97778global step 5300, epoch: 5, batch: 772, loss: 0.00837, acc: 0.99053,f1_macro: 1.00000global step 5400, epoch: 5, batch: 872, loss: 0.05018, acc: 0.99097,f1_macro: 0.97674global step 5500, epoch: 5, batch: 972, loss: 0.04738, acc: 0.99126,f1_macro: 0.97959global step 5600, epoch: 5, batch: 1072, loss: 0.01556, acc: 0.99128,f1_macro: 1.00000eval loss: 0.79161, accu: 0.82597Best Val acc 0.82597</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># if __name__ == &#x27;__main__&#x27;:</span></span><br><span class="line"><span class="comment">#     train()</span></span><br><span class="line"><span class="comment">#     inference()</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inference()</span><br></pre></td></tr></table></figure><pre><code>[2022-11-23 19:47:36,006] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams100%|██████████| 65/65 [00:02&lt;00:00, 31.80it/s]100%|██████████| 65/65 [00:01&lt;00:00, 33.80it/s]100%|██████████| 65/65 [00:01&lt;00:00, 32.83it/s]100%|██████████| 65/65 [00:01&lt;00:00, 33.76it/s]100%|██████████| 65/65 [00:01&lt;00:00, 33.67it/s]</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;预测句子是否是语义病句,语义错误和拼写错误、语法错误&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="病句识别" scheme="https://du2279664786.github.io/tags/%E7%97%85%E5%8F%A5%E8%AF%86%E5%88%AB/"/>
    
    <category term="WarmUP" scheme="https://du2279664786.github.io/tags/WarmUP/"/>
    
  </entry>
  
  <entry>
    <title>Pre-training</title>
    <link href="https://du2279664786.github.io/posts/36882.html"/>
    <id>https://du2279664786.github.io/posts/36882.html</id>
    <published>2022-11-21T12:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.773Z</updated>
    
    <content type="html"><![CDATA[<p>Pre-train BERT (Chinese language model) from scratch</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers,tokenizers</span><br><span class="line">transformers.__version__,tokenizers.__version__</span><br></pre></td></tr></table></figure><pre><code>(&#39;4.24.0&#39;, &#39;0.13.2&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tokenizers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, LineByLineTextDataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train a tokenizer</span></span><br><span class="line"></span><br><span class="line">bwpt = tokenizers.BertWordPieceTokenizer(vocab_file=<span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># filepath = &quot;../input/bert-bangla/raw_bangla_for_BERT.txt&quot;</span></span><br><span class="line">filepath = <span class="string">&quot;./train.txt&quot;</span></span><br><span class="line"></span><br><span class="line">bwpt.train(</span><br><span class="line">    files=[filepath],</span><br><span class="line">    vocab_size=<span class="number">50000</span>,</span><br><span class="line">    min_frequency=<span class="number">3</span>,</span><br><span class="line">    limit_alphabet=<span class="number">1000</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bwpt.save(<span class="string">&#x27;./训练中文的bert输出/&#x27;</span>, <span class="string">&#x27;name&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#39;./训练中文的bert输出/name-vocab.txt&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the tokenizer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vocab_file_dir = &#x27;/kaggle/input/bert-bangla/bangla-vocab.txt&#x27; </span></span><br><span class="line">vocab_file_dir = <span class="string">&#x27;./vocab.txt&#x27;</span> </span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(vocab_file_dir)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&#x27;今天晚上我要吃啵啵鱼&#x27;</span></span><br><span class="line"></span><br><span class="line">encoded_input = tokenizer.tokenize(sentence)</span><br><span class="line"><span class="built_in">print</span>(encoded_input)</span><br><span class="line"><span class="comment"># print(encoded_input[&#x27;input_ids&#x27;])</span></span><br></pre></td></tr></table></figure><pre><code>[&#39;今&#39;, &#39;天&#39;, &#39;晚&#39;, &#39;上&#39;, &#39;我&#39;, &#39;要&#39;, &#39;吃&#39;, &#39;啵&#39;, &#39;啵&#39;, &#39;鱼&#39;]C:\Users\dupeibo\Anaconda3\envs\pt\lib\site-packages\transformers\tokenization_utils_base.py:1679: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won&#39;t be possible anymore in v5. Use a model identifier or the path to a directory instead.  warnings.warn(</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">transformers has a predefined class LineByLineTextDataset()</span></span><br><span class="line"><span class="string">which reads your text line by line and converts them to tokens</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">dataset= LineByLineTextDataset(</span><br><span class="line">    tokenizer = tokenizer,</span><br><span class="line"><span class="comment">#     file_path = &#x27;/kaggle/input/bert-bangla/raw_bangla_for_BERT.txt&#x27;,</span></span><br><span class="line">    file_path = <span class="string">&#x27;./train.txt&#x27;</span>,</span><br><span class="line">    block_size = <span class="number">128</span>  <span class="comment"># maximum sequence length</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;No. of lines: &#x27;</span>, <span class="built_in">len</span>(dataset)) <span class="comment"># No of lines in your datset</span></span><br><span class="line">dataset</span><br></pre></td></tr></table></figure><pre><code>No. of lines:  24494Wall time: 10.4 s&lt;transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x2084eb42ac8&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">config = BertConfig(</span><br><span class="line">    vocab_size=<span class="number">50000</span>,</span><br><span class="line">    hidden_size=<span class="number">768</span>, </span><br><span class="line">    num_hidden_layers=<span class="number">6</span>, </span><br><span class="line">    num_attention_heads=<span class="number">12</span>,</span><br><span class="line">    max_position_embeddings=<span class="number">512</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">model = BertForMaskedLM(config)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;No of parameters: &#x27;</span>, model.num_parameters())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 做掩码机制</span></span><br><span class="line">data_collator = DataCollatorForLanguageModeling(</span><br><span class="line">    tokenizer=tokenizer, mlm=<span class="literal">True</span>, mlm_probability=<span class="number">0.15</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>No of parameters:  82556240</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&#x27;./训练中文的bert输出/&#x27;</span>,</span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">1</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    save_steps=<span class="number">10_000</span>,</span><br><span class="line">    save_total_limit=<span class="number">2</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    train_dataset=dataset,</span><br><span class="line">    prediction_loss_only=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">trainer.train()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Epoch:   0%|          | 0/1 [00:00&lt;?, ?it/s]Iteration:   0%|          | 0/6124 [00:00&lt;?, ?it/s]&#123;&quot;loss&quot;: 7.0956006441116335, &quot;learning_rate&quot;: 4.591770084911823e-05, &quot;epoch&quot;: 0.08164598301763554, &quot;step&quot;: 500&#125;&#123;&quot;loss&quot;: 6.214028715133667, &quot;learning_rate&quot;: 4.183540169823645e-05, &quot;epoch&quot;: 0.16329196603527107, &quot;step&quot;: 1000&#125;&#123;&quot;loss&quot;: 5.860672056674957, &quot;learning_rate&quot;: 3.775310254735467e-05, &quot;epoch&quot;: 0.24493794905290658, &quot;step&quot;: 1500&#125;&#123;&quot;loss&quot;: 5.599938702583313, &quot;learning_rate&quot;: 3.367080339647289e-05, &quot;epoch&quot;: 0.32658393207054215, &quot;step&quot;: 2000&#125;&#123;&quot;loss&quot;: 5.412256263256073, &quot;learning_rate&quot;: 2.958850424559112e-05, &quot;epoch&quot;: 0.4082299150881777, &quot;step&quot;: 2500&#125;&#123;&quot;loss&quot;: 5.261007954120636, &quot;learning_rate&quot;: 2.550620509470934e-05, &quot;epoch&quot;: 0.48987589810581317, &quot;step&quot;: 3000&#125;&#123;&quot;loss&quot;: 5.095327672958374, &quot;learning_rate&quot;: 2.1423905943827566e-05, &quot;epoch&quot;: 0.5715218811234487, &quot;step&quot;: 3500&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 1.734160679294579e-05, &quot;epoch&quot;: 0.6531678641410843, &quot;step&quot;: 4000&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 1.3259307642064011e-05, &quot;epoch&quot;: 0.7348138471587198, &quot;step&quot;: 4500&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 9.177008491182235e-06, &quot;epoch&quot;: 0.8164598301763554, &quot;step&quot;: 5000&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 5.094709340300458e-06, &quot;epoch&quot;: 0.8981058131939909, &quot;step&quot;: 5500&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 1.0124101894186806e-06, &quot;epoch&quot;: 0.9797517962116263, &quot;step&quot;: 6000&#125;Wall time: 17min 31sTrainOutput(global_step=6124, training_loss=nan)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trainer.save_model(&#x27;./训练中文的bert输出&#x27;)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">&#x27;./&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fill_mask = pipeline(</span><br><span class="line">    <span class="string">&quot;fill-mask&quot;</span>,</span><br><span class="line">    model=model,</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fill_mask(<span class="string">&#x27;心[MASK]病&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#123;&#39;score&#39;: 0.3431047201156616,  &#39;token&#39;: 5552,  &#39;token_str&#39;: &#39;脏&#39;,  &#39;sequence&#39;: &#39;心 脏 病&#39;&#125;, &#123;&#39;score&#39;: 0.07011183351278305,  &#39;token&#39;: 2552,  &#39;token_str&#39;: &#39;心&#39;,  &#39;sequence&#39;: &#39;心 心 病&#39;&#125;, &#123;&#39;score&#39;: 0.05838495120406151,  &#39;token&#39;: 4567,  &#39;token_str&#39;: &#39;病&#39;,  &#39;sequence&#39;: &#39;心 病 病&#39;&#125;, &#123;&#39;score&#39;: 0.014283978380262852,  &#39;token&#39;: 107,  &#39;token_str&#39;: &#39;&quot;&#39;,  &#39;sequence&#39;: &#39;心 &quot; 病&#39;&#125;, &#123;&#39;score&#39;: 0.011550793424248695,  &#39;token&#39;: 7315,  &#39;token_str&#39;: &#39;闷&#39;,  &#39;sequence&#39;: &#39;心 闷 病&#39;&#125;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fill_mask(<span class="string">&#x27;怀[MASK]&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#123;&#39;score&#39;: 0.4942161738872528,  &#39;token&#39;: 2097,  &#39;token_str&#39;: &#39;孕&#39;,  &#39;sequence&#39;: &#39;怀 孕&#39;&#125;, &#123;&#39;score&#39;: 0.050573259592056274,  &#39;token&#39;: 2577,  &#39;token_str&#39;: &#39;怀&#39;,  &#39;sequence&#39;: &#39;怀 怀&#39;&#125;, &#123;&#39;score&#39;: 0.01493070088326931,  &#39;token&#39;: 107,  &#39;token_str&#39;: &#39;&quot;&#39;,  &#39;sequence&#39;: &#39;怀 &quot;&#39;&#125;, &#123;&#39;score&#39;: 0.010810167528688908,  &#39;token&#39;: 5307,  &#39;token_str&#39;: &#39;经&#39;,  &#39;sequence&#39;: &#39;怀 经&#39;&#125;, &#123;&#39;score&#39;: 0.00741073302924633,  &#39;token&#39;: 1453,  &#39;token_str&#39;: &#39;周&#39;,  &#39;sequence&#39;: &#39;怀 周&#39;&#125;]</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;Pre-train BERT (Chinese language model) from scratch&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Pre-training" scheme="https://du2279664786.github.io/tags/Pre-training/"/>
    
    <category term="BERT" scheme="https://du2279664786.github.io/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>基于BERT的文本分类</title>
    <link href="https://du2279664786.github.io/posts/20881.html"/>
    <id>https://du2279664786.github.io/posts/20881.html</id>
    <published>2022-11-21T10:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.776Z</updated>
    
    <content type="html"><![CDATA[<p>通过智能化手段识别其中是否存在“虚报、假报”的情况</p><span id="more"></span><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>企业自主填报安全生产隐患，对于将风险消除在事故萌芽阶段具有重要意义。企业在填报隐患时，往往存在不认真填报的情况，“虚报、假报”隐患内容，增大了企业监管的难度。采用大数据手段分析隐患内容，找出不切实履行主体责任的企业，向监管部门进行推送，实现精准执法，能够提高监管手段的有效性，增强企业安全责任意识。</p><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>本赛题提供企业填报隐患数据，参赛选手需通过智能化手段识别其中是否存在“虚报、假报”的情况。</p><h1 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h1><p>本赛题数据集为脱敏后的企业填报自查隐患记录,数据说明如下：</p><ul><li><p>训练集数据包含“【id、level_1（一级标准）、level_2（二级标准）、level_3（三级标准）、level_4（四级标准）、content（隐患内容）和label（标签）】”共7个字段。<br>其中“id”为主键，无业务意义；“一级标准、二级标准、三级标准、四级标准”为《深圳市安全隐患自查和巡查基本指引（2016年修订版）》规定的排查指引，一级标准对应不同隐患类型，二至四级标准是对一级标准的细化，企业自主上报隐患时，根据不同类型隐患的四级标准开展隐患自查工作；“隐患内容”为企业上报的具体隐患；“标签”标识的是该条隐患的合格性，“1”表示隐患填报不合格，“0”表示隐患填报合格。</p></li><li><p>预测结果文件results.csv</p></li></ul><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669078534834.png" alt="1669078534834"></p><h1 id="评测标准"><a href="#评测标准" class="headerlink" title="评测标准"></a>评测标准</h1><p>本赛题采用F1 -score作为模型评判标准。</p><h1 id="具体实现代码如下："><a href="#具体实现代码如下：" class="headerlink" title="具体实现代码如下："></a>具体实现代码如下：</h1><h2 id="导入所以需要的包"><a href="#导入所以需要的包" class="headerlink" title="导入所以需要的包"></a>导入所以需要的包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入transformers</span></span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="comment"># from transformers import BertModel, BertTokenizer,BertConfig, AdamW, get_linear_schedule_with_warmup</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel, AutoTokenizer,AutoConfig, AdamW, get_linear_schedule_with_warmup</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常用包</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> rcParams</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> rc</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> textwrap <span class="keyword">import</span> wrap</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format=<span class="string">&#x27;retina&#x27;</span> <span class="comment"># 主题</span></span><br></pre></td></tr></table></figure><h2 id="初始化设置"><a href="#初始化设置" class="headerlink" title="初始化设置"></a>初始化设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, palette=<span class="string">&#x27;muted&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br><span class="line">HAPPY_COLORS_PALETTE = [<span class="string">&quot;#01BEFE&quot;</span>, <span class="string">&quot;#FFDD00&quot;</span>, <span class="string">&quot;#FF7D00&quot;</span>, <span class="string">&quot;#FF006D&quot;</span>, <span class="string">&quot;#ADFF02&quot;</span>, <span class="string">&quot;#8F00FF&quot;</span>]</span><br><span class="line">sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))</span><br><span class="line">rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = <span class="number">12</span>, <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">RANDOM_SEED = <span class="number">42</span></span><br><span class="line">np.random.seed(RANDOM_SEED)</span><br><span class="line">torch.manual_seed(RANDOM_SEED)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">device</span><br></pre></td></tr></table></figure><pre><code>device(type=&#39;cuda&#39;, index=0)</code></pre><h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sub=pd.read_csv(<span class="string">&#x27;./data/02企业隐患排查/sub.csv&#x27;</span>)</span><br><span class="line">test=pd.read_csv(<span class="string">&#x27;./data/02企业隐患排查/test.csv&#x27;</span>)</span><br><span class="line">train=pd.read_csv(<span class="string">&#x27;./data/02企业隐患排查/train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（二）电气安全</td>      <td>6、移动用电产品、电动工具及照明</td>      <td>1、移动使用的用电产品和I类电动工具的绝缘线，必须采用三芯(单相)或四芯(三相)多股铜芯橡套软线。</td>      <td>使用移动手动电动工具,外接线绝缘皮破损,应停止使用.</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>3、消防设施、器材和消防安全标志是否在位、完整；</td>      <td>一般</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>2、防火检查</td>      <td>6、重点工种人员以及其他员工消防知识的掌握情况；</td>      <td>消防知识要加强</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>3、消防设施、器材和消防安全标志是否在位、完整；</td>      <td>消防通道有货物摆放 清理不及时</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>4、常闭式防火门是否处于关闭状态，防火卷帘下是否堆放物品影响使用；</td>      <td>防火门打开状态</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>交通运输类（现场）—2016版</td>      <td>（一）消防安全</td>      <td>2、防火检查</td>      <td>2、安全疏散通道、疏散指示标志、应急照明和安全出口情况。</td>      <td>RB1洗地机占用堵塞安全通道</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>工业/危化品类（选项）—2016版</td>      <td>（二）仓库</td>      <td>1、一般要求</td>      <td>1、库房内储存物品应分类、分堆、限额存放。</td>      <td>未分类堆放</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>3、消防设施、器材和消防安全标志是否在位、完整；</td>      <td>消防设施、器材和消防安全标志是否在位、完整</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>商贸服务教文卫类（现场）—2016版</td>      <td>（二）电气安全</td>      <td>3、电气线路及电源插头插座</td>      <td>3、电源插座、电源插头应按规定正确接线。</td>      <td>插座随意放在电器旁边</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>商贸服务教文卫类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>6、其他消防安全情况。</td>      <td>检查中发现一瓶灭火器过期</td>    </tr>  </tbody></table></div><h2 id="查看数据的形状"><a href="#查看数据的形状" class="headerlink" title="查看数据的形状"></a>查看数据的形状</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train.shape,test.shape,sub.shape&quot;</span>,train.shape,test.shape,sub.shape)</span><br></pre></td></tr></table></figure><pre><code>train.shape,test.shape,sub.shape (12000, 7) (18000, 6) (18000, 2)</code></pre><h2 id="查看是否存在空值"><a href="#查看是否存在空值" class="headerlink" title="查看是否存在空值"></a>查看是否存在空值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[train[<span class="string">&#x27;content&#x27;</span>].isna()]</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>6193</th>      <td>6193</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>3、消防设施、器材和消防安全标志是否在位、完整；</td>      <td>NaN</td>      <td>1</td>    </tr>    <tr>      <th>9248</th>      <td>9248</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>4、常闭式防火门是否处于关闭状态，防火卷帘下是否堆放物品影响使用；</td>      <td>NaN</td>      <td>1</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train null nums&#x27;</span>)</span><br><span class="line">train.shape[<span class="number">0</span>]-train.count()</span><br></pre></td></tr></table></figure><pre><code>train null numsid         0level_1    0level_2    0level_3    0level_4    0content    2label      0dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test null nums&#x27;</span>)</span><br><span class="line">test.shape[<span class="number">0</span>]-test.count()</span><br></pre></td></tr></table></figure><pre><code>test null numsid         0level_1    0level_2    0level_3    0level_4    0content    4dtype: int64</code></pre><h2 id="查看标签的分布"><a href="#查看标签的分布" class="headerlink" title="查看标签的分布"></a>查看标签的分布</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;label&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>0    107121     1288Name: label, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sns.countplot(train.label)</span></span><br><span class="line"><span class="comment"># plt.xlabel(&#x27;label count&#x27;)</span></span><br></pre></td></tr></table></figure><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train.fillna(<span class="string">&quot;空值&quot;</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">test.fillna(<span class="string">&quot;空值&quot;</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.shape[<span class="number">0</span>]-train.count()</span><br></pre></td></tr></table></figure><pre><code>id         0level_1    0level_2    0level_3    0level_4    0content    0label      0dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;level_3&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>1、防火巡查             42252、防火检查             29112、配电箱（柜、板）          7101、作业通道              6643、电气线路及电源插头插座       497                   ... 3、安全带                 14、特种设备及操作人员管理记录       14、安全技术交底              13、停车场                 11、水库安全                1Name: level_3, Length: 153, dtype: int64</code></pre><h3 id="对训练集处理"><a href="#对训练集处理" class="headerlink" title="对训练集处理"></a>对训练集处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;level_1&#x27;</span>] = train[<span class="string">&#x27;level_1&#x27;</span>].apply(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;（&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">train[<span class="string">&#x27;level_2&#x27;</span>] = train[<span class="string">&#x27;level_2&#x27;</span>].apply(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;）&#x27;</span>)[<span class="number">1</span>])</span><br><span class="line">train[<span class="string">&#x27;level_3&#x27;</span>] = train[<span class="string">&#x27;level_3&#x27;</span>].apply(<span class="keyword">lambda</span> x:re.split(<span class="string">&#x27;[0-9]、&#x27;</span>,x)[-<span class="number">1</span>])</span><br><span class="line">train[<span class="string">&#x27;level_4&#x27;</span>] = train[<span class="string">&#x27;level_4&#x27;</span>].apply(<span class="keyword">lambda</span> x:re.split(<span class="string">&#x27;[0-9]、&#x27;</span>,x)[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="对测试集处理"><a href="#对测试集处理" class="headerlink" title="对测试集处理"></a>对测试集处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test[<span class="string">&#x27;level_1&#x27;</span>] = test[<span class="string">&#x27;level_1&#x27;</span>].apply(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;（&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">test[<span class="string">&#x27;level_2&#x27;</span>] = test[<span class="string">&#x27;level_2&#x27;</span>].apply(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;）&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">test[<span class="string">&#x27;level_3&#x27;</span>] = test[<span class="string">&#x27;level_3&#x27;</span>].apply(<span class="keyword">lambda</span> x:re.split(<span class="string">&#x27;[0-9]、&#x27;</span>,x)[-<span class="number">1</span>])</span><br><span class="line">test[<span class="string">&#x27;level_4&#x27;</span>] = test[<span class="string">&#x27;level_4&#x27;</span>].apply(<span class="keyword">lambda</span> x:re.split(<span class="string">&#x27;[0-9]、&#x27;</span>,x)[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>工业/危化品类</td>      <td>电气安全</td>      <td>移动用电产品、电动工具及照明</td>      <td>移动使用的用电产品和I类电动工具的绝缘线，必须采用三芯(单相)或四芯(三相)多股铜芯橡套软线。</td>      <td>使用移动手动电动工具,外接线绝缘皮破损,应停止使用.</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>消防设施、器材和消防安全标志是否在位、完整；</td>      <td>一般</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火检查</td>      <td>重点工种人员以及其他员工消防知识的掌握情况；</td>      <td>消防知识要加强</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>消防设施、器材和消防安全标志是否在位、完整；</td>      <td>消防通道有货物摆放 清理不及时</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>常闭式防火门是否处于关闭状态，防火卷帘下是否堆放物品影响使用；</td>      <td>防火门打开状态</td>      <td>0</td>    </tr>  </tbody></table></div><h2 id="文本拼接"><a href="#文本拼接" class="headerlink" title="文本拼接"></a>文本拼接</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;text&#x27;</span>]=train[<span class="string">&#x27;content&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+train[<span class="string">&#x27;level_1&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+train[<span class="string">&#x27;level_2&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+train[<span class="string">&#x27;level_3&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+train[<span class="string">&#x27;level_4&#x27;</span>]</span><br><span class="line">test[<span class="string">&#x27;text&#x27;</span>]=test[<span class="string">&#x27;content&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+test[<span class="string">&#x27;level_1&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+test[<span class="string">&#x27;level_2&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+test[<span class="string">&#x27;level_3&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+test[<span class="string">&#x27;level_4&#x27;</span>]</span><br><span class="line">train.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>      <th>label</th>      <th>text</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>工业/危化品类</td>      <td>电气安全</td>      <td>移动用电产品、电动工具及照明</td>      <td>移动使用的用电产品和I类电动工具的绝缘线，必须采用三芯(单相)或四芯(三相)多股铜芯橡套软线。</td>      <td>使用移动手动电动工具,外接线绝缘皮破损,应停止使用.</td>      <td>0</td>      <td>使用移动手动电动工具,外接线绝缘皮破损,应停止使用.[SEP]工业/危化品类[SEP]电气安...</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>消防设施、器材和消防安全标志是否在位、完整；</td>      <td>一般</td>      <td>1</td>      <td>一般[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]消防设施、器材和消...</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火检查</td>      <td>重点工种人员以及其他员工消防知识的掌握情况；</td>      <td>消防知识要加强</td>      <td>0</td>      <td>消防知识要加强[SEP]工业/危化品类[SEP]消防检查[SEP]防火检查[SEP]重点工种...</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>消防设施、器材和消防安全标志是否在位、完整；</td>      <td>消防通道有货物摆放 清理不及时</td>      <td>0</td>      <td>消防通道有货物摆放 清理不及时[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[...</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>常闭式防火门是否处于关闭状态，防火卷帘下是否堆放物品影响使用；</td>      <td>防火门打开状态</td>      <td>0</td>      <td>防火门打开状态[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]常闭式防...</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;text_len&#x27;</span>]=train[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="built_in">len</span>)</span><br><span class="line">train[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="built_in">len</span>).describe()<span class="comment"># 298-12=286</span></span><br></pre></td></tr></table></figure><pre><code>count    12000.000000mean        80.444500std         21.910859min         43.00000025%         66.00000050%         75.00000075%         92.000000max        298.000000Name: text, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="built_in">len</span>).describe() <span class="comment"># 520-12=518</span></span><br></pre></td></tr></table></figure><pre><code>count    18000.000000mean        80.762611std         22.719823min         43.00000025%         66.00000050%         76.00000075%         92.000000max        520.000000Name: text, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;text_len&#x27;</span>].plot(kind=<span class="string">&#x27;kde&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;AxesSubplot:ylabel=&#39;Density&#39;&gt;</code></pre><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079055167.png" alt="1669079055167"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sum</span>(train[<span class="string">&#x27;text_len&#x27;</span>]&gt;<span class="number">100</span>) <span class="comment"># text文本长度大于100的个数     1878</span></span><br><span class="line"><span class="built_in">sum</span>(train[<span class="string">&#x27;text_len&#x27;</span>]&gt;<span class="number">200</span>) <span class="comment"># text文本长度大于200的个数     11</span></span><br></pre></td></tr></table></figure><h1 id="模型的加载和配置"><a href="#模型的加载和配置" class="headerlink" title="模型的加载和配置"></a>模型的加载和配置</h1><h3 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PRE_TRAINED_MODEL_NAME = <span class="string">&#x27;bert-base-chinese&#x27;</span></span><br><span class="line"><span class="comment"># PRE_TRAINED_MODEL_NAME = &#x27;hfl/chinese-roberta-wwm-ext&#x27;</span></span><br><span class="line"><span class="comment"># PRE_TRAINED_MODEL_NAME = &#x27;hfl/chinese-roberta-wwm&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br></pre></td></tr></table></figure><pre><code>PreTrainedTokenizerFast(name_or_path=&#39;bert-base-chinese&#39;, vocab_size=21128, model_max_len=512, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens=&#123;&#39;unk_token&#39;: &#39;[UNK]&#39;, &#39;sep_token&#39;: &#39;[SEP]&#39;, &#39;pad_token&#39;: &#39;[PAD]&#39;, &#39;cls_token&#39;: &#39;[CLS]&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;&#125;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_txt = <span class="string">&#x27;今天早上9点半起床，我在学习预训练模型的使用.&#x27;</span></span><br><span class="line"><span class="built_in">len</span>(sample_txt)</span><br></pre></td></tr></table></figure><pre><code>23</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tokens = tokenizer.tokenize(sample_txt)</span><br><span class="line">token_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;文本为: <span class="subst">&#123;sample_txt&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;分词的列表为: <span class="subst">&#123;tokens&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;词对应的唯一id: <span class="subst">&#123;token_ids&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>文本为: 今天早上9点半起床，我在学习预训练模型的使用.分词的列表为: [&#39;今&#39;, &#39;天&#39;, &#39;早&#39;, &#39;上&#39;, &#39;9&#39;, &#39;点&#39;, &#39;半&#39;, &#39;起&#39;, &#39;床&#39;, &#39;，&#39;, &#39;我&#39;, &#39;在&#39;, &#39;学&#39;, &#39;习&#39;, &#39;预&#39;, &#39;训&#39;, &#39;练&#39;, &#39;模&#39;, &#39;型&#39;, &#39;的&#39;, &#39;使&#39;, &#39;用&#39;, &#39;.&#39;]词对应的唯一id: [791, 1921, 3193, 677, 130, 4157, 1288, 6629, 2414, 8024, 2769, 1762, 2110, 739, 7564, 6378, 5298, 3563, 1798, 4638, 886, 4500, 119]</code></pre><h3 id="查看特殊的Token"><a href="#查看特殊的Token" class="headerlink" title="查看特殊的Token"></a>查看特殊的Token</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.sep_token,tokenizer.sep_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[SEP]&#39;, 102)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.cls_token,tokenizer.cls_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[CLS]&#39;, 101)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pad_token,tokenizer.pad_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[PAD]&#39;, 0)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.mask_token,tokenizer.mask_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[MASK]&#39;, 103)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.unk_token,tokenizer.unk_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[UNK]&#39;, 100)</code></pre><h3 id="简单的编码测试"><a href="#简单的编码测试" class="headerlink" title="简单的编码测试"></a>简单的编码测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">encoding=tokenizer.encode_plus(</span><br><span class="line">    sample_txt,</span><br><span class="line">    <span class="comment"># sample_txt_another,</span></span><br><span class="line">    max_length=<span class="number">32</span>,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,<span class="comment"># [CLS]和[SEP]</span></span><br><span class="line">    return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">    pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">    return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">    return_tensors=<span class="string">&#x27;pt&#x27;</span>,<span class="comment"># Pytorch tensor张量</span></span><br><span class="line"></span><br><span class="line">)</span><br><span class="line">encoding</span><br></pre></td></tr></table></figure><pre><code>Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.&#123;&#39;input_ids&#39;: tensor([[ 101,  791, 1921, 3193,  677,  130, 4157, 1288, 6629, 2414, 8024, 2769,         1762, 2110,  739, 7564, 6378, 5298, 3563, 1798, 4638,  886, 4500,  119,          102,    0,    0,    0,    0,    0,    0,    0]]), &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,         0, 0, 0, 0, 0, 0, 0, 0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,         1, 0, 0, 0, 0, 0, 0, 0]])&#125;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoding[<span class="string">&#x27;attention_mask&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 0, 0, 0, 0, 0, 0, 0])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">token_lens = []</span><br><span class="line"><span class="comment"># 选的是每一句话的长度</span></span><br><span class="line"><span class="keyword">for</span> txt <span class="keyword">in</span> train.text:</span><br><span class="line"><span class="comment">#     print(txt)</span></span><br><span class="line">    tokens = tokenizer.encode(txt, max_length=<span class="number">512</span>)</span><br><span class="line">    token_lens.append(<span class="built_in">len</span>(tokens))</span><br><span class="line"><span class="comment"># token_lens</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(token_lens)</span><br><span class="line">plt.xlim([<span class="number">0</span>, <span class="number">256</span>]);</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Token count&#x27;</span>);</span><br></pre></td></tr></table></figure><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079168255.png" alt="1669079168255"></p><p>​    </p><h3 id="通过分析，长度一般都在160之内"><a href="#通过分析，长度一般都在160之内" class="headerlink" title="通过分析，长度一般都在160之内"></a>通过分析，长度一般都在160之内</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">160</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten()</span><br></pre></td></tr></table></figure><pre><code>tensor([ 101,  791, 1921, 3193,  677,  130, 4157, 1288, 6629, 2414, 8024, 2769,        1762, 2110,  739, 7564, 6378, 5298, 3563, 1798, 4638,  886, 4500,  119,         102,    0,    0,    0,    0,    0,    0,    0])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoding[<span class="string">&#x27;input_ids&#x27;</span>]</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 101,  791, 1921, 3193,  677,  130, 4157, 1288, 6629, 2414, 8024, 2769,         1762, 2110,  739, 7564, 6378, 5298, 3563, 1798, 4638,  886, 4500,  119,          102,    0,    0,    0,    0,    0,    0,    0]])</code></pre><h2 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnterpriseDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,texts,labels,tokenizer,max_len</span>):</span><br><span class="line">        self.texts=texts</span><br><span class="line">        self.labels=labels</span><br><span class="line">        self.tokenizer=tokenizer</span><br><span class="line">        self.max_len=max_len</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,item</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        item 为数据索引，迭代取第item条数据</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        text=<span class="built_in">str</span>(self.texts[item])</span><br><span class="line">        label=self.labels[item]</span><br><span class="line">        </span><br><span class="line">        encoding=self.tokenizer.encode_plus(</span><br><span class="line">            text,</span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_len,</span><br><span class="line">            return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">            pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">            return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(encoding[&#x27;input_ids&#x27;])</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;texts&#x27;</span>:text,</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>:encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten(),</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>:encoding[<span class="string">&#x27;attention_mask&#x27;</span>].flatten(),</span><br><span class="line">            <span class="comment"># toeken_type_ids:0</span></span><br><span class="line">            <span class="string">&#x27;labels&#x27;</span>:torch.tensor(label,dtype=torch.long)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h3 id="分割数据集"><a href="#分割数据集" class="headerlink" title="分割数据集"></a>分割数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_train, df_test = train_test_split(train, test_size=<span class="number">0.1</span>, random_state=RANDOM_SEED)</span><br><span class="line">df_val, df_test = train_test_split(df_test, test_size=<span class="number">0.5</span>, random_state=RANDOM_SEED)</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure><pre><code>((10800, 9), (600, 9), (600, 9))</code></pre><h3 id="创建DataLoader"><a href="#创建DataLoader" class="headerlink" title="创建DataLoader"></a>创建DataLoader</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_data_loader</span>(<span class="params">df,tokenizer,max_len,batch_size</span>):</span><br><span class="line">    ds=EnterpriseDataset(</span><br><span class="line">        texts=df[<span class="string">&#x27;text&#x27;</span>].values,</span><br><span class="line">        labels=df[<span class="string">&#x27;label&#x27;</span>].values,</span><br><span class="line">        tokenizer=tokenizer,</span><br><span class="line">        max_len=max_len</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> DataLoader(</span><br><span class="line">        ds,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line"><span class="comment">#         num_workers=4 # windows多线程</span></span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)</span><br><span class="line">val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)</span><br><span class="line">test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(train_data_loader))</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;texts&#39;: [&#39;指示标识不清楚[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]安全出口、疏散通道是否畅通，安全疏散指示标志、应急照明是否完好；&#39;,  &#39;发现本月有灭火器过期，已安排购买灭火器更换[SEP]商贸服务教文卫类[SEP]消防检查[SEP]防火检查[SEP]灭火器材配置及有效情况。&#39;,  &#39;安全出口标志灯有一个有故障，已买回安装改正。[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]安全出口、疏散通道是否畅通，安全疏散指示标志、应急照明是否完好；&#39;,  &#39;堵了消防通道[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]安全出口、疏散通道是否畅通，安全疏散指示标志、应急照明是否完好；&#39;], &#39;input_ids&#39;: tensor([[ 101, 2900, 4850, 3403, 6399,  679, 3926, 3504,  102, 2339,  689,  120,          1314, 1265, 1501, 5102,  102, 3867, 7344, 3466, 3389,  102, 7344, 4125,          2337, 3389,  102, 2128, 1059, 1139, 1366,  510, 4541, 3141, 6858, 6887,          3221, 1415, 4517, 6858, 8024, 2128, 1059, 4541, 3141, 2900, 4850, 3403,          2562,  510, 2418, 2593, 4212, 3209, 3221, 1415, 2130, 1962, 8039,  102,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0],         [ 101, 1355, 4385, 3315, 3299, 3300, 4127, 4125, 1690, 6814, 3309, 8024,          2347, 2128, 2961, 6579,  743, 4127, 4125, 1690, 3291, 2940,  102, 1555,          6588, 3302, 1218, 3136, 3152, 1310, 5102,  102, 3867, 7344, 3466, 3389,           102, 7344, 4125, 3466, 3389,  102, 4127, 4125, 1690, 3332, 6981, 5390,          1350, 3300, 3126, 2658, 1105,  511,  102,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0],         [ 101, 2128, 1059, 1139, 1366, 3403, 2562, 4128, 3300,  671,  702, 3300,          3125, 7397, 8024, 2347,  743, 1726, 2128, 6163, 3121, 3633,  511,  102,          2339,  689,  120, 1314, 1265, 1501, 5102,  102, 3867, 7344, 3466, 3389,           102, 7344, 4125, 2337, 3389,  102, 2128, 1059, 1139, 1366,  510, 4541,          3141, 6858, 6887, 3221, 1415, 4517, 6858, 8024, 2128, 1059, 4541, 3141,          2900, 4850, 3403, 2562,  510, 2418, 2593, 4212, 3209, 3221, 1415, 2130,          1962, 8039,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0],         [ 101, 1843,  749, 3867, 7344, 6858, 6887,  102, 2339,  689,  120, 1314,          1265, 1501, 5102,  102, 3867, 7344, 3466, 3389,  102, 7344, 4125, 2337,          3389,  102, 2128, 1059, 1139, 1366,  510, 4541, 3141, 6858, 6887, 3221,          1415, 4517, 6858, 8024, 2128, 1059, 4541, 3141, 2900, 4850, 3403, 2562,           510, 2418, 2593, 4212, 3209, 3221, 1415, 2130, 1962, 8039,  102,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), &#39;labels&#39;: tensor([0, 0, 0, 0])&#125;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_data_loader))</span><br><span class="line">data.keys()</span><br></pre></td></tr></table></figure><pre><code>dict_keys([&#39;texts&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;input_ids&#x27;</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;attention_mask&#x27;</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;labels&#x27;</span>].shape)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([4, 160])torch.Size([4, 160])torch.Size([4])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoding[<span class="string">&#x27;input_ids&#x27;</span>]</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 101,  791, 1921, 3193,  677,  130, 4157, 1288, 6629, 2414, 8024, 2769,         1762, 2110,  739, 7564, 6378, 5298, 3563, 1798, 4638,  886, 4500,  119,          102,    0,    0,    0,    0,    0,    0,    0]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">last_hidden_state, pooled_output = bert_model(</span><br><span class="line">    input_ids=encoding[<span class="string">&#x27;input_ids&#x27;</span>], </span><br><span class="line">    attention_mask=encoding[<span class="string">&#x27;attention_mask&#x27;</span>],</span><br><span class="line">    return_dict = <span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="查看输出结果"><a href="#查看输出结果" class="headerlink" title="查看输出结果"></a>查看输出结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">last_hidden_state[<span class="number">0</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><pre><code>torch.Size([768])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pooled_output</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.9999,  0.9998,  0.9989,  0.9629,  0.3075, -0.1866, -0.9904,  0.8628,          0.9710, -0.9993,  1.0000,  1.0000,  0.9312, -0.9394,  0.9998, -0.9999,          0.0417,  0.9999,  0.9458,  0.3190,  1.0000, -1.0000, -0.9062, -0.9048,          0.1764,  0.9983,  0.9346, -0.8122, -0.9999,  0.9996,  0.7879,  0.9999,          0.8475, -1.0000, -1.0000,  0.9413, -0.8260,  0.9889, -0.4976, -0.9857,         -0.9955, -0.9580,  0.5833, -0.9996, -0.8932,  0.8563, -1.0000, -0.9999,          0.9719,  0.9999, -0.7430, -0.9993,  0.9756, -0.9754,  0.2991,  0.8933,         -0.9991,  0.9987,  1.0000,  0.4156,  0.9992, -0.9452, -0.8020, -0.9999,          1.0000, -0.9964, -0.9900,  0.4365,  1.0000,  1.0000, -0.9400,  0.8794,          1.0000,  0.9105, -0.6616,  1.0000, -0.9999,  0.6892, -1.0000, -0.9817,          1.0000,  0.9957, -0.8844, -0.8248, -0.9921, -0.9999, -0.9998,  1.0000,          0.5228,  0.1297,  0.9932, -0.9999, -1.0000,  0.9993, -0.9996, -0.9948,         -0.9561,  0.9996, -0.5785, -0.9386, -0.2035,  0.9086, -0.9999, -0.9993,          0.9959,  0.9984,  0.6953, -0.9995,  1.0000,  0.8610, -1.0000, -0.4507,         -1.0000,  0.2384, -0.9812,  0.9998,  0.9504,  0.5421,  0.9995, -0.9998,          0.9320, -0.9941, -0.9718, -0.9910,  0.9822,  1.0000,  0.9997, -0.9990,          1.0000,  1.0000,  0.8608,  0.9964, -0.9997,  0.9799,  0.5985, -0.9098,          0.5329, -0.6345,  1.0000,  0.9872,  0.9970, -0.9719,  0.9988, -0.9933,          1.0000, -0.9999,  0.9973, -1.0000, -0.6550,  0.9996,  0.8899,  1.0000,          0.2969,  0.9999, -0.9983, -0.9991,  0.9906, -0.6590,  0.9872, -1.0000,          0.7658,  0.7876, -0.8556,  0.6304, -1.0000,  1.0000, -0.7938,  1.0000,          0.9898,  0.2216, -0.9942, -0.9969,  0.8345, -0.9998, -0.9779,  0.9914,          0.5227,  0.9992, -0.9893, -0.9889,  0.2325, -0.9887, -0.9999,  0.9885,          0.0340,  0.9284,  0.5197,  0.4143,  0.8315,  0.1585, -0.5348,  1.0000,          0.2361,  0.9985,  0.9999, -0.3446,  0.1012, -0.9924, -1.0000, -0.7542,          0.9999, -0.2807, -0.9999,  0.9490, -1.0000,  0.9906, -0.7288, -0.5263,         -0.9545, -0.9999,  0.9998, -0.9286, -0.9997, -0.5303,  0.8886,  0.5605,         -0.9989, -0.3324,  0.9804, -0.9075,  0.9905, -0.9800, -0.9946,  0.6855,         -0.9393,  0.9929,  0.9874,  1.0000,  0.9997, -0.0714, -0.9440,  1.0000,          0.1676, -1.0000,  0.5573, -0.9611,  0.8835,  0.9999, -0.9980,  0.9294,          1.0000,  0.7968,  1.0000, -0.7065, -0.9793, -0.9997,  1.0000,  0.9922,          0.9999, -0.9984, -0.9995, -0.1701, -0.5426, -1.0000, -1.0000, -0.6334,          0.9969,  0.9999, -0.1620, -0.9818, -0.9921, -0.9994,  1.0000, -0.9759,          1.0000,  0.8570, -0.7434, -0.9164,  0.9438, -0.7311, -0.9986, -0.3936,         -0.9997, -0.9650, -1.0000,  0.9433, -0.9999, -1.0000,  0.6913,  1.0000,          0.8762, -1.0000,  0.9997,  0.9764,  0.7094, -0.9294,  0.9522, -1.0000,          1.0000, -0.9965,  0.9428, -0.9972, -0.9897, -0.7680,  0.9922,  0.9999,         -0.9999, -0.9597, -0.9922, -0.9807, -0.3632,  0.9936, -0.7280,  0.4117,         -0.9498, -0.9666,  0.9545, -0.9957, -0.9970,  0.4028,  1.0000, -0.9798,          1.0000,  0.9941,  1.0000,  0.9202, -0.9942,  0.9996,  0.5352, -0.5836,         -0.8829, -0.9418,  0.9497, -0.0532,  0.6966, -0.9999,  0.9998,  0.9917,          0.9612,  0.7289,  0.0167,  0.3179,  0.9627, -0.9911,  0.9995, -0.9996,         -0.6737,  0.9991,  1.0000,  0.9932,  0.4880, -0.7488,  0.9986, -0.9961,          0.9995, -1.0000,  0.9999, -0.9940,  0.9705, -0.9970, -0.9856,  1.0000,          0.9846, -0.7932,  0.9997, -0.9386,  0.9938,  0.9738,  0.8173,  0.9913,          0.9981,  1.0000, -0.9998, -0.9918, -0.9727, -0.9987, -0.9955, -1.0000,         -0.1038, -1.0000, -0.9874, -0.9287,  0.5109, -0.9056,  0.1022,  0.7864,         -0.8197,  0.5724, -0.5905,  0.2713, -0.7239, -0.9976, -0.9844, -1.0000,         -0.9988,  0.8835,  0.9999, -0.9997,  0.9999, -0.9999, -0.9782,  0.9383,         -0.5609,  0.7721,  0.9999, -1.0000,  0.9585,  0.9987,  1.0000,  0.9960,          0.9993, -0.9741, -0.9999, -0.9989, -0.9999, -1.0000, -0.9998,  0.9343,          0.6337, -1.0000,  0.0902,  0.8980,  1.0000,  0.9964, -0.9985, -0.6136,         -0.9996, -0.8252,  0.9996, -0.0566, -1.0000,  0.9962, -0.8744,  1.0000,         -0.8865,  0.9879,  0.8897,  0.9571,  0.9823, -1.0000,  0.9145,  1.0000,          0.0365, -1.0000, -0.9985, -0.9075, -0.9998,  0.0369,  0.8120,  0.9999,         -1.0000, -0.9155, -0.9975,  0.7988,  0.9922,  0.9998,  0.9982,  0.9267,          0.9165,  0.5368,  0.1464,  0.9998,  0.4663, -0.9989,  0.9996, -0.7952,          0.4527, -1.0000,  0.9998,  0.4073,  0.9999,  0.9159, -0.5480, -0.6821,         -0.9904,  0.9938,  1.0000, -0.4229, -0.4845, -0.9981, -1.0000, -0.9861,         -0.0950, -0.4625, -0.9629, -0.9998,  0.6675, -0.5244,  1.0000,  1.0000,          0.9924, -0.9253, -0.9974,  0.9974, -0.9012,  0.9900, -0.2582, -1.0000,         -0.9919, -0.9986,  1.0000, -0.9716, -0.9262, -0.9911, -0.2593,  0.5919,         -0.9999, -0.4994, -0.9962,  0.9818,  1.0000, -0.9996,  0.9918, -0.9970,          0.7085, -0.1369,  0.8077,  0.9955, -0.3394, -0.5860, -0.6887, -0.9841,          0.9970,  0.9987, -0.9948, -0.8401,  0.9999,  0.0856,  0.9999,  0.5099,          0.9466,  0.9567,  1.0000,  0.8771,  1.0000, -0.0815,  1.0000,  0.9999,         -0.9392,  0.5744,  0.8723, -0.9686,  0.5958,  0.9822,  0.9997,  0.8854,         -0.1952, -0.9967,  0.9994,  1.0000,  1.0000, -0.3391,  0.9883, -0.4452,          0.9252,  0.4495,  0.9870,  0.3479,  0.2266,  0.9942,  0.9990, -0.9999,         -0.9999, -1.0000,  1.0000,  0.9996, -0.6637, -1.0000,  0.9999,  0.4543,          0.7471,  0.9983,  0.3772, -0.9812,  0.9853, -0.9995, -0.3404,  0.9788,          0.9867,  0.7564,  0.9995, -0.9997,  0.7990,  1.0000,  0.0752,  0.9999,          0.2912, -0.9941,  0.9970, -0.9935, -0.9995, -0.9743,  0.9991,  0.9981,         -0.9273, -0.8402,  0.9996, -0.9999,  0.9999, -0.9998,  0.9724, -0.9939,          1.0000, -0.9752, -0.9998, -0.3806,  0.8830,  0.8352, -0.8892,  1.0000,         -0.8875, -0.8107,  0.7083, -0.8909, -0.9931, -0.9630,  0.0800, -1.0000,          0.7777, -0.9611,  0.5867, -0.9947, -0.9999,  1.0000, -0.9084, -0.9414,          0.9999, -0.8838, -1.0000,  0.9549, -0.9999, -0.6522,  0.7967, -0.6850,          0.1524, -1.0000,  0.4800,  0.9999, -0.9998, -0.7089, -0.9129, -0.9864,          0.6220,  0.8855,  0.9855, -0.8651,  0.3988, -0.2548,  0.9793, -0.7212,         -0.2582, -0.9999, -0.8692, -0.6282, -0.9999, -0.9999, -1.0000,  1.0000,          0.9996,  0.9999, -0.5600,  0.7442,  0.9460,  0.9927, -0.9999,  0.4407,         -0.0461,  0.9937, -0.4887, -0.9994, -0.9198, -1.0000, -0.6905,  0.3538,         -0.7728,  0.6622,  1.0000,  0.9999, -0.9999, -0.9994, -0.9995, -0.9979,          0.9998,  0.9999,  0.9996, -0.9072, -0.5844,  0.9997,  0.9689,  0.5231,         -0.9999, -0.9981, -0.9999,  0.7505, -0.9922, -0.9986,  0.9971,  1.0000,          0.8730, -1.0000, -0.9533,  1.0000,  0.9997,  1.0000, -0.7768,  0.9999,         -0.9838,  0.9819, -0.9993,  1.0000, -1.0000,  1.0000,  0.9999,  0.9809,          0.9984, -0.9928,  0.9776, -0.9998, -0.7407,  0.9298, -0.4495, -0.9902,          0.8053,  0.9996, -0.9952,  1.0000,  0.9243, -0.2028,  0.8002,  0.9873,          0.9419, -0.6913, -0.9999,  0.8162,  0.9995,  0.9509,  1.0000,  0.9177,          0.9996, -0.9839, -0.9998,  0.9914, -0.6991, -0.7821, -0.9998,  1.0000,          1.0000, -0.9999, -0.9227,  0.7483,  0.1186,  1.0000,  0.9963,  0.9971,          0.9857,  0.3887,  0.9996, -0.9999,  0.8526, -0.9980, -0.8613,  0.9999,         -0.9899,  0.9999, -0.9981,  1.0000, -0.9858,  0.9944,  0.9989,  0.9684,         -0.9968,  1.0000,  0.8246, -0.9956, -0.8348, -0.9374, -0.9999,  0.7827]],       grad_fn=&lt;TanhBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">last_hidden_state.shape <span class="comment"># 每个token的向量表示</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([1, 32, 768])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert_model.config</span><br></pre></td></tr></table></figure><pre><code>BertConfig &#123;  &quot;_name_or_path&quot;: &quot;bert-base-chinese&quot;,  &quot;architectures&quot;: [    &quot;BertForMaskedLM&quot;  ],  &quot;attention_probs_dropout_prob&quot;: 0.1,  &quot;classifier_dropout&quot;: null,  &quot;directionality&quot;: &quot;bidi&quot;,  &quot;hidden_act&quot;: &quot;gelu&quot;,  &quot;hidden_dropout_prob&quot;: 0.1,  &quot;hidden_size&quot;: 768,  &quot;initializer_range&quot;: 0.02,  &quot;intermediate_size&quot;: 3072,  &quot;layer_norm_eps&quot;: 1e-12,  &quot;max_position_embeddings&quot;: 512,  &quot;model_type&quot;: &quot;bert&quot;,  &quot;num_attention_heads&quot;: 12,  &quot;num_hidden_layers&quot;: 12,  &quot;pad_token_id&quot;: 0,  &quot;pooler_fc_size&quot;: 768,  &quot;pooler_num_attention_heads&quot;: 12,  &quot;pooler_num_fc_layers&quot;: 3,  &quot;pooler_size_per_head&quot;: 128,  &quot;pooler_type&quot;: &quot;first_token_transform&quot;,  &quot;position_embedding_type&quot;: &quot;absolute&quot;,  &quot;transformers_version&quot;: &quot;4.24.0&quot;,  &quot;type_vocab_size&quot;: 2,  &quot;use_cache&quot;: true,  &quot;vocab_size&quot;: 21128&#125;</code></pre><h2 id="bert后面又接了一个全连接层"><a href="#bert后面又接了一个全连接层" class="headerlink" title="bert后面又接了一个全连接层"></a>bert后面又接了一个全连接层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnterpriseDangerClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(EnterpriseDangerClassifier, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br><span class="line">        self.drop = nn.Dropout(p=<span class="number">0.3</span>)</span><br><span class="line">        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) <span class="comment"># 两个类别</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        _, pooled_output = self.bert(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            return_dict = <span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        output = self.drop(pooled_output) <span class="comment"># dropout</span></span><br><span class="line">        <span class="keyword">return</span> self.out(output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class_names=[<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h2 id="将数据和模型送到CUDA"><a href="#将数据和模型送到CUDA" class="headerlink" title="将数据和模型送到CUDA"></a>将数据和模型送到CUDA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer,BertConfig, AdamW, get_linear_schedule_with_warmup</span><br><span class="line"></span><br><span class="line">model = EnterpriseDangerClassifier(<span class="built_in">len</span>(class_names))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure><pre><code>Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: [&#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.seq_relationship.weight&#39;]- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(input_ids.shape) <span class="comment"># batch size x seq length</span></span><br><span class="line"><span class="built_in">print</span>(attention_mask.shape) <span class="comment"># batch size x seq length</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([4, 160])torch.Size([4, 160])</code></pre><h2 id="得到单个batch的输出token"><a href="#得到单个batch的输出token" class="headerlink" title="得到单个batch的输出token"></a>得到单个batch的输出token</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model(input_ids, attention_mask)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.2120, -0.4050],        [ 0.3156, -0.4160],        [ 0.5127, -0.4634],        [ 0.3168,  0.5057]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.softmax(model(input_ids, attention_mask), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.4999, 0.5001],        [0.5258, 0.4742],        [0.5899, 0.4101],        [0.4575, 0.5425]], device=&#39;cuda:0&#39;, grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre><h2 id="训练模型前期配置"><a href="#训练模型前期配置" class="headerlink" title="训练模型前期配置"></a>训练模型前期配置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">EPOCHS = <span class="number">10</span> <span class="comment"># 训练轮数</span></span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>, correct_bias=<span class="literal">False</span>)</span><br><span class="line">total_steps = <span class="built_in">len</span>(train_data_loader) * EPOCHS</span><br><span class="line"></span><br><span class="line"><span class="comment"># Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,</span></span><br><span class="line"><span class="comment"># 等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳</span></span><br><span class="line">scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">  optimizer,</span><br><span class="line">  num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">  num_training_steps=total_steps</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer： 优化器</span></span><br><span class="line"><span class="comment"># num_warmup_steps：初始预热步数</span></span><br><span class="line"><span class="comment"># num_training_steps：整个训练过程的总步数</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss().to(device)</span><br></pre></td></tr></table></figure><h2 id="定义模型的训练"><a href="#定义模型的训练" class="headerlink" title="定义模型的训练"></a>定义模型的训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params"></span></span><br><span class="line"><span class="params">  model, </span></span><br><span class="line"><span class="params">  data_loader, </span></span><br><span class="line"><span class="params">  loss_fn, </span></span><br><span class="line"><span class="params">  optimizer, </span></span><br><span class="line"><span class="params">  device, </span></span><br><span class="line"><span class="params">  scheduler, </span></span><br><span class="line"><span class="params">  n_examples</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    model = model.train() <span class="comment"># train模式</span></span><br><span class="line">    losses = []</span><br><span class="line">    correct_predictions = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data_loader:</span><br><span class="line">        input_ids = d[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">        attention_mask = d[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">        targets = d[<span class="string">&quot;labels&quot;</span>].to(device)</span><br><span class="line">        outputs = model(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask</span><br><span class="line">        )</span><br><span class="line">        _, preds = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line">        correct_predictions += torch.<span class="built_in">sum</span>(preds == targets)</span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 预热学习率</span></span><br><span class="line">        scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> correct_predictions.double() / n_examples, np.mean(losses)</span><br></pre></td></tr></table></figure><h2 id="模型的评估函数"><a href="#模型的评估函数" class="headerlink" title="模型的评估函数"></a>模型的评估函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">eval_model</span>(<span class="params">model, data_loader, loss_fn, device, n_examples</span>):</span><br><span class="line">    model = model.<span class="built_in">eval</span>() <span class="comment"># 验证预测模式</span></span><br><span class="line"></span><br><span class="line">    losses = []</span><br><span class="line">    correct_predictions = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data_loader:</span><br><span class="line">            input_ids = d[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">            attention_mask = d[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">            targets = d[<span class="string">&quot;labels&quot;</span>].to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(</span><br><span class="line">                input_ids=input_ids,</span><br><span class="line">                attention_mask=attention_mask</span><br><span class="line">            )</span><br><span class="line">            _, preds = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">            correct_predictions += torch.<span class="built_in">sum</span>(preds == targets)</span><br><span class="line">            losses.append(loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> correct_predictions.double() / n_examples, np.mean(losses)</span><br></pre></td></tr></table></figure><h2 id="训练模型：10EPOCHS"><a href="#训练模型：10EPOCHS" class="headerlink" title="训练模型：10EPOCHS"></a>训练模型：10EPOCHS</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">history = defaultdict(<span class="built_in">list</span>) <span class="comment"># 记录10轮loss和acc</span></span><br><span class="line">best_accuracy = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;EPOCHS&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    train_acc, train_loss = train_epoch(</span><br><span class="line">        model,</span><br><span class="line">        train_data_loader,</span><br><span class="line">        loss_fn,</span><br><span class="line">        optimizer,</span><br><span class="line">        device,</span><br><span class="line">        scheduler,</span><br><span class="line">        <span class="built_in">len</span>(df_train)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Train loss <span class="subst">&#123;train_loss&#125;</span> accuracy <span class="subst">&#123;train_acc&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    val_acc, val_loss = eval_model(</span><br><span class="line">        model,</span><br><span class="line">        val_data_loader,</span><br><span class="line">        loss_fn,</span><br><span class="line">        device,</span><br><span class="line">        <span class="built_in">len</span>(df_val)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Val   loss <span class="subst">&#123;val_loss&#125;</span> accuracy <span class="subst">&#123;val_acc&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    history[<span class="string">&#x27;train_acc&#x27;</span>].append(train_acc)</span><br><span class="line">    history[<span class="string">&#x27;train_loss&#x27;</span>].append(train_loss)</span><br><span class="line">    history[<span class="string">&#x27;val_acc&#x27;</span>].append(val_acc)</span><br><span class="line">    history[<span class="string">&#x27;val_loss&#x27;</span>].append(val_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_acc &gt; best_accuracy:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&#x27;best_model_state.bin&#x27;</span>)</span><br><span class="line">        best_accuracy = val_acc</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/10----------Train loss 0.4988938277521757 accuracy 0.8899999999999999Val   loss 0.4194765945523977 accuracy 0.9Epoch 2/10----------Train loss 0.4967574527254328 accuracy 0.8905555555555555Val   loss 0.43736912585794924 accuracy 0.9Epoch 3/10----------Train loss 0.49347498720511795 accuracy 0.8905555555555555Val   loss 0.41818931301434836 accuracy 0.9Epoch 4/10----------Train loss 0.4900011462407807 accuracy 0.8905555555555555Val   loss 0.42409916249414287 accuracy 0.9Epoch 5/10----------Train loss 0.4952681002088098 accuracy 0.8888888888888888Val   loss 0.31909402589624125 accuracy 0.9Epoch 6/10----------Train loss 0.2478140213253425 accuracy 0.9463888888888888Val   loss 0.1787985412031412 accuracy 0.9666666666666667Epoch 7/10----------Train loss 0.17434944392257387 accuracy 0.9677777777777777Val   loss 0.15001839348037416 accuracy 0.9700000000000001Epoch 8/10----------Train loss 0.12048366091100939 accuracy 0.9775925925925926Val   loss 0.11547344802587758 accuracy 0.9783333333333334Epoch 9/10----------Train loss 0.10136666681817992 accuracy 0.9813888888888889Val   loss 0.10292303454208498 accuracy 0.9800000000000001Epoch 10/10----------Train loss 0.08721379442805402 accuracy 0.9831481481481481Val   loss 0.12598223814862042 accuracy 0.9766666666666667</code></pre><h2 id="准确率绘图"><a href="#准确率绘图" class="headerlink" title="准确率绘图"></a>准确率绘图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.plot([i.cpu() <span class="keyword">for</span> i <span class="keyword">in</span> history[<span class="string">&#x27;train_acc&#x27;</span>]], label=<span class="string">&#x27;train accuracy&#x27;</span>)</span><br><span class="line">plt.plot([i.cpu() <span class="keyword">for</span> i <span class="keyword">in</span> history[<span class="string">&#x27;val_acc&#x27;</span>]], label=<span class="string">&#x27;validation accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&#x27;Training history&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">1</span>]);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079408883.png" alt="1669079408883"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">test_acc, _ = eval_model(</span><br><span class="line">  model,</span><br><span class="line">  test_data_loader,</span><br><span class="line">  loss_fn,</span><br><span class="line">  device,</span><br><span class="line">  <span class="built_in">len</span>(df_test)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_acc.item()</span><br></pre></td></tr></table></figure><pre><code>0.9783333333333334</code></pre><h2 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_predictions</span>(<span class="params">model, data_loader</span>):</span><br><span class="line">    model = model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    raw_texts = []</span><br><span class="line">    predictions = []</span><br><span class="line">    prediction_probs = []</span><br><span class="line">    real_values = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data_loader:</span><br><span class="line">            texts = d[<span class="string">&quot;texts&quot;</span>]</span><br><span class="line">            input_ids = d[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">            attention_mask = d[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">            targets = d[<span class="string">&quot;labels&quot;</span>].to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(</span><br><span class="line">                input_ids=input_ids,</span><br><span class="line">                attention_mask=attention_mask</span><br><span class="line">            )</span><br><span class="line">            _, preds = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>) <span class="comment"># 类别</span></span><br><span class="line"></span><br><span class="line">            probs = F.softmax(outputs, dim=<span class="number">1</span>) <span class="comment"># 概率</span></span><br><span class="line"></span><br><span class="line">            raw_texts.extend(texts)</span><br><span class="line">            predictions.extend(preds)</span><br><span class="line">            prediction_probs.extend(probs)</span><br><span class="line">            real_values.extend(targets)</span><br><span class="line"></span><br><span class="line">    predictions = torch.stack(predictions).cpu()</span><br><span class="line">    prediction_probs = torch.stack(prediction_probs).cpu()</span><br><span class="line">    real_values = torch.stack(real_values).cpu()</span><br><span class="line">    <span class="keyword">return</span> raw_texts, predictions, prediction_probs, real_values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_texts, y_pred, y_pred_probs, y_test = get_predictions(</span><br><span class="line">  model,</span><br><span class="line">  test_data_loader</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred, target_names=[<span class="built_in">str</span>(label) <span class="keyword">for</span> label <span class="keyword">in</span> class_names])) <span class="comment"># 分类报告</span></span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.99      0.99      0.99       554           1       0.84      0.89      0.86        46    accuracy                           0.98       600   macro avg       0.91      0.94      0.93       600weighted avg       0.98      0.98      0.98       600</code></pre><h2 id="查看混淆矩阵"><a href="#查看混淆矩阵" class="headerlink" title="查看混淆矩阵"></a>查看混淆矩阵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_confusion_matrix</span>(<span class="params">confusion_matrix</span>):</span><br><span class="line">    hmap = sns.heatmap(confusion_matrix, annot=<span class="literal">True</span>, fmt=<span class="string">&quot;d&quot;</span>, cmap=<span class="string">&quot;Blues&quot;</span>)</span><br><span class="line">    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=<span class="number">0</span>, ha=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line">    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=<span class="number">30</span>, ha=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True label&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Predicted label&#x27;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line">df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)</span><br><span class="line">show_confusion_matrix(df_cm)</span><br></pre></td></tr></table></figure><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079467197.png" alt="1669079467197">    </p><h2 id="评估单条数据"><a href="#评估单条数据" class="headerlink" title="评估单条数据"></a>评估单条数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">idx = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">sample_text = y_texts[idx]</span><br><span class="line">true_label = y_test[idx]</span><br><span class="line">pred_df = pd.DataFrame(&#123;</span><br><span class="line">  <span class="string">&#x27;class_names&#x27;</span>: class_names,</span><br><span class="line">  <span class="string">&#x27;values&#x27;</span>: y_pred_probs[idx]</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_d</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>class_names</th>      <th>values</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>0.999889</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>0.000111</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>.join(wrap(sample_text)))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;True label: <span class="subst">&#123;class_names[true_label]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>小A班应急照明灯坏[SEP]商贸服务教文卫类[SEP]消防检查[SEP]防火巡查[SEP]安全出口、疏散通道是否畅通，安全疏散指示标志、应急照明是否完好。True label: 0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=<span class="string">&#x27;values&#x27;</span>, y=<span class="string">&#x27;class_names&#x27;</span>, data=pred_df, orient=<span class="string">&#x27;h&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;sentiment&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;probability&#x27;</span>)</span><br><span class="line">plt.xlim([<span class="number">0</span>, <span class="number">1</span>]);</span><br></pre></td></tr></table></figure><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079499324.png" alt="1669079499324"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_text = <span class="string">&#x27; &#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">encoded_text = tokenizer.encode_plus(</span><br><span class="line">  sample_text,</span><br><span class="line">  max_length=MAX_LEN,</span><br><span class="line">  add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">  return_token_type_ids=<span class="literal">False</span>,</span><br><span class="line">  pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">  return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">  return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input_ids = encoded_text[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">attention_mask = encoded_text[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line"></span><br><span class="line">output = model(input_ids, attention_mask)</span><br><span class="line">_, prediction = torch.<span class="built_in">max</span>(output, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Sample text: <span class="subst">&#123;sample_text&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Danger label  : <span class="subst">&#123;class_names[prediction]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>Sample text:  Danger label  : 1</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;通过智能化手段识别其中是否存在“虚报、假报”的情况&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="BERT" scheme="https://du2279664786.github.io/tags/BERT/"/>
    
    <category term="文本分类" scheme="https://du2279664786.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>Transformer总结和梳理</title>
    <link href="https://du2279664786.github.io/posts/55106.html"/>
    <id>https://du2279664786.github.io/posts/55106.html</id>
    <published>2022-11-20T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.768Z</updated>
    
    <content type="html"><![CDATA[<p>Transformer最终总结版，超级详细！</p><span id="more"></span><p>首先来看一下Transformer结构的结构：<br><img src="https://img-blog.csdnimg.cn/de74182b27f24a84a28fdd5f7204f0cd.png" alt="在这里插入图片描述"><br>Transformer是由Encoder和Decoder两大部分组成，首先我们先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构。<br>        Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，然后又做了一个ADD&amp;Norm，再通过Feed Forward进行输出，最后又做了一个ADD&amp;Norm，这就是Decoder。<br>        Decoder和Encoder有很多相同的地方，Decoder首先也是需要进行 Input Embedding 和 Positional Embedding 求作为输入，并且Decoder中的第一个Attention模块为Mask Attention，还有一个就是Decoder的K和V分别均来自Encoder。<br>        接下来看一下每个模块的具体理解：</p><h1 id="Positional-encoding"><a href="#Positional-encoding" class="headerlink" title="Positional encoding"></a>Positional encoding</h1><p>首先对于文本特征，需要进行Embedding，由于transformer抛弃了Rnn的结构，不能捕捉到序列的信息，交换单词位置，得到相应的attention也会发生交换，并不会发生数值上的改变，所以要对input进行Positional Encoding。</p><p>Positional encoding和input embedding是同等维度的，所以可以将两者进行相加，的到输入向量<br><img src="https://img-blog.csdnimg.cn/be30b27838dd411c89d793432ff72582.png" alt="在这里插入图片描述"><br>接下来看一些Positional Encoding的计算公式：<br><img src="https://img-blog.csdnimg.cn/6e9a80e756b94a70aeef8a79097eb7a6.png" alt="在这里插入图片描述"><br>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值，也就是说：对于单个token的d_model维度的词向量，奇数位置取cos，偶数位置取sin，最终的到一个维度和word embedding维度一样的矩阵，接下来可以看一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_positional_encoding</span>(<span class="params">max_seq_len, embed_dim</span>):</span><br><span class="line">    <span class="comment"># 初始化一个positional encoding</span></span><br><span class="line">    <span class="comment"># embed_dim: 字嵌入的维度</span></span><br><span class="line">    <span class="comment"># max_seq_len: 最大的序列长度</span></span><br><span class="line">    positional_encoding = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * i / embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embed_dim)] <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(embed_dim) <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len)])</span><br><span class="line"></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i 偶数</span></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1 奇数</span></span><br><span class="line">    <span class="keyword">return</span> positional_encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">positional_encoding = get_positional_encoding(max_seq_len=<span class="number">100</span>, embed_dim=<span class="number">16</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">sns.heatmap(positional_encoding)</span><br><span class="line">plt.title(<span class="string">&quot;Sinusoidal Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;hidden dimension&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;sequence length&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>首先求初始向量：positional_encoding，然后对其奇数列求sin，偶数列求cos：<br><img src="https://img-blog.csdnimg.cn/2da3445d7953426394ab2e46d8820baf.png" alt="在这里插入图片描述"><br>最终得到positional encoding之后的数据可视化：<br><img src="https://img-blog.csdnimg.cn/507cc7ca0ba34f8fb2ec87216689857c.png" alt="在这里插入图片描述"></p><h1 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h1><p>何为self-attention？首先我们要明白什么是attention，对于传统的seq2seq任务，例如中-英文翻译，输入中文，得到英文，即source是中文句子（x1 x2 x3）,英文句子是target（y1 y2 y3）<br><img src="https://img-blog.csdnimg.cn/296c379fd77c475ebf77c06fa8e42e59.png" alt="在这里插入图片描述"><br>attention机制发生在target的元素和source中的所有元素之间。简单的将就是attention机制中的权重计算需要target参与，即在上述Encoder-Decoder模型中，Encoder和Decoder两部分都需要参与运算。</p><p>而对于self-attention，它不需要Decoder的参与，而是source内部元素之间发生的运算，对于输入向量X，对其做线性变换，分别得到Q、K、V矩阵<br><img src="https://img-blog.csdnimg.cn/17358bbb1cf641d78117625fb5a00d31.png" alt="在这里插入图片描述"><br>然后去计算attention，Q、K点乘得到初步的权重因子，并对Q、K点乘结果进行放缩，除以sqrt（dk），Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差，最终再加一个softmax就得到了self attention的输出。<br><img src="https://img-blog.csdnimg.cn/07fdbd802d114b7193c70e9fc451ba22.png" alt="在这里插入图片描述"></p><h2 id="Multi–head-attention"><a href="#Multi–head-attention" class="headerlink" title="Multi–head-attention"></a>Multi–head-attention</h2><p>Multi–head-attention使用了多个头进行运算，捕捉到了更多的信息，多头的数量用h表示，一般h&#x3D;8，表示8个头<br><img src="https://img-blog.csdnimg.cn/7cad1e37120b4d40ad64140677452ec1.png" alt="在这里插入图片描述"><br>在输入每个self-attention之前，我们需将输入X均分的分到h个头中，得到Z1-Z7八个头的输出结果。<br><img src="https://img-blog.csdnimg.cn/d5763f726a5c48f292a140f84c0e5200.png" alt="在这里插入图片描述"><br>对于每个头计算相应的attention score，将其进行拼接，再与W0进行一个线性变换，就得到最终输出的Z。<br><img src="https://img-blog.csdnimg.cn/05bfb9eb87bb4f3b8066c8190c0dff0f.png" alt="在这里插入图片描述"></p><h1 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add&amp;Norm"></a>Add&amp;Norm</h1><h2 id="Add操作"><a href="#Add操作" class="headerlink" title="Add操作"></a>Add操作</h2><p>首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，再通过Feed Forward进行输出。</p><p>由下图可以看出：在Encoder层和Decoder层中都用到了Add&amp;Norm操作，即残差连接和层归一化操作。<br><img src="https://img-blog.csdnimg.cn/5ef722ad3c5b407482ac132b0883c59c.png" alt="在这里插入图片描述"><br>什么是残差连接呢？残差连接就是把网络的输入和输出相加，即网络的输出为F(x)+x，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就变成了F(x)+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项‘1’，有效解决了梯度消失问题。</p><h2 id="Norm操作"><a href="#Norm操作" class="headerlink" title="Norm操作"></a>Norm操作</h2><p>首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇<a href="https://mp.weixin.qq.com/s/HNCl6MPS_hjTVHNt7UkYyw">文章</a>，才明白了Norm是什么。</p><p>假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]</span><br><span class="line">[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]]</span><br></pre></td></tr></table></figure><p>如果是在做BatchNorm（BN）的话，其计算过程如下：BN1&#x3D;(w11+w12+w13+w14+w41+<br>w42+w43+w44)&#x2F;8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean</p><p>如果是在做LayerNorm（LN）的话，则会进如下计算：LN1&#x3D;(w11+w12+w13+w14+w21+<br>w22+w23+w24+w31+w32+w33+w34)&#x2F;12，同理会得到LN2，最终得到[LN1,LN2]两个mean</p><p>如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1&#x3D;(w11+w12+w13+w14)&#x2F;4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]<br>下图完美的揭示了，这几种Norm<br><img src="https://img-blog.csdnimg.cn/a143d6b41e654fa1849f44580401110c.png" alt="在这里插入图片描述"><br>接下来我们来看一下Transformer中的Norm：首先生成[2,3,4]形状的数据，使用原始的编码方式进行编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> InstanceNorm2d</span><br><span class="line">random_seed = <span class="number">123</span></span><br><span class="line">torch.manual_seed(random_seed)</span><br><span class="line"></span><br><span class="line">batch_size, seq_size, dim = <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">embedding = torch.randn(batch_size, seq_size, dim)</span><br><span class="line"></span><br><span class="line">layer_norm = torch.nn.LayerNorm(dim, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y: &quot;</span>, layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y:  tensor([[[ <span class="number">1.5524</span>,  <span class="number">0.0155</span>, -<span class="number">0.3596</span>, -<span class="number">1.2083</span>],</span><br><span class="line">         [ <span class="number">0.5851</span>,  <span class="number">1.3263</span>, -<span class="number">0.7660</span>, -<span class="number">1.1453</span>],</span><br><span class="line">         [ <span class="number">0.2864</span>,  <span class="number">0.0185</span>,  <span class="number">1.2388</span>, -<span class="number">1.5437</span>]],</span><br><span class="line">        [[ <span class="number">1.1119</span>, -<span class="number">0.3988</span>,  <span class="number">0.7275</span>, -<span class="number">1.4406</span>],</span><br><span class="line">         [-<span class="number">0.4144</span>, -<span class="number">1.1914</span>,  <span class="number">0.0548</span>,  <span class="number">1.5510</span>],</span><br><span class="line">         [ <span class="number">0.3914</span>, -<span class="number">0.5591</span>,  <span class="number">1.4105</span>, -<span class="number">1.2428</span>]]])</span><br></pre></td></tr></table></figure><p>接下来手动去进行一下编码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以发现和LayerNorm的结果是一样的，也就是说明Norm是对d_model进行的Norm，会给我们[batch,sqe_length]形状的平均值。<br>加下来进行batch_norm,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_norm = torch.nn.LayerNorm([seq_size,dim], elementwise_affine = <span class="literal">False</span>)</span><br><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1822</span>,  <span class="number">0.4419</span>, -<span class="number">0.3196</span>, -<span class="number">1.9889</span>],</span><br><span class="line">         [-<span class="number">0.6677</span>, -<span class="number">0.2537</span>, -<span class="number">0.8151</span>,  <span class="number">1.5143</span>],</span><br><span class="line">         [ <span class="number">0.7174</span>,  <span class="number">1.2147</span>, -<span class="number">0.0852</span>, -<span class="number">0.9403</span>]],</span><br><span class="line">        [[-<span class="number">0.0138</span>,  <span class="number">1.5666</span>, -<span class="number">2.1726</span>,  <span class="number">1.0590</span>],</span><br><span class="line">         [ <span class="number">0.6646</span>,  <span class="number">0.6852</span>, -<span class="number">0.8706</span>, -<span class="number">0.0442</span>],</span><br><span class="line">         [-<span class="number">0.1163</span>,  <span class="number">0.1389</span>,  <span class="number">0.4454</span>, -<span class="number">1.3423</span>]]])</span><br></pre></td></tr></table></figure><p>可以看到BN的计算的mean形状为[2, 1, 1]，并且Norm结果也和上面的两个不一样，这就充分说明了Norm是在对最后一个维度求平均。<br>那么什么又是Instancenorm呢？接下来再来实现一下instancenorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instance_norm = InstanceNorm2d(<span class="number">3</span>, affine=<span class="literal">False</span>)</span><br><span class="line">output = instance_norm(embedding.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)) <span class="comment">#InstanceNorm2D需要(N,C,H,W)的shape作为输入</span></span><br><span class="line">layer_norm = torch.nn.LayerNorm(<span class="number">4</span>, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br></pre></td></tr></table></figure><p>可以看出无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><h1 id="FeedForward"><a href="#FeedForward" class="headerlink" title="FeedForward"></a>FeedForward</h1><p>接下来是FeedForward，FeedForward是Multi-Head Attention的输出做了残差连接和Norm之后得数据，然后FeedForward做了两次线性线性变换，为的是更加深入的提取特征。<br><img src="https://img-blog.csdnimg.cn/b976d7add795475fac7bbc6c5f01121f.png" alt="在这里插入图片描述"><br>可以看出在每次线性变换都引入了非线性激活函数Relu，在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强，FeedForward的计算公式如下：max相当于Relu<br><img src="https://img-blog.csdnimg.cn/43b6886add22435795f3f8a7a889c58e.png" alt="在这里插入图片描述"></p><p>所以FeedForward的作用是：通过线性变换，先将数据映射到高纬度的空间再映射到低纬度的空间，提取了更深层次的特征</p><h1 id="MASK"><a href="#MASK" class="headerlink" title="MASK"></a>MASK</h1><p>最后是MASK介绍，Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分</p><h2 id="Padding-Masked"><a href="#Padding-Masked" class="headerlink" title="Padding Masked"></a>Padding Masked</h2><p>对于Transformer而言，每次的输入为：[batch_size,seq_length,d_module]结构，由于句子一般是长短不一的，而输入的数据需要是固定的格式，所以要对句子进行处理。<br>通常会把每个句子按照最大长度进行补齐，所以当句子不够长时，需要进行补0操作，以保证输入数据结构的完整性<br>但是在计算注意力机制时的Softmax函数时，就会出现问题，Padding数值为0的话，仍然会影响到Softmax的计算结果，即无效数据参加了运算。<br>为了不让Padding数据产生影响，通常会将Padding数据变为负无穷，这样的话就不会影响Softmax函数了</p><h2 id="Self-Attention-Masked"><a href="#Self-Attention-Masked" class="headerlink" title="Self-Attention Masked"></a>Self-Attention Masked</h2><p>Self-Attention Masked只发生在Decoder操作中，在Decoder中，我们的预测是一个一个进行的，即输入一个token，输出下一个token，在网上看到一个很好的解释如下：<br>假设我们当前在进行机器翻译<br>    输入：我很好<br>    输出：I am fine<br>接下来是Decoder执行步骤<br>第一步：<br>    ·初始输入： 起始符</s> + Positional Encoding（位置编码）<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“I”<br>第二步：<br>    ·初始输入：起始符</s> + “I”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“am”<br>第三步：<br>    ·初始输入：起始符</s> + “I”+ “am”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“fine”<br>上面就是Decoder的执行过程，在预测出“I”之前，我们是不可能知道‘am’的，所以要将数进行Mask，防止看到当前值后面的值，如下图所示：当我们仅知道start的时候，后面的关系是不知道的，所以start和“I”以及其它单词的Score都为负无穷，当预测出“I”之后，再去预测“am”，最终得到下面第三个得分矩阵。<br><img src="https://img-blog.csdnimg.cn/8df2f100c0a447909a8f9d99cbf86a1d.png"></p><p>最后经过Softmax处理之后，得到最终的得分矩阵<br><img src="https://img-blog.csdnimg.cn/90d840807bf04b89bf8e32dd6bf19e85.png"></p><p>最后不要忘了Decoder中依然采用的是Masked Multi-Head Attention，即多次进行Mask机制</p><p>写在最后：笔者也是刚入门深度学习，对知识也是初步的认识，如果文章有错，请大佬们斧正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Transformer最终总结版，超级详细！&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://du2279664786.github.io/posts/16107.html"/>
    <id>https://du2279664786.github.io/posts/16107.html</id>
    <published>2022-11-20T14:55:10.000Z</published>
    <updated>2022-11-24T13:20:55.203Z</updated>
    
    <content type="html"><![CDATA[<div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["大家好，这是我的第一篇文章，欢迎查看"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><span id="more"></span><div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["青青陵上柏，磊磊涧中石。", "人生天地间，忽如远行客。","斗酒相娱乐，聊厚不为薄。", "驱车策驽马，游戏宛与洛。","洛中何郁郁，冠带自相索。","长衢罗夹巷，王侯多第宅。","两宫遥相望，双阙百余尺。","极宴娱心意，戚戚何所迫？"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><h1 id="欢迎查看我的第一篇文章"><a href="#欢迎查看我的第一篇文章" class="headerlink" title="欢迎查看我的第一篇文章"></a>欢迎查看我的第一篇文章</h1><p>写在前面：这是我的Github，欢迎star：<a href="https://github.com/du2279664786">https://github.com/du2279664786</a></p><p>大一的时候搭建过一次博客，但是由于长时间不使用，导致配置文件丢失<br>所以在大三上学期我又重新搭建了一下，将去年（大二）所学的知识进行了“简单”的汇总（PS：李明虎老师讲的实在是太丰富了，我只整理了简单的）</p><p>本博客为学习博客，旨在记录自己的学习经历和知识回顾</p><p>转眼间已经大三了，回顾过去两年：<br>        ·大一上：人工智能导论<br>        ·大一下：爬虫<br>        ·大二上：机器学习<br>        ·大二下：深度学习和神经网络<br>….. …..</p><p>希望考研可以成功上岸！</p>]]></content>
    
    
    <summary type="html">&lt;div id=&quot;binft&quot;&gt;&lt;/div&gt;
  &lt;script&gt;
    var binft = function (r) {
      function t() {
        return b[Math.floor(Math.random() * b.length)]
      }  
      function e() {
        return String.fromCharCode(94 * Math.random() + 33)
      }
      function n(r) {
        for (var n = document.createDocumentFragment(), i = 0; r &gt; i; i++) {
          var l = document.createElement(&quot;span&quot;);
          l.textContent = e(), l.style.color = t(), n.appendChild(l)
        }
        return n
      }
      function i() {
        var t = o[c.skillI];
        c.step ? c.step-- : (c.step = g, c.prefixP &lt; l.length ? (c.prefixP &gt;= 0 &amp;&amp; (c.text += l[c.prefixP]), c.prefixP++) : &quot;forward&quot; === c.direction ? c.skillP &lt; t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = &quot;backward&quot;, c.delay = a) : c.skillP &gt; 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = &quot;forward&quot;)), r.textContent = c.text, r.appendChild(n(c.prefixP &lt; l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)
      }
      var l = &quot;&quot;,
      o = [&quot;大家好，这是我的第一篇文章，欢迎查看&quot;].map(function (r) {
      return r + &quot;&quot;
      }),
      a = 2,
      g = 1,
      s = 5,
      d = 75,
      b = [&quot;rgb(110,64,170)&quot;, &quot;rgb(150,61,179)&quot;, &quot;rgb(191,60,175)&quot;, &quot;rgb(228,65,157)&quot;, &quot;rgb(254,75,131)&quot;, &quot;rgb(255,94,99)&quot;, &quot;rgb(255,120,71)&quot;, &quot;rgb(251,150,51)&quot;, &quot;rgb(226,183,47)&quot;, &quot;rgb(198,214,60)&quot;, &quot;rgb(175,240,91)&quot;, &quot;rgb(127,246,88)&quot;, &quot;rgb(82,246,103)&quot;, &quot;rgb(48,239,130)&quot;, &quot;rgb(29,223,163)&quot;, &quot;rgb(26,199,194)&quot;, &quot;rgb(35,171,216)&quot;, &quot;rgb(54,140,225)&quot;, &quot;rgb(76,110,219)&quot;, &quot;rgb(96,84,200)&quot;],
      c = {
        text: &quot;&quot;,
        prefixP: -s,
        skillI: 0,
        skillP: 0,
        direction: &quot;forward&quot;,
        delay: a,
        step: g
      };
      i()
      };
      binft(document.getElementById(&#39;binft&#39;));
  &lt;/script&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>什么是Warmup</title>
    <link href="https://du2279664786.github.io/posts/1669.html"/>
    <id>https://du2279664786.github.io/posts/1669.html</id>
    <published>2022-11-16T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.767Z</updated>
    
    <content type="html"><![CDATA[<p>Warmup可以逐渐地将学习率从一个小的值提升到一个大的值</p><span id="more"></span><h1 id="Warmup"><a href="#Warmup" class="headerlink" title="Warmup"></a>Warmup</h1><h3 id="什么是预热学习率-Warmup-？"><a href="#什么是预热学习率-Warmup-？" class="headerlink" title="什么是预热学习率(Warmup)？"></a>什么是预热学习率(Warmup)？</h3><p>先来看一下预热的定义：预热指的是为防止急热，焊接前先对材料预热。</p><p>而预热学习率指的是对学习率进行控制，使学习率缓慢的变化，而不是直接变为设定值。</p><p>下面我们通过一个具体的例子来展示Warmup：</p><p>设置初始化lr &#x3D; 0.1,假设所有步数num_step &#x3D; 20000,warmup_step &#x3D; 5000，即当步数达到5000时，达到所设置的学习率</p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">warmup_steps = <span class="number">5000</span></span><br><span class="line">init_lr = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 模拟训练20000步</span></span><br><span class="line">max_steps = <span class="number">20000</span></span><br><span class="line">a = []</span><br><span class="line"><span class="keyword">for</span> train_steps <span class="keyword">in</span> <span class="built_in">range</span>(max_steps):</span><br><span class="line">    <span class="keyword">if</span> warmup_steps <span class="keyword">and</span> train_steps &lt; warmup_steps:</span><br><span class="line">        warmup_percent_done = train_steps / warmup_steps</span><br><span class="line">        warmup_learning_rate = init_lr * warmup_percent_done  <span class="comment">#gradual warmup_lr</span></span><br><span class="line">        learning_rate = warmup_learning_rate</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#learning_rate = np.sin(learning_rate)  #预热学习率结束后,学习率呈sin衰减</span></span><br><span class="line">        learning_rate = learning_rate**<span class="number">1.0001</span> <span class="comment">#预热学习率结束后,学习率呈指数衰减(近似模拟指数衰减)</span></span><br><span class="line">    <span class="keyword">if</span> (train_steps+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">             <span class="built_in">print</span>(<span class="string">&quot;train_steps:%.3f--warmup_steps:%.3f--learning_rate:%.3f&quot;</span> % (</span><br><span class="line">                 train_steps+<span class="number">1</span>,warmup_steps,learning_rate))</span><br><span class="line">    a.append(learning_rate)</span><br><span class="line">plt.plot(a)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可得学习率变化如下：</p><p><img src="https://img-blog.csdnimg.cn/ddd9decebde845009dbf4aad2a84ac4a.png"></p><h3 id="为什么要进行预热学习率"><a href="#为什么要进行预热学习率" class="headerlink" title="为什么要进行预热学习率"></a>为什么要进行预热学习率</h3><p>由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Warmup可以逐渐地将学习率从一个小的值提升到一个大的值&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中的MASK理解</title>
    <link href="https://du2279664786.github.io/posts/35596.html"/>
    <id>https://du2279664786.github.io/posts/35596.html</id>
    <published>2022-11-15T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.771Z</updated>
    
    <content type="html"><![CDATA[<p>Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分</p><span id="more"></span><p>上一篇文章我们介绍了<a href="https://blog.csdn.net/weixin_51756104/article/details/127250190?spm=1001.2014.3001.5501">对Transformer中FeedForward层的理解</a>，今天我们来介绍一下对MASK的理解<br>老规矩，还是先放一张Transformer的图片<br><img src="https://img-blog.csdnimg.cn/de74182b27f24a84a28fdd5f7204f0cd.png" alt="在这里插入图片描述"><br>Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，而所谓的MASK在Encoder和Decoder两个结构中都有使用。</p><p>Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分</p><h1 id="Padding-Masked"><a href="#Padding-Masked" class="headerlink" title="Padding Masked"></a>Padding Masked</h1><p>对于Transformer而言，每次的输入为：[batch_size,seq_length,d_module]结构，由于句子一般是长短不一的，而输入的数据需要是固定的格式，所以要对句子进行处理。<br>通常会把每个句子按照最大长度进行补齐，所以当句子不够长时，需要进行补0操作，以保证输入数据结构的完整性<br>但是在计算注意力机制时的Softmax函数时，就会出现问题，Padding数值为0的话，仍然会影响到Softmax的计算结果，即无效数据参加了运算。<br>为了不让Padding数据产生影响，通常会将Padding数据变为负无穷，这样的话就不会影响Softmax函数了</p><h1 id="Sequence-Masked"><a href="#Sequence-Masked" class="headerlink" title="Sequence Masked"></a>Sequence Masked</h1><p>Sequence Masked只发生在Decoder操作中，在Decoder中，我们的预测是一个一个进行的，即输入一个token，输出下一个token，在网上看到一个很好的解释如下：<br>假设我们当前在进行机器翻译<br>    输入：我很好<br>    输出：I am fine<br>接下来是Decoder执行步骤<br>第一步：<br>    ·初始输入： 起始符</s> + Positional Encoding（位置编码）<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“I”<br>第二步：<br>    ·初始输入：起始符</s> + “I”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“am”<br>第三步：<br>    ·初始输入：起始符</s> + “I”+ “am”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“fine”<br>上面就是Decoder的执行过程，在预测出“I”之前，我们是不可能知道‘am’的，所以要将数进行Mask，防止看到当前值后面的值，如下图所示：当我们仅知道start的时候，后面的关系是不知道的，所以start和“I”以及其它单词的Score都为负无穷，当预测出“I”之后，再去预测“am”，最终得到下面第三个得分矩阵。<br><img src="https://img-blog.csdnimg.cn/8df2f100c0a447909a8f9d99cbf86a1d.png"></p><p>最后经过Softmax处理之后，得到最终的得分矩阵<br><img src="https://img-blog.csdnimg.cn/90d840807bf04b89bf8e32dd6bf19e85.png"></p><p>最后不要忘了Decoder中依然采用的是Masked Multi-Head Attention，即多次进行Mask机制</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
    <category term="Mask" scheme="https://du2279664786.github.io/tags/Mask/"/>
    
  </entry>
  
  <entry>
    <title>第五届“泰迪杯”数据分析技能赛</title>
    <link href="https://du2279664786.github.io/posts/14939.html"/>
    <id>https://du2279664786.github.io/posts/14939.html</id>
    <published>2022-11-13T14:55:10.000Z</published>
    <updated>2022-12-30T11:55:49.525Z</updated>
    
    <content type="html"><![CDATA[<p>第五届“泰迪杯”数据分析技能赛-全国二等奖</p><span id="more"></span><p>今天刚结束了第五届“泰迪杯”数据分析技能赛，我们选择的B题做，昨天的A题太伤心了，做不出来A题（时间太短，再加上太麻烦，晚上直接跑路去操场看70周年校庆了）</p><p>先上代码：<a href="https://github.com/du2279664786/2022-taidi-competition">https://github.com/du2279664786/2022-taidi-competition</a>，直接在前面仓库可以看到代码和相关数据</p><h1 id="竞赛日程；"><a href="#竞赛日程；" class="headerlink" title="竞赛日程；"></a>竞赛日程；</h1><p>报名起始时间：2022年9月5日-11月10日<br>赛前指导时间：2022年9月13日-11月10日<br>A题竞赛时间：2022年11月12日 8:00-21:00（8:00:00公布赛题）<br>B题竞赛时间：2022年11月13日 8:00-20:00（8:00:00公布赛题）<br>视频答辩时间：2022年12月上旬，具体时间另行通知<br>成绩公示时间：2022年12月中下旬<br>最终成绩公布时间：2022年12月中下旬</p><h1 id="赛题基本情况"><a href="#赛题基本情况" class="headerlink" title="赛题基本情况"></a>赛题基本情况</h1><p>A题基本如下：<br><img src="https://img-blog.csdnimg.cn/fc0e1942da0840e581af2f1a07f92294.png"><br>B题基本如下：<br><img src="https://img-blog.csdnimg.cn/60fe3adf19504c08858bb6e0335dae6c.png"><br><img src="https://img-blog.csdnimg.cn/0bd891ee6bc648db963bbb5be25ff832.png"><br><img src="https://img-blog.csdnimg.cn/32ce46c165fb4f959267a3ee388aa68e.png"><br><img src="https://img-blog.csdnimg.cn/528ac1e435114bd781e6ad1f5de35bb8.png"></p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>昨天做A题，今天做B题，每个题的做题时间是8:00-20:00（8:00:00公布赛题），共12个小时的竞赛时间，完成一篇论文</p><p>可以看出，A题和我们人工智能专业相关性不大，所以昨天我和两个队友基本出于摆烂状态，也没有提交论文，基本放弃了A题，而转身去看建校70周年的节目去了</p><p>然后今天早晨又看了B题，发现可以冲，于是我们三个就去201教室开干，一共写了20页，差不多从9点开干，一直到晚上8点，难度不大，但也需要一定的思考逻辑，模型部分如下图展示：<br><img src="https://img-blog.csdnimg.cn/492c0617f3ce49c68980a8c3d4e23b36.png"><br><img src="https://img-blog.csdnimg.cn/4b25af9f99db44c195bb9ce3f6079b28.png"><br><img src="https://img-blog.csdnimg.cn/b7c75b831b574d3fb7c78f100c022287.png"></p><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>做的还可以，基本都是按照题目要求做的，美中不足就是正第一页排版不大好，其他的都还可以，希望能够摸一个奖</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第五届“泰迪杯”数据分析技能赛-全国二等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据挖掘" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数据分析" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>第三届“大湾区杯”粤港澳金融数学建模竞赛</title>
    <link href="https://du2279664786.github.io/posts/25176.html"/>
    <id>https://du2279664786.github.io/posts/25176.html</id>
    <published>2022-11-08T14:55:10.000Z</published>
    <updated>2022-12-30T11:56:06.367Z</updated>
    
    <content type="html"><![CDATA[<p>2022年第三届“大湾区杯”粤港澳金融数学建模竞赛-全国三等奖</p><span id="more"></span><p>老规矩先上代码仓库：<a href="https://github.com/du2279664786/2022-JinRong-Mathematics-modeling">https://github.com/du2279664786/2022-JinRong-Mathematics-modeling</a></p><h1 id="竞赛日程"><a href="#竞赛日程" class="headerlink" title="竞赛日程"></a>竞赛日程</h1><p>报名时间：10月12日10:00:00-10月30日22:00:00<br>竞赛时间：11月1日10:00:00-11月8日14:00:00</p><h1 id="赛题基本情况"><a href="#赛题基本情况" class="headerlink" title="赛题基本情况"></a>赛题基本情况</h1><p>A题如下<br><img src="https://img-blog.csdnimg.cn/4d314192ae304e3ab220fddc93379418.png"><br>B题如下<br><img src="https://img-blog.csdnimg.cn/1c6b8f2fcc844fc891edbf488f9078df.png"><br>AB两个题都是比较偏向金融的（题目就叫‘金融’数学建模）</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>我们三个都是计算机专业的，对金融一窍不通，刚开始选择的是A题，因为我们感觉A比较偏代码，实践性更强一些，但是当我们做了两天之后发现：A题根本没有头绪，对他们所给的数据也是不会处理，11月3号，还是没有思路，于是我们果断换成了B题，容易水论文<br><img src="https://img-blog.csdnimg.cn/83574b6bd9624a229fb935074e4c808a.png"></p><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>整体还行，至少尽力做了，对金融领域不大了解，这也是我们的短处，希望能摸一个奖！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年第三届“大湾区杯”粤港澳金融数学建模竞赛-全国三等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数学建模" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
    <category term="金融" scheme="https://du2279664786.github.io/tags/%E9%87%91%E8%9E%8D/"/>
    
  </entry>
  
  <entry>
    <title>中国大学生计算机设计大赛复盘</title>
    <link href="https://du2279664786.github.io/posts/194.html"/>
    <id>https://du2279664786.github.io/posts/194.html</id>
    <published>2022-10-27T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.764Z</updated>
    
    <content type="html"><![CDATA[<p>2022年中国大学生计算机设计大赛-人工智能挑战赛-国家二等奖</p><span id="more"></span><p>此篇文章写于2022年10月27日，为了复盘、回顾上一次的计算机设计大赛<br>中国大学生计算机设计大赛</p><h1 id="日程"><a href="#日程" class="headerlink" title="日程"></a>日程</h1><p>5-6月份开始初赛省赛，我和我的两位队友努力的写文档，整理代码，提交了相关资料，由于初赛（省赛）没有答辩，所以差不多等到六月多收到获得省二的通知<br>7月份多得之进了国赛<br>7月底-8月底就开始修改文档，修改代码，录制相关视频，等待国赛的答辩</p><h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><p>项目背景：在数字经济蓬勃发展的时代背景下， “数字化”“智慧化”成为了企业管理转型升级的核心引擎。传统建管理模式已不再符合可持续发展的要求，迫切需要利用以信息技术为代表的现代科技手段，实现中国企业管理转型升级与跨越式发展。为了更好的解决时代问题的痛点，并在一定程度上节约人力成本，本项目设计了一个基于云端和硬件的人工智能多场景实时物体监测平台，将多场景实时监测与报警系统进行了融合创新，做到了监控、检测、分类、识别四位一体的平台建设。<br>整体布局：<br><img src="https://img-blog.csdnimg.cn/d652a2b775ee475f951a36c928d2054e.png" alt="在这里插入图片描述"></p><p>算法介绍：<br>移动物体检测：本项目采用OpenCV（Open Source Computer Vision Library）算法，使用灰度转化、图片缩放和高斯滤波等相关操作，对图像进行预处理，增强了移动物体的可检测性。</p><p>物体识别：为提高精确度并且处理时间尽可能短，本项目采用了运算速度比较快的 Yolov3 算法，它是基于深度学习框架Darknet的目标检测开源项目，不仅可以充分发挥多核处理器和 GPU 并行运算的功能，还可以基于预训练模型进行实时目标检测,预期效果如下：<br><img src="https://img-blog.csdnimg.cn/d1d870e51929499fb0637ed6a3ba3703.png" alt="在这里插入图片描述"><br>以及安全帽检测、疲劳监测、口罩检测<br>具体代码仓库如下：<a href="https://github.com/du2279664786/Chinese-undergraduate-computer-design-contest">https://github.com/du2279664786/Chinese-undergraduate-computer-design-contest</a></p><p>最终获得证书：一个省二证书、一个国二证书、外加一枚金匾<br><img src="https://img-blog.csdnimg.cn/736f6b9f339841fc832f0180d06ab5d8.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/99c107c9a863454e8ee0d84169935b90.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年中国大学生计算机设计大赛-人工智能挑战赛-国家二等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="机器视觉" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>数学建模省一思路及其代码</title>
    <link href="https://du2279664786.github.io/posts/1054.html"/>
    <id>https://du2279664786.github.io/posts/1054.html</id>
    <published>2022-10-14T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.762Z</updated>
    
    <content type="html"><![CDATA[<p>2022年全国大学生数学建模竞赛山东省一等奖</p><span id="more"></span><p>连肝三天，撸了三天代码，9.17早-9.18晚日夜不停，终于和队友拿下了2022年全国大学生数学建模竞赛山东省一等奖<br>基本思路如下：<br><img src="https://img-blog.csdnimg.cn/c6128ba97db94cf489182b2fe05ee6ec.jpeg"><br>下面是代码部分，仓库连接如下：<a href="https://github.com/du2279664786/CUMCM">https://github.com/du2279664786/CUMCM</a><br><img src="https://img-blog.csdnimg.cn/1b6a9e5ed0054967b790696d20f287ce.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年全国大学生数学建模竞赛山东省一等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数学建模" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中FeedForward层的理解</title>
    <link href="https://du2279664786.github.io/posts/46858.html"/>
    <id>https://du2279664786.github.io/posts/46858.html</id>
    <published>2022-10-10T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.761Z</updated>
    
    <content type="html"><![CDATA[<p>在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强</p><span id="more"></span><p>上一篇我们介绍了<a href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/">对Add&amp;Norm层的理解</a>，有不大熟悉的可以看一下上篇文章。</p><p>今天来说一下Transformer中FeedForward层，首先还是先来回顾一下Transformer的基本结构：首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，然后又做了一个ADD&amp;Norm，再通过Feed Forward进行输出。<br><img src="https://img-blog.csdnimg.cn/4af25c021fd14b70aa2648a925fadf54.png"><br>FeedForward的输入是什么呢？是Multi-Head Attention的输出做了残差连接和Norm之后得数据，然后FeedForward做了两次线性线性变换，为的是更加深入的提取特征。<br><img src="https://img-blog.csdnimg.cn/b976d7add795475fac7bbc6c5f01121f.png" alt="在这里插入图片描述"><br>可以看出在每次线性变换都引入了非线性激活函数Relu，在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强，FeedForward的计算公式如下：max相当于Relu<br><img src="https://img-blog.csdnimg.cn/43b6886add22435795f3f8a7a889c58e.png" alt="在这里插入图片描述"></p><p>所以FeedForward的作用是：通过线性变换，先将数据映射到高纬度的空间再映射到低纬度的空间，提取了更深层次的特征</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Add&amp;Norm层的理解</title>
    <link href="https://du2279664786.github.io/posts/34541.html"/>
    <id>https://du2279664786.github.io/posts/34541.html</id>
    <published>2022-10-09T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.759Z</updated>
    
    <content type="html"><![CDATA[<p>无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><span id="more"></span><h1 id="Add操作"><a href="#Add操作" class="headerlink" title="Add操作"></a>Add操作</h1><p>首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，再通过Feed Forward进行输出。</p><p>由下图可以看出：在Encoder层和Decoder层中都用到了Add&amp;Norm操作，即残差连接和层归一化操作。<br><img src="https://img-blog.csdnimg.cn/5ef722ad3c5b407482ac132b0883c59c.png" alt="在这里插入图片描述"><br>什么是残差连接呢？残差连接就是把网络的输入和输出相加，即网络的输出为F(x)+x，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就变成了F(x)+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项‘1’，有效解决了梯度消失问题。</p><h1 id="Norm操作"><a href="#Norm操作" class="headerlink" title="Norm操作"></a>Norm操作</h1><p>首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇<a href="https://mp.weixin.qq.com/s/HNCl6MPS_hjTVHNt7UkYyw">文章</a>，才明白了Norm是什么。</p><p>假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]</span><br><span class="line">[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]]</span><br></pre></td></tr></table></figure><p>如果是在做BatchNorm（BN）的话，其计算过程如下：BN1&#x3D;(w11+w12+w13+w14+w41+<br>w42+w43+w44)&#x2F;8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean</p><p>如果是在做LayerNorm（LN）的话，则会进如下计算：LN1&#x3D;(w11+w12+w13+w14+w21+<br>w22+w23+w24+w31+w32+w33+w34)&#x2F;12，同理会得到LN2，最终得到[LN1,LN2]两个mean</p><p>如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1&#x3D;(w11+w12+w13+w14)&#x2F;4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]<br>下图完美的揭示了，这几种Norm<br><img src="https://img-blog.csdnimg.cn/a143d6b41e654fa1849f44580401110c.png" alt="在这里插入图片描述"><br>接下来我们来看一下Transformer中的Norm：首先生成[2,3,4]形状的数据，使用原始的编码方式进行编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> InstanceNorm2d</span><br><span class="line">random_seed = <span class="number">123</span></span><br><span class="line">torch.manual_seed(random_seed)</span><br><span class="line"></span><br><span class="line">batch_size, seq_size, dim = <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">embedding = torch.randn(batch_size, seq_size, dim)</span><br><span class="line"></span><br><span class="line">layer_norm = torch.nn.LayerNorm(dim, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y: &quot;</span>, layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y:  tensor([[[ <span class="number">1.5524</span>,  <span class="number">0.0155</span>, -<span class="number">0.3596</span>, -<span class="number">1.2083</span>],</span><br><span class="line">         [ <span class="number">0.5851</span>,  <span class="number">1.3263</span>, -<span class="number">0.7660</span>, -<span class="number">1.1453</span>],</span><br><span class="line">         [ <span class="number">0.2864</span>,  <span class="number">0.0185</span>,  <span class="number">1.2388</span>, -<span class="number">1.5437</span>]],</span><br><span class="line">        [[ <span class="number">1.1119</span>, -<span class="number">0.3988</span>,  <span class="number">0.7275</span>, -<span class="number">1.4406</span>],</span><br><span class="line">         [-<span class="number">0.4144</span>, -<span class="number">1.1914</span>,  <span class="number">0.0548</span>,  <span class="number">1.5510</span>],</span><br><span class="line">         [ <span class="number">0.3914</span>, -<span class="number">0.5591</span>,  <span class="number">1.4105</span>, -<span class="number">1.2428</span>]]])</span><br></pre></td></tr></table></figure><p>接下来手动去进行一下编码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以发现和LayerNorm的结果是一样的，也就是说明Norm是对d_model进行的Norm，会给我们[batch,sqe_length]形状的平均值。<br>加下来进行batch_norm,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_norm = torch.nn.LayerNorm([seq_size,dim], elementwise_affine = <span class="literal">False</span>)</span><br><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1822</span>,  <span class="number">0.4419</span>, -<span class="number">0.3196</span>, -<span class="number">1.9889</span>],</span><br><span class="line">         [-<span class="number">0.6677</span>, -<span class="number">0.2537</span>, -<span class="number">0.8151</span>,  <span class="number">1.5143</span>],</span><br><span class="line">         [ <span class="number">0.7174</span>,  <span class="number">1.2147</span>, -<span class="number">0.0852</span>, -<span class="number">0.9403</span>]],</span><br><span class="line">        [[-<span class="number">0.0138</span>,  <span class="number">1.5666</span>, -<span class="number">2.1726</span>,  <span class="number">1.0590</span>],</span><br><span class="line">         [ <span class="number">0.6646</span>,  <span class="number">0.6852</span>, -<span class="number">0.8706</span>, -<span class="number">0.0442</span>],</span><br><span class="line">         [-<span class="number">0.1163</span>,  <span class="number">0.1389</span>,  <span class="number">0.4454</span>, -<span class="number">1.3423</span>]]])</span><br></pre></td></tr></table></figure><p>可以看到BN的计算的mean形状为[2, 1, 1]，并且Norm结果也和上面的两个不一样，这就充分说明了Norm是在对最后一个维度求平均。<br>那么什么又是Instancenorm呢？接下来再来实现一下instancenorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instance_norm = InstanceNorm2d(<span class="number">3</span>, affine=<span class="literal">False</span>)</span><br><span class="line">output = instance_norm(embedding.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)) <span class="comment">#InstanceNorm2D需要(N,C,H,W)的shape作为输入</span></span><br><span class="line">layer_norm = torch.nn.LayerNorm(<span class="number">4</span>, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br></pre></td></tr></table></figure><p>可以看出无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><p>如果喜欢文章请点个赞，笔者也是一个刚入门Transformer的小白，一起学习，共同努力。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
    <category term="Add&amp;Norm" scheme="https://du2279664786.github.io/tags/Add-Norm/"/>
    
  </entry>
  
</feed>
