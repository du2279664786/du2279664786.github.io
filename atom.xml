<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>江东的笔记</title>
  
  <subtitle>Be overcome difficulties is victory</subtitle>
  <link href="https://du2279664786.github.io/atom.xml" rel="self"/>
  
  <link href="https://du2279664786.github.io/"/>
  <updated>2022-11-07T05:09:17.321Z</updated>
  <id>https://du2279664786.github.io/</id>
  
  <author>
    <name>江东</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="https://du2279664786.github.io/2022/11/07/hello-world/"/>
    <id>https://du2279664786.github.io/2022/11/07/hello-world/</id>
    <published>2022-11-07T14:55:10.000Z</published>
    <updated>2022-11-07T05:09:17.321Z</updated>
    
    <content type="html"><![CDATA[<div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["大家好，这是我的第一篇文章，欢迎查看"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><span id="more"></span><div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["青青陵上柏，磊磊涧中石。", "人生天地间，忽如远行客。","斗酒相娱乐，聊厚不为薄。", "驱车策驽马，游戏宛与洛。","洛中何郁郁，冠带自相索。","长衢罗夹巷，王侯多第宅。","两宫遥相望，双阙百余尺。","极宴娱心意，戚戚何所迫？"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><h1 id="欢迎查看我的第一篇文章"><a href="#欢迎查看我的第一篇文章" class="headerlink" title="欢迎查看我的第一篇文章"></a>欢迎查看我的第一篇文章</h1><p>写在前面：这是我的Github，欢迎star：<a href="https://github.com/du2279664786">https://github.com/du2279664786</a></p><p>大一的时候搭建过一次博客，但是由于长时间不使用，导致配置文件丢失<br>所以在大三上学期我又重新搭建了一下，将去年（大二）所学的知识进行了“简单”的汇总（PS：李明虎老师讲的实在是太丰富了，我只整理了简单的）</p><p>本博客为学习博客，旨在记录自己的学习经历和知识回顾</p><p>转眼间已经大三了，回顾过去两年：<br>        ·大一上：人工智能导论<br>        ·大一下：爬虫<br>        ·大二上：机器学习<br>        ·大二下：深度学习和神经网络<br>….. …..</p><p>希望考研可以成功上岸！</p>]]></content>
    
    
    <summary type="html">&lt;div id=&quot;binft&quot;&gt;&lt;/div&gt;
  &lt;script&gt;
    var binft = function (r) {
      function t() {
        return b[Math.floor(Math.random() * b.length)]
      }  
      function e() {
        return String.fromCharCode(94 * Math.random() + 33)
      }
      function n(r) {
        for (var n = document.createDocumentFragment(), i = 0; r &gt; i; i++) {
          var l = document.createElement(&quot;span&quot;);
          l.textContent = e(), l.style.color = t(), n.appendChild(l)
        }
        return n
      }
      function i() {
        var t = o[c.skillI];
        c.step ? c.step-- : (c.step = g, c.prefixP &lt; l.length ? (c.prefixP &gt;= 0 &amp;&amp; (c.text += l[c.prefixP]), c.prefixP++) : &quot;forward&quot; === c.direction ? c.skillP &lt; t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = &quot;backward&quot;, c.delay = a) : c.skillP &gt; 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = &quot;forward&quot;)), r.textContent = c.text, r.appendChild(n(c.prefixP &lt; l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)
      }
      var l = &quot;&quot;,
      o = [&quot;大家好，这是我的第一篇文章，欢迎查看&quot;].map(function (r) {
      return r + &quot;&quot;
      }),
      a = 2,
      g = 1,
      s = 5,
      d = 75,
      b = [&quot;rgb(110,64,170)&quot;, &quot;rgb(150,61,179)&quot;, &quot;rgb(191,60,175)&quot;, &quot;rgb(228,65,157)&quot;, &quot;rgb(254,75,131)&quot;, &quot;rgb(255,94,99)&quot;, &quot;rgb(255,120,71)&quot;, &quot;rgb(251,150,51)&quot;, &quot;rgb(226,183,47)&quot;, &quot;rgb(198,214,60)&quot;, &quot;rgb(175,240,91)&quot;, &quot;rgb(127,246,88)&quot;, &quot;rgb(82,246,103)&quot;, &quot;rgb(48,239,130)&quot;, &quot;rgb(29,223,163)&quot;, &quot;rgb(26,199,194)&quot;, &quot;rgb(35,171,216)&quot;, &quot;rgb(54,140,225)&quot;, &quot;rgb(76,110,219)&quot;, &quot;rgb(96,84,200)&quot;],
      c = {
        text: &quot;&quot;,
        prefixP: -s,
        skillI: 0,
        skillP: 0,
        direction: &quot;forward&quot;,
        delay: a,
        step: g
      };
      i()
      };
      binft(document.getElementById(&#39;binft&#39;));
  &lt;/script&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>中国大学生计算机设计大赛复盘</title>
    <link href="https://du2279664786.github.io/2022/10/27/2022-10-27%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/"/>
    <id>https://du2279664786.github.io/2022/10/27/2022-10-27%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/</id>
    <published>2022-10-27T14:55:10.000Z</published>
    <updated>2022-11-07T05:01:37.083Z</updated>
    
    <content type="html"><![CDATA[<p>2022年中国大学生计算机设计大赛-人工智能挑战赛-国家二等奖</p><span id="more"></span><p>此篇文章写于2022年10月27日，为了复盘、回顾上一次的计算机设计大赛<br>中国大学生计算机设计大赛</p><h1 id="日程"><a href="#日程" class="headerlink" title="日程"></a>日程</h1><p>5-6月份开始初赛省赛，我和我的两位队友努力的写文档，整理代码，提交了相关资料，由于初赛（省赛）没有答辩，所以差不多等到六月多收到获得省二的通知<br>7月份多得之进了国赛<br>7月底-8月底就开始修改文档，修改代码，录制相关视频，等待国赛的答辩</p><h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><p>项目背景：在数字经济蓬勃发展的时代背景下， “数字化”“智慧化”成为了企业管理转型升级的核心引擎。传统建管理模式已不再符合可持续发展的要求，迫切需要利用以信息技术为代表的现代科技手段，实现中国企业管理转型升级与跨越式发展。为了更好的解决时代问题的痛点，并在一定程度上节约人力成本，本项目设计了一个基于云端和硬件的人工智能多场景实时物体监测平台，将多场景实时监测与报警系统进行了融合创新，做到了监控、检测、分类、识别四位一体的平台建设。<br>整体布局：<br><img src="https://img-blog.csdnimg.cn/d652a2b775ee475f951a36c928d2054e.png" alt="在这里插入图片描述"></p><p>算法介绍：<br>移动物体检测：本项目采用OpenCV（Open Source Computer Vision Library）算法，使用灰度转化、图片缩放和高斯滤波等相关操作，对图像进行预处理，增强了移动物体的可检测性。</p><p>物体识别：为提高精确度并且处理时间尽可能短，本项目采用了运算速度比较快的 Yolov3 算法，它是基于深度学习框架Darknet的目标检测开源项目，不仅可以充分发挥多核处理器和 GPU 并行运算的功能，还可以基于预训练模型进行实时目标检测,预期效果如下：<br><img src="https://img-blog.csdnimg.cn/d1d870e51929499fb0637ed6a3ba3703.png" alt="在这里插入图片描述"><br>以及安全帽检测、疲劳监测、口罩检测<br>具体代码仓库如下：<a href="https://github.com/du2279664786/Chinese-undergraduate-computer-design-contest">https://github.com/du2279664786/Chinese-undergraduate-computer-design-contest</a></p><p>最终获得证书：一个省二证书、一个国二证书、外加一枚金匾<br><img src="https://img-blog.csdnimg.cn/736f6b9f339841fc832f0180d06ab5d8.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/99c107c9a863454e8ee0d84169935b90.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年中国大学生计算机设计大赛-人工智能挑战赛-国家二等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="机器视觉" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>数学建模省一思路及其代码</title>
    <link href="https://du2279664786.github.io/2022/10/14/2022-10-14%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%9C%81%E4%B8%80%E6%80%9D%E8%B7%AF%E5%8F%8A%E5%85%B6%E4%BB%A3%E7%A0%81/"/>
    <id>https://du2279664786.github.io/2022/10/14/2022-10-14%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%9C%81%E4%B8%80%E6%80%9D%E8%B7%AF%E5%8F%8A%E5%85%B6%E4%BB%A3%E7%A0%81/</id>
    <published>2022-10-14T14:55:10.000Z</published>
    <updated>2022-10-21T02:34:41.450Z</updated>
    
    <content type="html"><![CDATA[<p>2022年全国大学生数学建模竞赛山东省一等奖</p><span id="more"></span><p>连肝三天，撸了三天代码，9.17早-9.18晚日夜不停，终于和队友拿下了2022年全国大学生数学建模竞赛山东省一等奖<br>基本思路如下：<br><img src="https://img-blog.csdnimg.cn/c6128ba97db94cf489182b2fe05ee6ec.jpeg"><br>下面是代码部分，仓库连接如下：<a href="https://github.com/du2279664786/CUMCM">https://github.com/du2279664786/CUMCM</a><br><img src="https://img-blog.csdnimg.cn/1b6a9e5ed0054967b790696d20f287ce.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年全国大学生数学建模竞赛山东省一等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中FeedForward层的理解</title>
    <link href="https://du2279664786.github.io/2022/10/10/2022-10-10%E5%AF%B9Transformer%E4%B8%ADFeedForward%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/10/2022-10-10%E5%AF%B9Transformer%E4%B8%ADFeedForward%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-10T14:55:10.000Z</published>
    <updated>2022-10-30T03:04:33.367Z</updated>
    
    <content type="html"><![CDATA[<p>在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强</p><span id="more"></span><p>上一篇我们介绍了<a href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/">对Add&amp;Norm层的理解</a>，有不大熟悉的可以看一下上篇文章。</p><p>今天来说一下Transformer中FeedForward层，首先还是先来回顾一下Transformer的基本结构：首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，然后又做了一个ADD&amp;Norm，再通过Feed Forward进行输出。<br><img src="https://img-blog.csdnimg.cn/4af25c021fd14b70aa2648a925fadf54.png"><br>FeedForward的输入是什么呢？是Multi-Head Attention的输出做了残差连接和Norm之后得数据，然后FeedForward做了两次线性线性变换，为的是更加深入的提取特征。<br><img src="https://img-blog.csdnimg.cn/b976d7add795475fac7bbc6c5f01121f.png" alt="在这里插入图片描述"><br>可以看出在每次线性变换都引入了非线性激活函数Relu，在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强，FeedForward的计算公式如下：max相当于Relu<br><img src="https://img-blog.csdnimg.cn/43b6886add22435795f3f8a7a889c58e.png" alt="在这里插入图片描述"></p><p>所以FeedForward的作用是：通过线性变换，先将数据映射到高纬度的空间再映射到低纬度的空间，提取了更深层次的特征</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Add&amp;Norm层的理解</title>
    <link href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&amp;Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&amp;Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-09T14:55:10.000Z</published>
    <updated>2022-10-18T11:40:29.638Z</updated>
    
    <content type="html"><![CDATA[<p>无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><span id="more"></span><h1 id="Add操作"><a href="#Add操作" class="headerlink" title="Add操作"></a>Add操作</h1><p>首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，再通过Feed Forward进行输出。</p><p>由下图可以看出：在Encoder层和Decoder层中都用到了Add&amp;Norm操作，即残差连接和层归一化操作。<br><img src="https://img-blog.csdnimg.cn/5ef722ad3c5b407482ac132b0883c59c.png" alt="在这里插入图片描述"><br>什么是残差连接呢？残差连接就是把网络的输入和输出相加，即网络的输出为F(x)+x，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就变成了F(x)+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项‘1’，有效解决了梯度消失问题。</p><h1 id="Norm操作"><a href="#Norm操作" class="headerlink" title="Norm操作"></a>Norm操作</h1><p>首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇<a href="https://mp.weixin.qq.com/s/HNCl6MPS_hjTVHNt7UkYyw">文章</a>，才明白了Norm是什么。</p><p>假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]</span><br><span class="line">[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]]</span><br></pre></td></tr></table></figure><p>如果是在做BatchNorm（BN）的话，其计算过程如下：BN1&#x3D;(w11+w12+w13+w14+w41+<br>w42+w43+w44)&#x2F;8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean</p><p>如果是在做LayerNorm（LN）的话，则会进如下计算：LN1&#x3D;(w11+w12+w13+w14+w21+<br>w22+w23+w24+w31+w32+w33+w34)&#x2F;12，同理会得到LN2，最终得到[LN1,LN2]两个mean</p><p>如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1&#x3D;(w11+w12+w13+w14)&#x2F;4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]<br>下图完美的揭示了，这几种Norm<br><img src="https://img-blog.csdnimg.cn/a143d6b41e654fa1849f44580401110c.png" alt="在这里插入图片描述"><br>接下来我们来看一下Transformer中的Norm：首先生成[2,3,4]形状的数据，使用原始的编码方式进行编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> InstanceNorm2d</span><br><span class="line">random_seed = <span class="number">123</span></span><br><span class="line">torch.manual_seed(random_seed)</span><br><span class="line"></span><br><span class="line">batch_size, seq_size, dim = <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">embedding = torch.randn(batch_size, seq_size, dim)</span><br><span class="line"></span><br><span class="line">layer_norm = torch.nn.LayerNorm(dim, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y: &quot;</span>, layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y:  tensor([[[ <span class="number">1.5524</span>,  <span class="number">0.0155</span>, -<span class="number">0.3596</span>, -<span class="number">1.2083</span>],</span><br><span class="line">         [ <span class="number">0.5851</span>,  <span class="number">1.3263</span>, -<span class="number">0.7660</span>, -<span class="number">1.1453</span>],</span><br><span class="line">         [ <span class="number">0.2864</span>,  <span class="number">0.0185</span>,  <span class="number">1.2388</span>, -<span class="number">1.5437</span>]],</span><br><span class="line">        [[ <span class="number">1.1119</span>, -<span class="number">0.3988</span>,  <span class="number">0.7275</span>, -<span class="number">1.4406</span>],</span><br><span class="line">         [-<span class="number">0.4144</span>, -<span class="number">1.1914</span>,  <span class="number">0.0548</span>,  <span class="number">1.5510</span>],</span><br><span class="line">         [ <span class="number">0.3914</span>, -<span class="number">0.5591</span>,  <span class="number">1.4105</span>, -<span class="number">1.2428</span>]]])</span><br></pre></td></tr></table></figure><p>接下来手动去进行一下编码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以发现和LayerNorm的结果是一样的，也就是说明Norm是对d_model进行的Norm，会给我们[batch,sqe_length]形状的平均值。<br>加下来进行batch_norm,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_norm = torch.nn.LayerNorm([seq_size,dim], elementwise_affine = <span class="literal">False</span>)</span><br><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1822</span>,  <span class="number">0.4419</span>, -<span class="number">0.3196</span>, -<span class="number">1.9889</span>],</span><br><span class="line">         [-<span class="number">0.6677</span>, -<span class="number">0.2537</span>, -<span class="number">0.8151</span>,  <span class="number">1.5143</span>],</span><br><span class="line">         [ <span class="number">0.7174</span>,  <span class="number">1.2147</span>, -<span class="number">0.0852</span>, -<span class="number">0.9403</span>]],</span><br><span class="line">        [[-<span class="number">0.0138</span>,  <span class="number">1.5666</span>, -<span class="number">2.1726</span>,  <span class="number">1.0590</span>],</span><br><span class="line">         [ <span class="number">0.6646</span>,  <span class="number">0.6852</span>, -<span class="number">0.8706</span>, -<span class="number">0.0442</span>],</span><br><span class="line">         [-<span class="number">0.1163</span>,  <span class="number">0.1389</span>,  <span class="number">0.4454</span>, -<span class="number">1.3423</span>]]])</span><br></pre></td></tr></table></figure><p>可以看到BN的计算的mean形状为[2, 1, 1]，并且Norm结果也和上面的两个不一样，这就充分说明了Norm是在对最后一个维度求平均。<br>那么什么又是Instancenorm呢？接下来再来实现一下instancenorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instance_norm = InstanceNorm2d(<span class="number">3</span>, affine=<span class="literal">False</span>)</span><br><span class="line">output = instance_norm(embedding.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)) <span class="comment">#InstanceNorm2D需要(N,C,H,W)的shape作为输入</span></span><br><span class="line">layer_norm = torch.nn.LayerNorm(<span class="number">4</span>, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br></pre></td></tr></table></figure><p>可以看出无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><p>如果喜欢文章请点个赞，笔者也是一个刚入门Transformer的小白，一起学习，共同努力。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Add&amp;Norm" scheme="https://du2279664786.github.io/tags/Add-Norm/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中self-attention的理解</title>
    <link href="https://du2279664786.github.io/2022/10/08/2022-10-08%E5%AF%B9Transformer%E4%B8%ADself-attention%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/08/2022-10-08%E5%AF%B9Transformer%E4%B8%ADself-attention%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-08T14:55:10.000Z</published>
    <updated>2022-10-18T11:38:45.094Z</updated>
    
    <content type="html"><![CDATA[<p>Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差</p><span id="more"></span><h1 id="什么是self-attention"><a href="#什么是self-attention" class="headerlink" title="什么是self-attention"></a>什么是self-attention</h1><p>首先我们来看一下Transformer架构：对于input输出，首先进行input embedding，然后再进行positional encoding，将两者相加作为Encoder的输入，也就是输如X<img src="https://img-blog.csdnimg.cn/65ef7c5730514e2cac88d332e2588423.png" alt="在这里插入图片描述"><br>何为self-attention？首先我们要明白什么是attention，对于传统的seq2seq任务，例如中-英文翻译，输入中文，得到英文，即source是中文句子（x1 x2 x3）,英文句子是target（y1 y2 y3）<br><img src="https://img-blog.csdnimg.cn/296c379fd77c475ebf77c06fa8e42e59.png" alt="在这里插入图片描述"><br>attention机制发生在target的元素和source中的所有元素之间。简单的将就是attention机制中的权重计算需要target参与，即在上述Encoder-Decoder模型中，Encoder和Decoder两部分都需要参与运算。</p><p>而对于self-attention，它不需要Decoder的参与，而是source内部元素之间发生的运算，对于输入向量X，对其做线性变换，分别得到Q、K、V矩阵<br><img src="https://img-blog.csdnimg.cn/17358bbb1cf641d78117625fb5a00d31.png" alt="在这里插入图片描述"><br>然后去计算attention，Q、K点乘得到初步的权重因子，并对Q、K点乘结果进行放缩，除以sqrt（dk），Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差，最终再加一个softmax就得到了self attention的输出。<br><img src="https://img-blog.csdnimg.cn/07fdbd802d114b7193c70e9fc451ba22.png" alt="在这里插入图片描述"></p><h1 id="Multi–head-attention"><a href="#Multi–head-attention" class="headerlink" title="Multi–head-attention"></a>Multi–head-attention</h1><p>Multi–head-attention使用了多个头进行运算，捕捉到了更多的信息，多头的数量用h表示，一般h&#x3D;8，表示8个头<br><img src="https://img-blog.csdnimg.cn/7cad1e37120b4d40ad64140677452ec1.png" alt="在这里插入图片描述"><br>在输入每个self-attention之前，我们需将输入X均分的分到h个头中，得到Z1-Z7八个头的输出结果。<br><img src="https://img-blog.csdnimg.cn/d5763f726a5c48f292a140f84c0e5200.png" alt="在这里插入图片描述"><br>对于每个头计算相应的attention score，将其进行拼接，再与W0进行一个线性变换，就得到最终输出的Z。<br><img src="https://img-blog.csdnimg.cn/05bfb9eb87bb4f3b8066c8190c0dff0f.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Positional Encoding的理解</title>
    <link href="https://du2279664786.github.io/2022/10/07/2022-10-07%E5%AF%B9Transformer%E4%B8%ADPositional%20Encoding%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/07/2022-10-07%E5%AF%B9Transformer%E4%B8%ADPositional%20Encoding%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-07T14:55:10.000Z</published>
    <updated>2022-10-19T03:21:15.466Z</updated>
    
    <content type="html"><![CDATA[<p>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值</p><span id="more"></span><p>首先来看一下Transformer结构的结构：<br><img src="https://img-blog.csdnimg.cn/c7ce14349b734567a014c4cf679398fe.png" alt="在这里插入图片描述"><br>Transformer是由Encoder和Decoder两大部分组成，首先对于文本特征，需要进行Embedding，由于transformer抛弃了Rnn的结构，不能捕捉到序列的信息，交换单词位置，得到相应的attention也会发生交换，并不会发生数值上的改变，所以要对input进行Positional Encoding。</p><p>Positional encoding和input embedding是同等维度的，所以可以将两者进行相加，的到输入向量<br><img src="https://img-blog.csdnimg.cn/be30b27838dd411c89d793432ff72582.png" alt="在这里插入图片描述"><br>接下来看一些Positional Encoding的计算公式：<br><img src="https://img-blog.csdnimg.cn/6e9a80e756b94a70aeef8a79097eb7a6.png" alt="在这里插入图片描述"><br>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值，也就是说：对于单个token的d_model维度的词向量，奇数位置取cos，偶数位置取sin，最终的到一个维度和word embedding维度一样的矩阵，接下来可以看一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_positional_encoding</span>(<span class="params">max_seq_len, embed_dim</span>):</span><br><span class="line">    <span class="comment"># 初始化一个positional encoding</span></span><br><span class="line">    <span class="comment"># embed_dim: 字嵌入的维度</span></span><br><span class="line">    <span class="comment"># max_seq_len: 最大的序列长度</span></span><br><span class="line">    positional_encoding = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * i / embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embed_dim)] <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(embed_dim) <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len)])</span><br><span class="line"></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i 偶数</span></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1 奇数</span></span><br><span class="line">    <span class="keyword">return</span> positional_encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">positional_encoding = get_positional_encoding(max_seq_len=<span class="number">100</span>, embed_dim=<span class="number">16</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">sns.heatmap(positional_encoding)</span><br><span class="line">plt.title(<span class="string">&quot;Sinusoidal Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;hidden dimension&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;sequence length&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>首先求初始向量：positional_encoding，然后对其奇数列求sin，偶数列求cos：<br><img src="https://img-blog.csdnimg.cn/2da3445d7953426394ab2e46d8820baf.png" alt="在这里插入图片描述"><br>最终得到positional encoding之后的数据可视化：<br><img src="https://img-blog.csdnimg.cn/507cc7ca0ba34f8fb2ec87216689857c.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Positional Encoding" scheme="https://du2279664786.github.io/tags/Positional-Encoding/"/>
    
  </entry>
  
  <entry>
    <title>怎么理解预训练模型？</title>
    <link href="https://du2279664786.github.io/2022/10/06/2022-10-06%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F/"/>
    <id>https://du2279664786.github.io/2022/10/06/2022-10-06%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F/</id>
    <published>2022-10-06T14:55:10.000Z</published>
    <updated>2022-10-19T03:22:28.472Z</updated>
    
    <content type="html"><![CDATA[<p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性……</p><span id="more"></span><h1 id="什么是预训练"><a href="#什么是预训练" class="headerlink" title="什么是预训练"></a>什么是预训练</h1><p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性，然后将其中的共性“移植”到特定任务的模型中，再使用相关特定领域的少量标注数据进行“微调”，这样的话，模型只需要从”共性“出发，去“学习”该特定任务的“特殊”部分即可。</p><h1 id="预训练的思想"><a href="#预训练的思想" class="headerlink" title="预训练的思想"></a>预训练的思想</h1><p>预训练的思想是：模型的参数不再是随机初始化的，而是通过一些任务进行预先训练，得到一套模型参数，然后用这套参数对模型进行初始化，再进行训练</p><h1 id="CV领域的预训练"><a href="#CV领域的预训练" class="headerlink" title="CV领域的预训练"></a>CV领域的预训练</h1><p>首先对于CV领域图片分类任务，常用的深度学习模型是卷积视神经网络，对于多层的卷积神经网络来说，不同的层学到的特征是不同的，为了捕获更多的特征，浅层的感受野较小，所以浅层学到的特征往往是更加通用的，包含更多的像素点的信息，比如一些细粒度的信息：颜色、纹理、边缘等。<br>通常在大规模图片数据上预先获得‘通用特征’，然后再去做下游任务：<br><img src="https://img-blog.csdnimg.cn/455a209bb08941ff8d390ede716556b4.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性……&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="预训练" scheme="https://du2279664786.github.io/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>HuggingFace的安装和编码</title>
    <link href="https://du2279664786.github.io/2022/10/05/2022-10-05HuggingFace%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A0%81/"/>
    <id>https://du2279664786.github.io/2022/10/05/2022-10-05HuggingFace%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A0%81/</id>
    <published>2022-10-05T14:55:10.000Z</published>
    <updated>2022-10-18T11:36:57.563Z</updated>
    
    <content type="html"><![CDATA[<p>模型的加载和编码以及基本的使用功能</p><span id="more"></span><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>使用以下命令安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></table></figure><h1 id="模型的加载"><a href="#模型的加载" class="headerlink" title="模型的加载"></a>模型的加载</h1><p>导入包：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from transformers import BertTokenizer</span><br></pre></td></tr></table></figure><p>加载预训练模型bert-base-chinese，初次加载可能需要较长的时间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#加载预训练字典和分词方法</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(</span><br><span class="line"># 下载或者从本地加载模型</span><br><span class="line">    pretrained_model_name_or_path=&#x27;bert-base-chinese&#x27;,</span><br><span class="line">    force_download=False,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>接下来就可以看到tokenizer的内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br><span class="line"></span><br><span class="line">#print:(name_or_path=&#x27;bert-base-chinese&#x27;, vocab_size=21128, model_max_len=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;unk_token&#x27;: &#x27;[UNK]&#x27;, &#x27;sep_token&#x27;: &#x27;[SEP]&#x27;, &#x27;pad_token&#x27;: &#x27;[PAD]&#x27;, &#x27;cls_token&#x27;: &#x27;[CLS]&#x27;, &#x27;mask_token&#x27;: &#x27;[MASK]&#x27;&#125;)</span><br></pre></td></tr></table></figure><h1 id="进行编码"><a href="#进行编码" class="headerlink" title="进行编码"></a>进行编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sents = [</span><br><span class="line">    &#x27;我在一所学院上大三。&#x27;,</span><br><span class="line">    &#x27;今天天气好，我来图书馆学习。&#x27;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">res= tokenizer.encode(</span><br><span class="line">    # 可以一次编码两个句子，即text和text_pair</span><br><span class="line">    text = sents[0],</span><br><span class="line">    text_pair=sents[1],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    return_tensors=None,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以打印出res的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 101为CLS    102为SEP    0为PAD</span><br><span class="line">#print(res):[101, 2769, 1762, 671, 2792, 2110, 7368, 677, 1920, 676, 511, 102, 791, 1921, 1921, 3698, 1962, 8024, 2769, 3341, 1745, 741, 7667, 2110, 739, 511, 102, 0, 0, 0]</span><br></pre></td></tr></table></figure><p>也可以查看编码之后的结果:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decode(res)</span><br><span class="line"># [CLS] 我 在 一 所 学 院 上 大 三 。 [SEP] 今 天 天 气 好 ， 我 来 图 书 馆 学 习 。 [SEP] [PAD] [PAD] [PAD]</span><br></pre></td></tr></table></figure><h1 id="多功能编码"><a href="#多功能编码" class="headerlink" title="多功能编码"></a>多功能编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">res = tokenizer.encode_plus(</span><br><span class="line">    # 可以一次编码两个句子</span><br><span class="line">    text = sents[0],</span><br><span class="line">    text_pair=sents[1],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    </span><br><span class="line">    # 返回值的类型：tf  pt  np</span><br><span class="line">    return_tensors=None,</span><br><span class="line">    </span><br><span class="line">    # 返回token_type_ids</span><br><span class="line">    return_token_type_ids=True,</span><br><span class="line">    </span><br><span class="line">    # 返回attention_mask</span><br><span class="line">    return_attention_mask = True,</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # 返回length标识长度</span><br><span class="line">    return_length = True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看编码的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for k,v in res.items():</span><br><span class="line">    print(k,&#x27;:&#x27;,v)</span><br><span class="line">    </span><br><span class="line">tokenizer.decode(res[&#x27;input_ids&#x27;])</span><br><span class="line"></span><br><span class="line"># input_ids:编码后的句子</span><br><span class="line"># token_type_ids:第一个句子和特殊符号的位置是0，第二个句子的位置是1</span><br><span class="line"># attention_mask:pad的位置是0，其它位置是1</span><br><span class="line"># length:返回句子的长度</span><br></pre></td></tr></table></figure><h1 id="将句子批量进行编码"><a href="#将句子批量进行编码" class="headerlink" title="将句子批量进行编码"></a>将句子批量进行编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">out = tokenizer.batch_encode_plus(</span><br><span class="line">    batch_text_or_text_pairs=[sents[0],sents[1]],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    </span><br><span class="line">    # 返回值的类型：tf  pt  np</span><br><span class="line">    return_tensors=None,</span><br><span class="line">    </span><br><span class="line">    # 返回token_type_ids</span><br><span class="line">    return_token_type_ids=True,</span><br><span class="line">    </span><br><span class="line">    # 返回attention_mask</span><br><span class="line">    return_attention_mask = True,</span><br><span class="line">    </span><br><span class="line">    # 返回length标识长度</span><br><span class="line">    return_length = True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;模型的加载和编码以及基本的使用功能&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>深度学习课程回顾</title>
    <link href="https://du2279664786.github.io/2022/07/10/2022-07-10%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/"/>
    <id>https://du2279664786.github.io/2022/07/10/2022-07-10%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/</id>
    <published>2022-07-10T14:55:10.000Z</published>
    <updated>2022-10-21T01:34:08.778Z</updated>
    
    <content type="html"><![CDATA[<p>大二下学期深度学习课程回顾</p><span id="more"></span><h1 id="第1章：深度学习概述"><a href="#第1章：深度学习概述" class="headerlink" title="第1章：深度学习概述"></a>第1章：深度学习概述</h1><p>BP神经网络的反向传播（《深度学习08_小复习.pptx》，链式法则：损失函数，激活函数、信号强度；每多一层多一个sigma符号：也是网络不能太深的原因之一）<br>    参数确定（权值个数和偏置个数）</p><h1 id="第2章：Pytorch简介"><a href="#第2章：Pytorch简介" class="headerlink" title="第2章：Pytorch简介"></a>第2章：Pytorch简介</h1><p>Pytorch安装要点：装python、装CUDA、装gpu版的torch<br>Linux编译安装Python的主要步骤：下载、解压、配置(.&#x2F;configure)、编译安装(make &amp;&amp; make install)<br>Tensor的基本知识：数据方面就是numpy，计算图方面是张量的本质。</p><h1 id="第3章：Pytorch计算图"><a href="#第3章：Pytorch计算图" class="headerlink" title="第3章：Pytorch计算图"></a>第3章：Pytorch计算图</h1><p>《深度学习08_小复习.pptx》<br>给几个公式，可以画出计算图，给计算图，可以复原出公式。<br>给计算图，可以向上算数据，可以算回传梯度<br>Pytorch的计算图结构：1、是否所有的叶子节点都可以设置为需要计算梯度，是否所有的叶子节点都需要计算梯度。2、是否默认所有的计算图都可以回传多次。3、计算图是否一个有向无环图。4、是否默认可以让中间节点保存梯度。5、非叶子节点的导函数是怎么来的？</p><h1 id="第4章：线性回归"><a href="#第4章：线性回归" class="headerlink" title="第4章：线性回归"></a>第4章：线性回归</h1><p>分类与回归，线性回归的基本概念，可用的解法。<br>小批量随机梯度下降法。<br>看网络写方程，看方程画网络。<br>数据加载、损失函数。</p><h1 id="第5章：Softmax回归"><a href="#第5章：Softmax回归" class="headerlink" title="第5章：Softmax回归"></a>第5章：Softmax回归</h1><p>Softmax和交叉熵，代码、计算、业务场景，<br>权值、偏置初始化<br>view()的业务意义</p><h1 id="第6章：多层感知机"><a href="#第6章：多层感知机" class="headerlink" title="第6章：多层感知机"></a>第6章：多层感知机</h1><p>单层和多层的区别（非线性激活函数的意义）<br>权值、偏置初始化，MLP的完备性<br>Relu激活函数</p><h1 id="第7章：模型训练与深度学习计算"><a href="#第7章：模型训练与深度学习计算" class="headerlink" title="第7章：模型训练与深度学习计算"></a>第7章：模型训练与深度学习计算</h1><p>为什么网络不能过深<br>Wd的原因和表现<br>Dropout的原理和实现<br>模型的读写<br>GPU计算的特点和历史<br>常用的框架代码：比如参数初始化、损失函数定义、优化器定义</p><h1 id="第8章：卷积"><a href="#第8章：卷积" class="headerlink" title="第8章：卷积"></a>第8章：卷积</h1><p>卷积的思想基础，能够解释局部性和平移不变性<br>对于某个像素的信息，应当只与其附近的像素有关系，超出一定的距离以后则无关<br>对某个区域进行的特征提取所得到的输出并不会由于平移操作而发生变化<br>卷积、池化的代码、计算<br>步幅、填充、多通道<br>    卷积和池化是否可以用相同的步幅、填充、输入通道、输出通道<br>1*1卷积</p><h1 id="第9章：机器视觉初步"><a href="#第9章：机器视觉初步" class="headerlink" title="第9章：机器视觉初步"></a>第9章：机器视觉初步</h1><p>LeNet、AlexNet、Vgg、NiN、GoogLeNet、ResNet<br>LeNet、典型的VGG块、inception块、Res块<br>过拟与DA，常用的DA技术<br>每个网络的名字的意义，历史脉络、网络深了是更容易过拟还是会减轻过拟。</p><h1 id="第10章：循环神经网络"><a href="#第10章：循环神经网络" class="headerlink" title="第10章：循环神经网络"></a>第10章：循环神经网络</h1><p>N元语法、词向量，两种常用的向量训练方法。<br>二阶马尔可夫展开<br>RNN、LSTM、GRU</p><h1 id="论述："><a href="#论述：" class="headerlink" title="论述："></a>论述：</h1><p>计算图、框架的作用<br>深度学习、卷积神经网络：简史、意义、动力<br>深度学习与机器学习的联系与区别<br>机器学习的核心理念<br>三个以上的观点，可以是自己的合理的观点，言之有物，自圆其说，字数达标，每个关键观点可酌情给予3-4分。<br>纯网络、教材内容不会得到高分。<br>如果有雷同答案，将判为抄袭，本题不得分，不区分抄袭与被抄袭者，欢迎你和同学讨论，但不要将你的完成文稿给别人，如果你已经给了别人，建议你自己再写一份。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大二下学期深度学习课程回顾&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="总结复盘" scheme="https://du2279664786.github.io/tags/%E6%80%BB%E7%BB%93%E5%A4%8D%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>机器学习课程回顾</title>
    <link href="https://du2279664786.github.io/2022/07/08/2022-07-08%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/"/>
    <id>https://du2279664786.github.io/2022/07/08/2022-07-08%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/</id>
    <published>2022-07-08T14:55:10.000Z</published>
    <updated>2022-10-21T01:33:12.143Z</updated>
    
    <content type="html"><![CDATA[<p>大二上学期机器学习课程回顾</p><span id="more"></span><h1 id="1-基本概念部分"><a href="#1-基本概念部分" class="headerlink" title="1.基本概念部分"></a>1.基本概念部分</h1><p>1、统计学习方法可以概括如下：……..<br>2、什么是有监督学习、无监督学习、半监督学习<br>3、有监督的学习的三要素两过程<br>4、生成式模型和判别式模型是什么意思，常见的代表模型有哪几个<br>5、什么叫过拟合、欠拟合，常用的减轻拟合的方法<br>6、如果clf是一个模拟的对象，则一般clf.train(X, y), clf.fit(X, y), clf.predict(test)是什么意思，执行后的结果或改变是什么<br>7、Precision, Recall, F1, Accuracy, AUC of ROC。上面这几个概念的定义、意义、计算。给定正负例的信号强度，能画出ROC<br>8、训练集、验证集、测试集的作用是什么，S折交叉验证是怎么回事<br>9、什么叫回归，什么叫聚类，什么叫分类</p><h1 id="2-Knn"><a href="#2-Knn" class="headerlink" title="2.Knn"></a>2.Knn</h1><p>中英文名字、算法理念、算法过程、算法伪代码，算法代码实现</p><h1 id="3-Kmeans"><a href="#3-Kmeans" class="headerlink" title="3.Kmeans"></a>3.Kmeans</h1><p>中英文名字、算法理念、算法过程、算法伪代码，算法代码实现</p><h1 id="4-最优化问题"><a href="#4-最优化问题" class="headerlink" title="4.最优化问题"></a>4.最优化问题</h1><p>最优化问题，迭代最优化问题，梯度下降法都是什么意思。<br>梯度下降法的算法理念、算法过程、算法伪代码、停机条件<br>给定函数、当前自变量、学习率，可算出下一次迭代的自变量<br>给定函数，能求出argmin和min</p><h1 id="5-感知机"><a href="#5-感知机" class="headerlink" title="5.感知机"></a>5.感知机</h1><p>算法理念、算法过程、算法伪代码，算法代码实现<br>感知机解的情况和业务意义，感知机的局限，感知机在机器学习中的地位</p><h1 id="6-线性回归"><a href="#6-线性回归" class="headerlink" title="6.线性回归"></a>6.线性回归</h1><p>线性回归的定义，解法，解的情况，广义线性回归<br>会手算简单的线性回归(单变量)</p><h1 id="7-逻辑回归"><a href="#7-逻辑回归" class="headerlink" title="7.逻辑回归"></a>7.逻辑回归</h1><p>线性回归的定义，解法，解的情况<br>Sigmoid函数及求导，求解最大似然估计</p><h1 id="8-朴素贝叶斯"><a href="#8-朴素贝叶斯" class="headerlink" title="8.朴素贝叶斯"></a>8.朴素贝叶斯</h1><p>给定一个小规模数据集，可以手算朴素贝叶斯</p><h1 id="9-决策树"><a href="#9-决策树" class="headerlink" title="9.决策树"></a>9.决策树</h1><p>决策树的基本算法<br>熵、基尼、熵增益、固有值、熵增益比的定义和业务意义<br>ID3、C4.5、Cart算法基本思路和伪代码</p><h1 id="10-提升方法"><a href="#10-提升方法" class="headerlink" title="10.提升方法"></a>10.提升方法</h1><p>Bagging和随机森林<br>能说清楚GBDT的脉络即：<br>adaboost的理念，加法模型，前向加法模型，提升树，回归树对残差的拟合，以及对梯度的拟合。</p><h1 id="11-SVM"><a href="#11-SVM" class="headerlink" title="11.SVM"></a>11.SVM</h1><p>线性可分支持向量机的基本脉络<br>松弛变量、核函数的业务背景和操作方法<br>SMO算法的大致过程</p><h1 id="12-NN"><a href="#12-NN" class="headerlink" title="12.NN"></a>12.NN</h1><p>说清楚神经网络学习和预测的过程<br>了解常见的神经网络，及中英文名称<br>对于多层神经网络，可以计算其待定参数的个数，并能说明BP算法如何更新网络参数</p><h1 id="13-Numpy"><a href="#13-Numpy" class="headerlink" title="13.Numpy"></a>13.Numpy</h1><p>基本的向量化运算，使用numpy常见的方法</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大二上学期机器学习课程回顾&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="总结复盘" scheme="https://du2279664786.github.io/tags/%E6%80%BB%E7%BB%93%E5%A4%8D%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>Python切换源，快速下载文件</title>
    <link href="https://du2279664786.github.io/2022/06/05/2022-06-05Python%E5%88%87%E6%8D%A2%E6%BA%90%EF%BC%8C%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/"/>
    <id>https://du2279664786.github.io/2022/06/05/2022-06-05Python%E5%88%87%E6%8D%A2%E6%BA%90%EF%BC%8C%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/</id>
    <published>2022-06-05T14:55:10.000Z</published>
    <updated>2022-10-21T01:39:16.932Z</updated>
    
    <content type="html"><![CDATA[<p>快速下载文件</p><span id="more"></span><h1 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h1><p>进入电脑路径’C:\Users\user_name\AppData\Roaming\pip’文件夹下，找到pip.ini文件<br><img src="https://img-blog.csdnimg.cn/d02a90ee68bd40248c2b39ea89154681.png" alt="在这里插入图片描述"><br>若没有此文件则可以创建一个pip文件，后缀为ini，，然后打开文件，将文件内容替换为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">global</span>]</span><br><span class="line"></span><br><span class="line">index-url=http://pypi.douban.com/simple</span><br><span class="line"></span><br><span class="line">[install]</span><br><span class="line"></span><br><span class="line">trusted-host=pypi.douban.com</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>保存文件即可<br><img src="https://img-blog.csdnimg.cn/435b149734414ae49fd8cb8cfbe6f124.png" alt="在这里插入图片描述"><br>下载速度惊人<br><img src="https://img-blog.csdnimg.cn/e904985a074142faaab9e22a7e273cc5.png" alt="在这里插入图片描述"><br>如果文章对您有帮助，请点赞+收藏</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;快速下载文件&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SOFTMAX回归模型</title>
    <link href="https://du2279664786.github.io/2022/05/08/2022-05-08SOFTMAX%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>https://du2279664786.github.io/2022/05/08/2022-05-08SOFTMAX%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-05-08T14:55:10.000Z</published>
    <updated>2022-10-18T09:14:53.067Z</updated>
    
    <content type="html"><![CDATA[<p>SOFTMAX函数的脉络梳理</p><span id="more"></span><p>2022-05-08SOFTMAX回归模型</p><h2 id="什么是SOFTMAX回归函数"><a href="#什么是SOFTMAX回归函数" class="headerlink" title="什么是SOFTMAX回归函数"></a>什么是SOFTMAX回归函数</h2><p>·softmax回归跟线性回归⼀样将输⼊特征与权᯿做线性叠加<br>·与线性回归的⼀个主要不同在于，softmax回归的输出值个数等于标签⾥的类别数<br>·SOFTMAX是一个单层的神经网络<br>结构图如下：<br><img src="https://img-blog.csdnimg.cn/1eef7cdde9b04e1d866471e374b3a80b.png" alt="在这里插入图片描述"><br>运算过程如下：<br><img src="https://img-blog.csdnimg.cn/4e928316894e4e298b9e71c18598fbd1.png" alt="在这里插入图片描述"><br>即我们通过神经网络预测，然后得到相应的一个分数，此时我们希望我们得到的分数是一个概率：<br><img src="https://img-blog.csdnimg.cn/c16934676c254f119005ddefbb0d5164.png" alt="在这里插入图片描述"><br>此时我们就应该选取一个合适的方案，来将预测的分数来转化为标签的概率，那么最合适的肯定是softmax了，softmax公式：<br><img src="https://img-blog.csdnimg.cn/df4b36ec76f44413b348ad2917c4d399.png" alt="在这里插入图片描述"><br>即将得到的分数都进行exp，然后求每个标签占比(概率)，最终我们得到的是预测概率最大的标签：<br><img src="https://img-blog.csdnimg.cn/656cab1ab05b4020ac06c14f0ea28951.png" alt="在这里插入图片描述"><br>很显然，我们只关注预测概率最大的标签（单标签预测）<br>总结了一下特性：<br>    ·结果都为正数：即将得分为负数的进行转化<br>    ·所有求和为1：所有概率相加等于1<br>    ·平移不变性：所有得分平移得到的结果不受影响<br>    ·最大–&gt;最大：预测得分最大的概率也大</p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>在这里我们使用的是交叉熵损失函数（Cross Entropy）<br><img src="https://img-blog.csdnimg.cn/eded5e651bfd471dada59239dc2b6c9d.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/cf23b1b754ab4f3eae57f49f5f12b0f1.png" alt="其中带下标的是向量 中⾮0即1的元素"></p><h2 id="实现代码："><a href="#实现代码：" class="headerlink" title="实现代码："></a>实现代码：</h2><p>·导入包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>若出现没有‘d2lzh_pytorch’这个包，<a href="https://blog.csdn.net/weixin_51756104/article/details/124626354?spm=1001.2014.3001.5501">点击此处离线安装</a><br>·导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>·定义网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, 1, 28, 28)</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"> </span><br><span class="line">net = LinearNet(num_inputs,num_outputs)</span><br></pre></td></tr></table></figure><p>·初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>·定义损失函数和优化器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>·训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;SOFTMAX函数的脉络梳理&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="SOFTMAX" scheme="https://du2279664786.github.io/tags/SOFTMAX/"/>
    
  </entry>
  
  <entry>
    <title>d2lzh_pytorch包离线安装</title>
    <link href="https://du2279664786.github.io/2022/05/07/2022-05-07d2lzh_pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/"/>
    <id>https://du2279664786.github.io/2022/05/07/2022-05-07d2lzh_pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/</id>
    <published>2022-05-07T14:55:10.000Z</published>
    <updated>2022-10-18T09:12:51.207Z</updated>
    
    <content type="html"><![CDATA[<p>线上安装经常出错，所以可以选择离线安装</p><span id="more"></span><p>在导入d2lzh_pytorch包时，一般会报错：<br><img src="https://img-blog.csdnimg.cn/56f5ff1ee876479e9707f06a234a04dd.png" alt="在这里插入图片描述"><br>我们可以离线下载包：<br>链接：<a href="https://pan.baidu.com/share/init?surl=iXyFqY8uM5PGhrthL_-9xQ#list/path=/">点击此处</a>，提取码：1314<br>下载后：进入到我们要使用的环境，按照如下位置安放包<br><img src="https://img-blog.csdnimg.cn/79fcfef8196a43c8bb122b712b2dce9d.png" alt="在这里插入图片描述"><br>最后就可以导入了：<br><img src="https://img-blog.csdnimg.cn/4c3b27ce6aa9488494f97a336101d894.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;线上安装经常出错，所以可以选择离线安装&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="d2lzh_pytorch包离线安装" scheme="https://du2279664786.github.io/tags/d2lzh-pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>线性回归的简洁实现</title>
    <link href="https://du2279664786.github.io/2022/04/28/2022-04-28%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://du2279664786.github.io/2022/04/28/2022-04-28%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-04-28T14:55:10.000Z</published>
    <updated>2022-10-18T09:11:02.360Z</updated>
    
    <content type="html"><![CDATA[<p>创建单层神经网络</p><span id="more"></span><p>线性回归详细实现，<a href="https://blog.csdn.net/weixin_51756104/article/details/124334225">请点击此处</a></p><h3 id="导入包："><a href="#导入包：" class="headerlink" title="导入包："></a>导入包：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data    <span class="comment"># 数据读取</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init     <span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim   <span class="comment"># 优化器</span></span><br></pre></td></tr></table></figure><h3 id="数据集的生成"><a href="#数据集的生成" class="headerlink" title="数据集的生成"></a>数据集的生成</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span>     <span class="comment"># 2个维度</span></span><br><span class="line">num_examples = <span class="number">1000</span>     <span class="comment"># 1000调数据</span></span><br><span class="line"><span class="comment"># 标准的参数</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples,num_inputs)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">label = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] +true_b</span><br><span class="line">label += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>,size=label.size()), dtype=torch.<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure><h3 id="数据的读取"><a href="#数据的读取" class="headerlink" title="数据的读取"></a>数据的读取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">30</span></span><br><span class="line">dataset = Data.TensorDataset(features,label)</span><br><span class="line">data_it = Data.DataLoader(dataset,batch_size,shuffle =<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>定义模型有多种方法：<br>方法一：继承nn.Module</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,n_feature</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(n_feature,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br></pre></td></tr></table></figure><p>方法二：nn.Sequential</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line"> )</span><br></pre></td></tr></table></figure><p>方法三：nn.Sequential()+add_module</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>方法四：导入OrderedDict</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))]))</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias,val=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="MSE损失函数"><a href="#MSE损失函数" class="headerlink" title="MSE损失函数"></a>MSE损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure><h3 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure><h3 id="模型的优化"><a href="#模型的优化" class="headerlink" title="模型的优化"></a>模型的优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_it:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output, y.view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(epoch, l.item())</span><br></pre></td></tr></table></figure><p>最后输出epoch和loss：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">0.3572668433189392</span></span><br><span class="line"><span class="number">2</span> <span class="number">0.005662666633725166</span></span><br><span class="line"><span class="number">3</span> <span class="number">0.00011592111695790663</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建单层神经网络&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>史上最详细的Pytorch+CUDA+CUDNN的安装(GPU版)</title>
    <link href="https://du2279664786.github.io/2022/04/27/2022-04-27%E5%8F%B2%E4%B8%8A%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84Pytorch+CUDA+CUDNN%E7%9A%84%E5%AE%89%E8%A3%85(GPU%E7%89%88)/"/>
    <id>https://du2279664786.github.io/2022/04/27/2022-04-27%E5%8F%B2%E4%B8%8A%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84Pytorch+CUDA+CUDNN%E7%9A%84%E5%AE%89%E8%A3%85(GPU%E7%89%88)/</id>
    <published>2022-04-27T14:55:10.000Z</published>
    <updated>2022-10-18T09:09:30.930Z</updated>
    
    <content type="html"><![CDATA[<p>炒鸡详细的pytorch GPU安装版本，从0搭建！</p><span id="more"></span><p>CPU版本的教程<a href="https://blog.csdn.net/weixin_51756104/article/details/124222546">请点击此处查看</a></p><h3 id="首先看一下自己的驱动："><a href="#首先看一下自己的驱动：" class="headerlink" title="首先看一下自己的驱动："></a>首先看一下自己的驱动：</h3><p>·如果驱动不支持CUDA11的话就要先更新驱动<br>·打开命令行win+r，输入cmd，在命令行输入：nvidia-smi   查看信息<br><img src="https://img-blog.csdnimg.cn/f1d12b1f1cb24e11a5e70143ef5c1195.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>这里可以看到我的驱动是512.2，根据下图可以看到驱动只要大于451.22就支持CUDA11，,pytorch最新本已经不支持CUDA10,如果驱动版本低于451,可以升级驱动，<a href="https://www.nvidia.com/en-us/geforce/drivers/">点击此处下载驱动</a>，下面是CUDA和显卡驱动对应的版本：<br><img src="https://img-blog.csdnimg.cn/ea404a8d657e418eaa0ab409ba8386f7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="安装Pytorch"><a href="#安装Pytorch" class="headerlink" title="安装Pytorch"></a>安装Pytorch</h3><p>此处使用的是本地安装(因为pip安装和conda安装本人都没有成功，可能是网络问题),<a href="https://download.pytorch.org/whl/torch_stable.html">点击此处</a>进行Pytorch的下载：可以看到我的CUDA是11.6版本：<br><img src="https://img-blog.csdnimg.cn/06c6e268cdfa4a2da9224cb2ba7f9b48.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>我们进入下载pytorch的网站，发现还没有CUDA11.6版本，我们可以下载CUDA11.5版本，<br>cu115代表CUDA11.5版本，cp38代表python的版本，选择合适的进行下载，我下载的是CUDA11.5版本，Python版本3.8,所以我们选择：<br><img src="https://img-blog.csdnimg.cn/ac64598e145c4b3b943c9516ba7df188.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>同理我们再选择torchvison和torchaudio的下载，下载完成后进行本地安装：使用pip install+安装包的路径安装，我的在D盘：<br><img src="https://img-blog.csdnimg.cn/5e018a995bc3415faf2ccdb8be245aa7.png" alt="："><br>此时我们就可以检测一下是否安装成功：<br><img src="https://img-blog.csdnimg.cn/2c27cd5cb5084ae3a502032a12bd7316.png" alt="在这里插入图片描述">可以看到已经成功了！！！</p><h3 id="CUDA安装"><a href="#CUDA安装" class="headerlink" title="CUDA安装"></a>CUDA安装</h3><p><a href="https://developer.nvidia.com/cuda-toolkit-archive">点击此处</a>，进入下载，选择合适自己的版本：<br><img src="https://img-blog.csdnimg.cn/91fb78d977294a21ae0068b7f1eb8234.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>选择好信息开始下载：<br><img src="https://img-blog.csdnimg.cn/5b0e8251a48640f9b1490f335f916d84.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>点击安装包，一路默认安装就行</p><h3 id="CUDNN安装"><a href="#CUDNN安装" class="headerlink" title="CUDNN安装"></a>CUDNN安装</h3><p><a href="https://developer.nvidia.com/rdp/cudnn-download">点击此处</a>，自行注册账号</p><p><img src="https://img-blog.csdnimg.cn/fcc85c8aeb184e60bf7f1899a3d3e681.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后点击下载：需要填写调查问卷，点击提交<br><img src="https://img-blog.csdnimg.cn/55be6915942542f2b7e61f587a44e4fa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>找到相应的版本：<br><img src="https://img-blog.csdnimg.cn/84f7be9ba3e74a5bb4566b39b57ea958.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>这里我的是windows CUDA11.6，所以我下载windows版本的压缩包<br><img src="https://img-blog.csdnimg.cn/cb5883133ef64418adefffb76c9aebfa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>下载完进行解压：<br><img src="https://img-blog.csdnimg.cn/69864e29e15c46afa22218a9edb63b6e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后找到我们CUDA11.6的位置，默认安装的在：C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA<br><img src="https://img-blog.csdnimg.cn/70e4ce0c5ba84be48fd10eecd82524af.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后我们找到刚刚解压的cudnn文件夹<br><img src="https://img-blog.csdnimg.cn/87f93e9696584e929c591d283fc883c2.png" alt="在这里插入图片描述"><br>将bin，include，lib文件夹下里面的‘文件’分别复制到CUDA相应的文件夹里面（复制的是里面的的文件，不是文件夹）：<br><img src="https://img-blog.csdnimg.cn/69d6624d180c40d999de7c7aa76bc6dc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h3><p>·查看CUDA，在命令行输入：nvcc -V，出现以下代表成功：<br><img src="https://img-blog.csdnimg.cn/2bf5e7e797224b03aa842149bab8c4cf.png" alt="在这里插入图片描述"><br>·查看cudnn，我们在命令行进入安装cuda的目录，我的是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11<span class="number">.6</span>\extras\demo_suite</span><br></pre></td></tr></table></figure><p>然后在命令行进入文件夹：<br><img src="https://img-blog.csdnimg.cn/3be6ef8c961743acad34f275013dcd1d.png" alt="在这里插入图片描述"><br>输入：bandwidthTest.exe<br><img src="https://img-blog.csdnimg.cn/c5722a063f564e788682cd13491f759a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>输入：deviceQuery.exe<br><img src="https://img-blog.csdnimg.cn/cc1b0f43a5944f03b12627fd3738dc74.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>表示安装成功！！！<br>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;炒鸡详细的pytorch GPU安装版本，从0搭建！&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="CUDA" scheme="https://du2279664786.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch线性回归的详细实现</title>
    <link href="https://du2279664786.github.io/2022/04/26/2022-04-26Pytorch%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%9E%E7%8E%B0/"/>
    <id>https://du2279664786.github.io/2022/04/26/2022-04-26Pytorch%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-04-26T14:55:10.000Z</published>
    <updated>2022-10-18T11:29:12.034Z</updated>
    
    <content type="html"><![CDATA[<p>创建单层神经⽹络</p><span id="more"></span><h2 id="线性回归-单层神经网络"><a href="#线性回归-单层神经网络" class="headerlink" title="线性回归-单层神经网络"></a>线性回归-单层神经网络</h2><p>线性回归是⼀个单层神经⽹络<br><img src="https://img-blog.csdnimg.cn/590a40befa164375b9ceb587cda58445.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&amp;emsp;输⼊分别为x1和x2，因此输⼊层的输⼊个数为2,输⼊个数也叫特征数或<br>特征向量维度,输出层的输出个数为1,输出层中的神经元和输⼊层中各个输⼊完全连<br>接,因此，这⾥的输出层⼜叫全连接层,即一个简单地线性回归。<br>&amp;emsp;假设我们有三个预测数据：<br><img src="https://img-blog.csdnimg.cn/e73f69360b1d4f4b90360de495dac343.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>转化为矩阵运算：<br><img src="https://img-blog.csdnimg.cn/eec385599fc3439c90c79ebce9737f30.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>即<br><img src="https://img-blog.csdnimg.cn/10cf121677624495bc8ad82f4f203933.png" alt="在这里插入图片描述"></p><h2 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h2><p>首先导入所需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure><p>生成数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_input = <span class="number">2</span></span><br><span class="line">num_example = <span class="number">1000</span>   <span class="comment"># 1000条样本</span></span><br><span class="line"><span class="comment"># 定义标准的参数</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]   </span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">np.random.seed(<span class="number">2012</span>)</span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">1000</span>,<span class="number">2</span>)))</span><br><span class="line"><span class="comment"># 构造标签</span></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] +true_b</span><br><span class="line">labels += torch.from_numpy(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>,size=labels.size()))</span><br><span class="line"><span class="built_in">print</span>(features,labels)</span><br></pre></td></tr></table></figure><p>数据的读取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_item</span>(<span class="params">bach_size,features,labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices) <span class="comment"># 样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, bach_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: <span class="built_in">min</span>(i + bach_size,num_examples)]) <span class="comment"># 最后⼀次可能不⾜⼀个batch</span></span><br><span class="line">        <span class="keyword">yield</span> features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br></pre></td></tr></table></figure><p>随机初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_input, <span class="number">1</span>)),dtype=torch.double) </span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.double)</span><br><span class="line">w.requires_grad = <span class="literal">True</span>     <span class="comment"># 定义为可求梯度</span></span><br><span class="line">b.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>定义线性回归函数，使⽤ mm 函数(矩阵相乘)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x,w,b</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.mm(x,w)+b</span><br></pre></td></tr></table></figure><p>定义损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">y_hat, y</span>): <span class="comment"># 本函数已保存在d2lzh_pytorch包中⽅便以后使⽤</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><p>定义优化函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">params, lr, batch_size</span>): <span class="comment"># 本函数已保存在d2lzh_pytorch包中⽅便以后使⽤</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size <span class="comment"># 修改的的param.data</span></span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">bach_size = <span class="number">30</span></span><br><span class="line">net = linear</span><br><span class="line">loss = loss</span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> data_item(bach_size=bach_size,features=features,labels=labels):</span><br><span class="line">        los = loss(linear(x,w,b),y).<span class="built_in">sum</span>()</span><br><span class="line">        los.backward()</span><br><span class="line">        </span><br><span class="line">        SGD([w,b],lr=lr,batch_size=bach_size)</span><br><span class="line"><span class="comment">#         print(b)</span></span><br><span class="line">        w.grad.zero_()</span><br><span class="line">        b.grad.zero_()</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br><span class="line"><span class="built_in">print</span>(true_w, <span class="string">&#x27;\n&#x27;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(true_b, <span class="string">&#x27;\n&#x27;</span>, b)</span><br></pre></td></tr></table></figure><p>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建单层神经⽹络&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="线性回归" scheme="https://du2279664786.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch自动求梯度</title>
    <link href="https://du2279664786.github.io/2022/04/19/2022-04-19Pytorch%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6/"/>
    <id>https://du2279664786.github.io/2022/04/19/2022-04-19Pytorch%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6/</id>
    <published>2022-04-19T14:55:10.000Z</published>
    <updated>2022-10-18T09:06:00.494Z</updated>
    
    <content type="html"><![CDATA[<p>创建Tensor的几种方式</p><span id="more"></span><h1 id="微分"><a href="#微分" class="headerlink" title="微分"></a>微分</h1><p>通常我们见到的微分方法有两种：<br>·符号微分法：<br><img src="https://img-blog.csdnimg.cn/bbaa997b4ed34967a69068ebfa46d97d.png" alt="在这里插入图片描述"><br>·数值微分法：<br><img src="https://img-blog.csdnimg.cn/ac854fda494b48c1825b1c860dc258f8.png" alt="∂f(x)/∂x=lim┬ℎ→0f(x+ℎ)−f(x)/ℎ"></p><h1 id="Pytorch自动微分"><a href="#Pytorch自动微分" class="headerlink" title="Pytorch自动微分"></a>Pytorch自动微分</h1><p>对于一个Tensor，如果它的属性requires_grad 设置为 True，它将开始追<br>踪(track)在其上的所有操作<br>我们定义一个初始的tensor并且requires_grad 设置为 True：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>此时，我们在x的基础上进行运算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"><span class="comment"># grad_fn属性代表y是否由运算得来</span></span><br></pre></td></tr></table></figure><p>此时我们就可以进一步运算：out &#x3D;（x+2）**2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z, out)</span><br></pre></td></tr></table></figure><p>反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward() <span class="comment"># 等价于 out.backward(torch.tensor(1.))</span></span><br></pre></td></tr></table></figure><p>此时我们就可以输出x的梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p>注意：grad在反向传播过程中是累加的(accumulated)，这意味着每⼀次运⾏反向传播，梯度都会累加之前的梯度，所以⼀般在反向传播之前需把梯度清零</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure><p>为什么会输出这个值呢？接下来看一下过程：<br>我们可以写出out的等式：<br><img src="https://img-blog.csdnimg.cn/8aee7ec7069c4ea5b5e54a8fcaf7d344.png" alt="在这里插入图片描述"><br>此时我们求o关于x的偏导：<br><img src="https://img-blog.csdnimg.cn/d01a2a217b0146faa398d2a078b6f8df.png" alt="在这里插入图片描述"><br>那么我们在在进行求梯度时为什么要求out的梯度呢？为什么最后要z.mean()呢？<br>很显然我们直接y.backward()会报错<img src="https://img-blog.csdnimg.cn/812ca92f731f4290909e9d92c40502a0.png" alt="在这里插入图片描述"></p><p>这是因为：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传⼊任何参数；否则，需要传⼊⼀个与 y 同形的 Tensor 。<br>在pytorch中：不允许张量对张量求导，只允许标量对张量求导，求导结果是和⾃变量同形的张量。所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量<br>接下来看一个实际的栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>], requires_grad=<span class="literal">True</span>) </span><br><span class="line">y = <span class="number">2</span> * x </span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>, <span class="number">8.</span>], grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure><p>此时我们直接y.backward()会报错，因为y不是标量，所以我们按照要求应该传入一个同形的张量，作为权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">y.backward(t)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p>d(y) &#x3D; 2<br>求导的同时也应该乘以相应的权重t &#x3D; torch.tensor([1,2,3,4])，所以最后输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>, <span class="number">8.</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建Tensor的几种方式&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="梯度" scheme="https://du2279664786.github.io/tags/%E6%A2%AF%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中Tensor的创建</title>
    <link href="https://du2279664786.github.io/2022/04/17/2022-04-17Pytorch%E4%B8%ADTensor%E7%9A%84%E5%88%9B%E5%BB%BA/"/>
    <id>https://du2279664786.github.io/2022/04/17/2022-04-17Pytorch%E4%B8%ADTensor%E7%9A%84%E5%88%9B%E5%BB%BA/</id>
    <published>2022-04-17T14:55:10.000Z</published>
    <updated>2022-10-18T09:04:24.850Z</updated>
    
    <content type="html"><![CDATA[<p>创建Tensor的几种方式</p><span id="more"></span><h1 id="创建Tensor"><a href="#创建Tensor" class="headerlink" title="创建Tensor"></a>创建Tensor</h1><h3 id="创建一个5行3列未初始化的tensor"><a href="#创建一个5行3列未初始化的tensor" class="headerlink" title="创建一个5行3列未初始化的tensor"></a>创建一个5行3列未初始化的tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.0194e-38</span>, <span class="number">1.0469e-38</span>, <span class="number">1.0010e-38</span>],</span><br><span class="line">        [<span class="number">8.9081e-39</span>, <span class="number">8.9082e-39</span>, <span class="number">5.9694e-39</span>],</span><br><span class="line">        [<span class="number">8.9082e-39</span>, <span class="number">1.0194e-38</span>, <span class="number">9.1837e-39</span>],</span><br><span class="line">        [<span class="number">4.6837e-39</span>, <span class="number">9.2755e-39</span>, <span class="number">1.0837e-38</span>],</span><br><span class="line">        [<span class="number">8.4490e-39</span>, <span class="number">1.1112e-38</span>, <span class="number">1.0194e-38</span>]])</span><br></pre></td></tr></table></figure><h3 id="创建一个5行3列随机初始化的tensor："><a href="#创建一个5行3列随机初始化的tensor：" class="headerlink" title="创建一个5行3列随机初始化的tensor："></a>创建一个5行3列随机初始化的tensor：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.5911</span>, <span class="number">0.9191</span>, <span class="number">0.9826</span>],</span><br><span class="line">        [<span class="number">0.4801</span>, <span class="number">0.1648</span>, <span class="number">0.8578</span>],</span><br><span class="line">        [<span class="number">0.9937</span>, <span class="number">0.8051</span>, <span class="number">0.6952</span>],</span><br><span class="line">        [<span class="number">0.9682</span>, <span class="number">0.1975</span>, <span class="number">0.1151</span>],</span><br><span class="line">        [<span class="number">0.2434</span>, <span class="number">0.2917</span>, <span class="number">0.7866</span>]])</span><br></pre></td></tr></table></figure><h3 id="创建Tensor还可以指定数据类型：创建一个5行3列的类型为long的全0数据"><a href="#创建Tensor还可以指定数据类型：创建一个5行3列的类型为long的全0数据" class="headerlink" title="创建Tensor还可以指定数据类型：创建一个5行3列的类型为long的全0数据"></a>创建Tensor还可以指定数据类型：创建一个5行3列的类型为long的全0数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><h3 id="直接输入数据进行创建tensor："><a href="#直接输入数据进行创建tensor：" class="headerlink" title="直接输入数据进行创建tensor："></a>直接输入数据进行创建tensor：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><h3 id="根据现有的tensor创建新的tensor："><a href="#根据现有的tensor创建新的tensor：" class="headerlink" title="根据现有的tensor创建新的tensor："></a>根据现有的tensor创建新的tensor：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn_like(x,dtype = torch.float64)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-<span class="number">0.6669</span>,  <span class="number">0.5308</span>,  <span class="number">1.5981</span>],</span><br><span class="line">        [ <span class="number">1.2061</span>,  <span class="number">0.6624</span>, -<span class="number">0.4535</span>],</span><br><span class="line">        [-<span class="number">0.5667</span>, -<span class="number">0.8755</span>, -<span class="number">2.1078</span>],</span><br><span class="line">        [-<span class="number">3.0560</span>, -<span class="number">0.6035</span>,  <span class="number">0.7990</span>],</span><br><span class="line">        [-<span class="number">0.3979</span>, -<span class="number">1.3582</span>, -<span class="number">0.8427</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><h3 id="2根据numpy创建新的tensor："><a href="#2根据numpy创建新的tensor：" class="headerlink" title="2根据numpy创建新的tensor："></a>2根据numpy创建新的tensor：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor转化为array</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line">b</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将array转化为tensor</span></span><br><span class="line">x = torch.from_numpy(b)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><p>不过要注意的是：无论是array转化为tensor，还是tensor转化为array，他们都是和原来的数据共享内存的，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b+=<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]]</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line"><span class="comment"># b的数值发生变化，x的数值也发生变化，需要注意</span></span><br></pre></td></tr></table></figure><p>更多详情请查看<a href="https://pytorch.org/docs/stable/tensors.html">官方文档</a><br>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建Tensor的几种方式&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>使用anacond prompt安装pytorch(CPU版)</title>
    <link href="https://du2279664786.github.io/2022/04/16/2022-04-16%E4%BD%BF%E7%94%A8anacond%20prompt%E5%AE%89%E8%A3%85pytorch(CPU%E7%89%88)/"/>
    <id>https://du2279664786.github.io/2022/04/16/2022-04-16%E4%BD%BF%E7%94%A8anacond%20prompt%E5%AE%89%E8%A3%85pytorch(CPU%E7%89%88)/</id>
    <published>2022-04-16T14:55:10.000Z</published>
    <updated>2022-10-18T09:02:26.645Z</updated>
    
    <content type="html"><![CDATA[<p>利用anaconda搭建一个新环境并安装CPU版本的pytorch</p><span id="more"></span><h1 id="使用anacond-prompt安装pytorch"><a href="#使用anacond-prompt安装pytorch" class="headerlink" title="使用anacond prompt安装pytorch"></a>使用anacond prompt安装pytorch</h1><p>GPU版本的教程<a href="https://blog.csdn.net/weixin_51756104/article/details/124398722?spm=1001.2014.3001.5501">请点击此处查看</a><br>·首先去anaconda<a href="https://www.anaconda.com/">官网</a>安装anaconda，然后打开anaconda prompt（pycharm配置环境略过）：<br>在命令行输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conda create –n pytorch python=<span class="number">3.8</span></span><br></pre></td></tr></table></figure><p>来创建一个关于pytorch的单独环境<br>这里的‘pytorch’是环境名称，python&#x3D;3.8是版本，都可以根据需求自行修改<br>然后就可以进入我们创建好的虚拟环境：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch</span><br></pre></td></tr></table></figure><p>然后我们可以切换一下源：切换源能够更快的下载 包<br>配置清华源，在命令行输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>然后一切准备工作做好之后，接下来开始安装pytorch，在命令行输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cpuonly -c pytorch   </span><br></pre></td></tr></table></figure><p>输入之后等待一会再输入‘y’（确认下载）<br>最后在命令行输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="built_in">list</span></span><br></pre></td></tr></table></figure><p>然后会列出这个环境所有的包，可以查看有没有‘torch’<br><img src="https://img-blog.csdnimg.cn/dc71bab57e4141bab8c166d8686d310d.png" alt="休闲就"><br>出现torch就代表成功了！<br>欢迎关注作者，有什么问题可以一起讨论！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;利用anaconda搭建一个新环境并安装CPU版本的pytorch&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
  </entry>
  
</feed>
