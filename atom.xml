<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>江东的笔记</title>
  
  <subtitle>Be overcome difficulties is victory</subtitle>
  <link href="https://du2279664786.github.io/atom.xml" rel="self"/>
  
  <link href="https://du2279664786.github.io/"/>
  <updated>2022-11-24T06:54:45.775Z</updated>
  <id>https://du2279664786.github.io/</id>
  
  <author>
    <name>江东</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="https://du2279664786.github.io/2022/11/21/hello-world/"/>
    <id>https://du2279664786.github.io/2022/11/21/hello-world/</id>
    <published>2022-11-21T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.775Z</updated>
    
    <content type="html"><![CDATA[<div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["大家好，这是我的第一篇文章，欢迎查看"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><span id="more"></span><div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["青青陵上柏，磊磊涧中石。", "人生天地间，忽如远行客。","斗酒相娱乐，聊厚不为薄。", "驱车策驽马，游戏宛与洛。","洛中何郁郁，冠带自相索。","长衢罗夹巷，王侯多第宅。","两宫遥相望，双阙百余尺。","极宴娱心意，戚戚何所迫？"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><h1 id="欢迎查看我的第一篇文章"><a href="#欢迎查看我的第一篇文章" class="headerlink" title="欢迎查看我的第一篇文章"></a>欢迎查看我的第一篇文章</h1><p>写在前面：这是我的Github，欢迎star：<a href="https://github.com/du2279664786">https://github.com/du2279664786</a></p><p>大一的时候搭建过一次博客，但是由于长时间不使用，导致配置文件丢失<br>所以在大三上学期我又重新搭建了一下，将去年（大二）所学的知识进行了“简单”的汇总（PS：李明虎老师讲的实在是太丰富了，我只整理了简单的）</p><p>本博客为学习博客，旨在记录自己的学习经历和知识回顾</p><p>转眼间已经大三了，回顾过去两年：<br>        ·大一上：人工智能导论<br>        ·大一下：爬虫<br>        ·大二上：机器学习<br>        ·大二下：深度学习和神经网络<br>….. …..</p><p>希望考研可以成功上岸！</p>]]></content>
    
    
    <summary type="html">&lt;div id=&quot;binft&quot;&gt;&lt;/div&gt;
  &lt;script&gt;
    var binft = function (r) {
      function t() {
        return b[Math.floor(Math.random() * b.length)]
      }  
      function e() {
        return String.fromCharCode(94 * Math.random() + 33)
      }
      function n(r) {
        for (var n = document.createDocumentFragment(), i = 0; r &gt; i; i++) {
          var l = document.createElement(&quot;span&quot;);
          l.textContent = e(), l.style.color = t(), n.appendChild(l)
        }
        return n
      }
      function i() {
        var t = o[c.skillI];
        c.step ? c.step-- : (c.step = g, c.prefixP &lt; l.length ? (c.prefixP &gt;= 0 &amp;&amp; (c.text += l[c.prefixP]), c.prefixP++) : &quot;forward&quot; === c.direction ? c.skillP &lt; t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = &quot;backward&quot;, c.delay = a) : c.skillP &gt; 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = &quot;forward&quot;)), r.textContent = c.text, r.appendChild(n(c.prefixP &lt; l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)
      }
      var l = &quot;&quot;,
      o = [&quot;大家好，这是我的第一篇文章，欢迎查看&quot;].map(function (r) {
      return r + &quot;&quot;
      }),
      a = 2,
      g = 1,
      s = 5,
      d = 75,
      b = [&quot;rgb(110,64,170)&quot;, &quot;rgb(150,61,179)&quot;, &quot;rgb(191,60,175)&quot;, &quot;rgb(228,65,157)&quot;, &quot;rgb(254,75,131)&quot;, &quot;rgb(255,94,99)&quot;, &quot;rgb(255,120,71)&quot;, &quot;rgb(251,150,51)&quot;, &quot;rgb(226,183,47)&quot;, &quot;rgb(198,214,60)&quot;, &quot;rgb(175,240,91)&quot;, &quot;rgb(127,246,88)&quot;, &quot;rgb(82,246,103)&quot;, &quot;rgb(48,239,130)&quot;, &quot;rgb(29,223,163)&quot;, &quot;rgb(26,199,194)&quot;, &quot;rgb(35,171,216)&quot;, &quot;rgb(54,140,225)&quot;, &quot;rgb(76,110,219)&quot;, &quot;rgb(96,84,200)&quot;],
      c = {
        text: &quot;&quot;,
        prefixP: -s,
        skillI: 0,
        skillP: 0,
        direction: &quot;forward&quot;,
        delay: a,
        step: g
      };
      i()
      };
      binft(document.getElementById(&#39;binft&#39;));
  &lt;/script&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>Pre-training</title>
    <link href="https://du2279664786.github.io/2022/11/21/2022-11-21%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%96%87%E7%9A%84bert/"/>
    <id>https://du2279664786.github.io/2022/11/21/2022-11-21%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%96%87%E7%9A%84bert/</id>
    <published>2022-11-21T12:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.773Z</updated>
    
    <content type="html"><![CDATA[<p>Pre-train BERT (Chinese language model) from scratch</p><span id="more"></span><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers,tokenizers</span><br><span class="line">transformers.__version__,tokenizers.__version__</span><br></pre></td></tr></table></figure><pre><code>(&#39;4.24.0&#39;, &#39;0.13.2&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tokenizers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, LineByLineTextDataset</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train a tokenizer</span></span><br><span class="line"></span><br><span class="line">bwpt = tokenizers.BertWordPieceTokenizer(vocab_file=<span class="literal">None</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># filepath = &quot;../input/bert-bangla/raw_bangla_for_BERT.txt&quot;</span></span><br><span class="line">filepath = <span class="string">&quot;./train.txt&quot;</span></span><br><span class="line"></span><br><span class="line">bwpt.train(</span><br><span class="line">    files=[filepath],</span><br><span class="line">    vocab_size=<span class="number">50000</span>,</span><br><span class="line">    min_frequency=<span class="number">3</span>,</span><br><span class="line">    limit_alphabet=<span class="number">1000</span></span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bwpt.save(<span class="string">&#x27;./训练中文的bert输出/&#x27;</span>, <span class="string">&#x27;name&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#39;./训练中文的bert输出/name-vocab.txt&#39;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load the tokenizer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># vocab_file_dir = &#x27;/kaggle/input/bert-bangla/bangla-vocab.txt&#x27; </span></span><br><span class="line">vocab_file_dir = <span class="string">&#x27;./vocab.txt&#x27;</span> </span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(vocab_file_dir)</span><br><span class="line"></span><br><span class="line">sentence = <span class="string">&#x27;今天晚上我要吃啵啵鱼&#x27;</span></span><br><span class="line"></span><br><span class="line">encoded_input = tokenizer.tokenize(sentence)</span><br><span class="line"><span class="built_in">print</span>(encoded_input)</span><br><span class="line"><span class="comment"># print(encoded_input[&#x27;input_ids&#x27;])</span></span><br></pre></td></tr></table></figure><pre><code>[&#39;今&#39;, &#39;天&#39;, &#39;晚&#39;, &#39;上&#39;, &#39;我&#39;, &#39;要&#39;, &#39;吃&#39;, &#39;啵&#39;, &#39;啵&#39;, &#39;鱼&#39;]C:\Users\dupeibo\Anaconda3\envs\pt\lib\site-packages\transformers\tokenization_utils_base.py:1679: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won&#39;t be possible anymore in v5. Use a model identifier or the path to a directory instead.  warnings.warn(</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">transformers has a predefined class LineByLineTextDataset()</span></span><br><span class="line"><span class="string">which reads your text line by line and converts them to tokens</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">dataset= LineByLineTextDataset(</span><br><span class="line">    tokenizer = tokenizer,</span><br><span class="line"><span class="comment">#     file_path = &#x27;/kaggle/input/bert-bangla/raw_bangla_for_BERT.txt&#x27;,</span></span><br><span class="line">    file_path = <span class="string">&#x27;./train.txt&#x27;</span>,</span><br><span class="line">    block_size = <span class="number">128</span>  <span class="comment"># maximum sequence length</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;No. of lines: &#x27;</span>, <span class="built_in">len</span>(dataset)) <span class="comment"># No of lines in your datset</span></span><br><span class="line">dataset</span><br></pre></td></tr></table></figure><pre><code>No. of lines:  24494Wall time: 10.4 s&lt;transformers.data.datasets.language_modeling.LineByLineTextDataset at 0x2084eb42ac8&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">config = BertConfig(</span><br><span class="line">    vocab_size=<span class="number">50000</span>,</span><br><span class="line">    hidden_size=<span class="number">768</span>, </span><br><span class="line">    num_hidden_layers=<span class="number">6</span>, </span><br><span class="line">    num_attention_heads=<span class="number">12</span>,</span><br><span class="line">    max_position_embeddings=<span class="number">512</span></span><br><span class="line">)</span><br><span class="line"> </span><br><span class="line">model = BertForMaskedLM(config)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;No of parameters: &#x27;</span>, model.num_parameters())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 做掩码机制</span></span><br><span class="line">data_collator = DataCollatorForLanguageModeling(</span><br><span class="line">    tokenizer=tokenizer, mlm=<span class="literal">True</span>, mlm_probability=<span class="number">0.15</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><pre><code>No of parameters:  82556240</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">&#x27;./训练中文的bert输出/&#x27;</span>,</span><br><span class="line">    overwrite_output_dir=<span class="literal">True</span>,</span><br><span class="line">    num_train_epochs=<span class="number">1</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    save_steps=<span class="number">10_000</span>,</span><br><span class="line">    save_total_limit=<span class="number">2</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    data_collator=data_collator,</span><br><span class="line">    train_dataset=dataset,</span><br><span class="line">    prediction_loss_only=<span class="literal">True</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">trainer.train()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>Epoch:   0%|          | 0/1 [00:00&lt;?, ?it/s]Iteration:   0%|          | 0/6124 [00:00&lt;?, ?it/s]&#123;&quot;loss&quot;: 7.0956006441116335, &quot;learning_rate&quot;: 4.591770084911823e-05, &quot;epoch&quot;: 0.08164598301763554, &quot;step&quot;: 500&#125;&#123;&quot;loss&quot;: 6.214028715133667, &quot;learning_rate&quot;: 4.183540169823645e-05, &quot;epoch&quot;: 0.16329196603527107, &quot;step&quot;: 1000&#125;&#123;&quot;loss&quot;: 5.860672056674957, &quot;learning_rate&quot;: 3.775310254735467e-05, &quot;epoch&quot;: 0.24493794905290658, &quot;step&quot;: 1500&#125;&#123;&quot;loss&quot;: 5.599938702583313, &quot;learning_rate&quot;: 3.367080339647289e-05, &quot;epoch&quot;: 0.32658393207054215, &quot;step&quot;: 2000&#125;&#123;&quot;loss&quot;: 5.412256263256073, &quot;learning_rate&quot;: 2.958850424559112e-05, &quot;epoch&quot;: 0.4082299150881777, &quot;step&quot;: 2500&#125;&#123;&quot;loss&quot;: 5.261007954120636, &quot;learning_rate&quot;: 2.550620509470934e-05, &quot;epoch&quot;: 0.48987589810581317, &quot;step&quot;: 3000&#125;&#123;&quot;loss&quot;: 5.095327672958374, &quot;learning_rate&quot;: 2.1423905943827566e-05, &quot;epoch&quot;: 0.5715218811234487, &quot;step&quot;: 3500&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 1.734160679294579e-05, &quot;epoch&quot;: 0.6531678641410843, &quot;step&quot;: 4000&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 1.3259307642064011e-05, &quot;epoch&quot;: 0.7348138471587198, &quot;step&quot;: 4500&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 9.177008491182235e-06, &quot;epoch&quot;: 0.8164598301763554, &quot;step&quot;: 5000&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 5.094709340300458e-06, &quot;epoch&quot;: 0.8981058131939909, &quot;step&quot;: 5500&#125;&#123;&quot;loss&quot;: NaN, &quot;learning_rate&quot;: 1.0124101894186806e-06, &quot;epoch&quot;: 0.9797517962116263, &quot;step&quot;: 6000&#125;Wall time: 17min 31sTrainOutput(global_step=6124, training_loss=nan)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># trainer.save_model(&#x27;./训练中文的bert输出&#x27;)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">&#x27;./&#x27;</span>)</span><br><span class="line"></span><br><span class="line">fill_mask = pipeline(</span><br><span class="line">    <span class="string">&quot;fill-mask&quot;</span>,</span><br><span class="line">    model=model,</span><br><span class="line">    tokenizer=tokenizer</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fill_mask(<span class="string">&#x27;心[MASK]病&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#123;&#39;score&#39;: 0.3431047201156616,  &#39;token&#39;: 5552,  &#39;token_str&#39;: &#39;脏&#39;,  &#39;sequence&#39;: &#39;心 脏 病&#39;&#125;, &#123;&#39;score&#39;: 0.07011183351278305,  &#39;token&#39;: 2552,  &#39;token_str&#39;: &#39;心&#39;,  &#39;sequence&#39;: &#39;心 心 病&#39;&#125;, &#123;&#39;score&#39;: 0.05838495120406151,  &#39;token&#39;: 4567,  &#39;token_str&#39;: &#39;病&#39;,  &#39;sequence&#39;: &#39;心 病 病&#39;&#125;, &#123;&#39;score&#39;: 0.014283978380262852,  &#39;token&#39;: 107,  &#39;token_str&#39;: &#39;&quot;&#39;,  &#39;sequence&#39;: &#39;心 &quot; 病&#39;&#125;, &#123;&#39;score&#39;: 0.011550793424248695,  &#39;token&#39;: 7315,  &#39;token_str&#39;: &#39;闷&#39;,  &#39;sequence&#39;: &#39;心 闷 病&#39;&#125;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fill_mask(<span class="string">&#x27;怀[MASK]&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>[&#123;&#39;score&#39;: 0.4942161738872528,  &#39;token&#39;: 2097,  &#39;token_str&#39;: &#39;孕&#39;,  &#39;sequence&#39;: &#39;怀 孕&#39;&#125;, &#123;&#39;score&#39;: 0.050573259592056274,  &#39;token&#39;: 2577,  &#39;token_str&#39;: &#39;怀&#39;,  &#39;sequence&#39;: &#39;怀 怀&#39;&#125;, &#123;&#39;score&#39;: 0.01493070088326931,  &#39;token&#39;: 107,  &#39;token_str&#39;: &#39;&quot;&#39;,  &#39;sequence&#39;: &#39;怀 &quot;&#39;&#125;, &#123;&#39;score&#39;: 0.010810167528688908,  &#39;token&#39;: 5307,  &#39;token_str&#39;: &#39;经&#39;,  &#39;sequence&#39;: &#39;怀 经&#39;&#125;, &#123;&#39;score&#39;: 0.00741073302924633,  &#39;token&#39;: 1453,  &#39;token_str&#39;: &#39;周&#39;,  &#39;sequence&#39;: &#39;怀 周&#39;&#125;]</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;Pre-train BERT (Chinese language model) from scratch&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="BERT" scheme="https://du2279664786.github.io/tags/BERT/"/>
    
    <category term="Pre-training" scheme="https://du2279664786.github.io/tags/Pre-training/"/>
    
  </entry>
  
  <entry>
    <title>基于BERT的文本分类</title>
    <link href="https://du2279664786.github.io/2022/11/21/2022-11-21%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <id>https://du2279664786.github.io/2022/11/21/2022-11-21%E5%9F%BA%E4%BA%8EBERT%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</id>
    <published>2022-11-21T10:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.776Z</updated>
    
    <content type="html"><![CDATA[<p>通过智能化手段识别其中是否存在“虚报、假报”的情况</p><span id="more"></span><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>企业自主填报安全生产隐患，对于将风险消除在事故萌芽阶段具有重要意义。企业在填报隐患时，往往存在不认真填报的情况，“虚报、假报”隐患内容，增大了企业监管的难度。采用大数据手段分析隐患内容，找出不切实履行主体责任的企业，向监管部门进行推送，实现精准执法，能够提高监管手段的有效性，增强企业安全责任意识。</p><h2 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h2><p>本赛题提供企业填报隐患数据，参赛选手需通过智能化手段识别其中是否存在“虚报、假报”的情况。</p><h1 id="数据简介"><a href="#数据简介" class="headerlink" title="数据简介"></a>数据简介</h1><p>本赛题数据集为脱敏后的企业填报自查隐患记录,数据说明如下：</p><ul><li><p>训练集数据包含“【id、level_1（一级标准）、level_2（二级标准）、level_3（三级标准）、level_4（四级标准）、content（隐患内容）和label（标签）】”共7个字段。<br>其中“id”为主键，无业务意义；“一级标准、二级标准、三级标准、四级标准”为《深圳市安全隐患自查和巡查基本指引（2016年修订版）》规定的排查指引，一级标准对应不同隐患类型，二至四级标准是对一级标准的细化，企业自主上报隐患时，根据不同类型隐患的四级标准开展隐患自查工作；“隐患内容”为企业上报的具体隐患；“标签”标识的是该条隐患的合格性，“1”表示隐患填报不合格，“0”表示隐患填报合格。</p></li><li><p>预测结果文件results.csv</p></li></ul><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669078534834.png" alt="1669078534834"></p><h1 id="评测标准"><a href="#评测标准" class="headerlink" title="评测标准"></a>评测标准</h1><p>本赛题采用F1 -score作为模型评判标准。</p><h1 id="具体实现代码如下："><a href="#具体实现代码如下：" class="headerlink" title="具体实现代码如下："></a>具体实现代码如下：</h1><h2 id="导入所以需要的包"><a href="#导入所以需要的包" class="headerlink" title="导入所以需要的包"></a>导入所以需要的包</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入transformers</span></span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="comment"># from transformers import BertModel, BertTokenizer,BertConfig, AdamW, get_linear_schedule_with_warmup</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModel, AutoTokenizer,AutoConfig, AdamW, get_linear_schedule_with_warmup</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常用包</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> rcParams</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> rc</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix, classification_report</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> textwrap <span class="keyword">import</span> wrap</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format=<span class="string">&#x27;retina&#x27;</span> <span class="comment"># 主题</span></span><br></pre></td></tr></table></figure><h2 id="初始化设置"><a href="#初始化设置" class="headerlink" title="初始化设置"></a>初始化设置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;whitegrid&#x27;</span>, palette=<span class="string">&#x27;muted&#x27;</span>, font_scale=<span class="number">1.2</span>)</span><br><span class="line">HAPPY_COLORS_PALETTE = [<span class="string">&quot;#01BEFE&quot;</span>, <span class="string">&quot;#FFDD00&quot;</span>, <span class="string">&quot;#FF7D00&quot;</span>, <span class="string">&quot;#FF006D&quot;</span>, <span class="string">&quot;#ADFF02&quot;</span>, <span class="string">&quot;#8F00FF&quot;</span>]</span><br><span class="line">sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))</span><br><span class="line">rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = <span class="number">12</span>, <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 固定随机种子</span></span><br><span class="line">RANDOM_SEED = <span class="number">42</span></span><br><span class="line">np.random.seed(RANDOM_SEED)</span><br><span class="line">torch.manual_seed(RANDOM_SEED)</span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">device</span><br></pre></td></tr></table></figure><pre><code>device(type=&#39;cuda&#39;, index=0)</code></pre><h1 id="读取数据"><a href="#读取数据" class="headerlink" title="读取数据"></a>读取数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sub=pd.read_csv(<span class="string">&#x27;./data/02企业隐患排查/sub.csv&#x27;</span>)</span><br><span class="line">test=pd.read_csv(<span class="string">&#x27;./data/02企业隐患排查/test.csv&#x27;</span>)</span><br><span class="line">train=pd.read_csv(<span class="string">&#x27;./data/02企业隐患排查/train.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（二）电气安全</td>      <td>6、移动用电产品、电动工具及照明</td>      <td>1、移动使用的用电产品和I类电动工具的绝缘线，必须采用三芯(单相)或四芯(三相)多股铜芯橡套软线。</td>      <td>使用移动手动电动工具,外接线绝缘皮破损,应停止使用.</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>3、消防设施、器材和消防安全标志是否在位、完整；</td>      <td>一般</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>2、防火检查</td>      <td>6、重点工种人员以及其他员工消防知识的掌握情况；</td>      <td>消防知识要加强</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>3、消防设施、器材和消防安全标志是否在位、完整；</td>      <td>消防通道有货物摆放 清理不及时</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>4、常闭式防火门是否处于关闭状态，防火卷帘下是否堆放物品影响使用；</td>      <td>防火门打开状态</td>      <td>0</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>交通运输类（现场）—2016版</td>      <td>（一）消防安全</td>      <td>2、防火检查</td>      <td>2、安全疏散通道、疏散指示标志、应急照明和安全出口情况。</td>      <td>RB1洗地机占用堵塞安全通道</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>工业/危化品类（选项）—2016版</td>      <td>（二）仓库</td>      <td>1、一般要求</td>      <td>1、库房内储存物品应分类、分堆、限额存放。</td>      <td>未分类堆放</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>3、消防设施、器材和消防安全标志是否在位、完整；</td>      <td>消防设施、器材和消防安全标志是否在位、完整</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>商贸服务教文卫类（现场）—2016版</td>      <td>（二）电气安全</td>      <td>3、电气线路及电源插头插座</td>      <td>3、电源插座、电源插头应按规定正确接线。</td>      <td>插座随意放在电器旁边</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>商贸服务教文卫类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>6、其他消防安全情况。</td>      <td>检查中发现一瓶灭火器过期</td>    </tr>  </tbody></table></div><h2 id="查看数据的形状"><a href="#查看数据的形状" class="headerlink" title="查看数据的形状"></a>查看数据的形状</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;train.shape,test.shape,sub.shape&quot;</span>,train.shape,test.shape,sub.shape)</span><br></pre></td></tr></table></figure><pre><code>train.shape,test.shape,sub.shape (12000, 7) (18000, 6) (18000, 2)</code></pre><h2 id="查看是否存在空值"><a href="#查看是否存在空值" class="headerlink" title="查看是否存在空值"></a>查看是否存在空值</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[train[<span class="string">&#x27;content&#x27;</span>].isna()]</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>6193</th>      <td>6193</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>3、消防设施、器材和消防安全标志是否在位、完整；</td>      <td>NaN</td>      <td>1</td>    </tr>    <tr>      <th>9248</th>      <td>9248</td>      <td>工业/危化品类（现场）—2016版</td>      <td>（一）消防检查</td>      <td>1、防火巡查</td>      <td>4、常闭式防火门是否处于关闭状态，防火卷帘下是否堆放物品影响使用；</td>      <td>NaN</td>      <td>1</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;train null nums&#x27;</span>)</span><br><span class="line">train.shape[<span class="number">0</span>]-train.count()</span><br></pre></td></tr></table></figure><pre><code>train null numsid         0level_1    0level_2    0level_3    0level_4    0content    2label      0dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test null nums&#x27;</span>)</span><br><span class="line">test.shape[<span class="number">0</span>]-test.count()</span><br></pre></td></tr></table></figure><pre><code>test null numsid         0level_1    0level_2    0level_3    0level_4    0content    4dtype: int64</code></pre><h2 id="查看标签的分布"><a href="#查看标签的分布" class="headerlink" title="查看标签的分布"></a>查看标签的分布</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;label&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>0    107121     1288Name: label, dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sns.countplot(train.label)</span></span><br><span class="line"><span class="comment"># plt.xlabel(&#x27;label count&#x27;)</span></span><br></pre></td></tr></table></figure><h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train.fillna(<span class="string">&quot;空值&quot;</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">test.fillna(<span class="string">&quot;空值&quot;</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.shape[<span class="number">0</span>]-train.count()</span><br></pre></td></tr></table></figure><pre><code>id         0level_1    0level_2    0level_3    0level_4    0content    0label      0dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;level_3&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure><pre><code>1、防火巡查             42252、防火检查             29112、配电箱（柜、板）          7101、作业通道              6643、电气线路及电源插头插座       497                   ... 3、安全带                 14、特种设备及操作人员管理记录       14、安全技术交底              13、停车场                 11、水库安全                1Name: level_3, Length: 153, dtype: int64</code></pre><h3 id="对训练集处理"><a href="#对训练集处理" class="headerlink" title="对训练集处理"></a>对训练集处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;level_1&#x27;</span>] = train[<span class="string">&#x27;level_1&#x27;</span>].apply(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;（&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">train[<span class="string">&#x27;level_2&#x27;</span>] = train[<span class="string">&#x27;level_2&#x27;</span>].apply(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;）&#x27;</span>)[<span class="number">1</span>])</span><br><span class="line">train[<span class="string">&#x27;level_3&#x27;</span>] = train[<span class="string">&#x27;level_3&#x27;</span>].apply(<span class="keyword">lambda</span> x:re.split(<span class="string">&#x27;[0-9]、&#x27;</span>,x)[-<span class="number">1</span>])</span><br><span class="line">train[<span class="string">&#x27;level_4&#x27;</span>] = train[<span class="string">&#x27;level_4&#x27;</span>].apply(<span class="keyword">lambda</span> x:re.split(<span class="string">&#x27;[0-9]、&#x27;</span>,x)[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="对测试集处理"><a href="#对测试集处理" class="headerlink" title="对测试集处理"></a>对测试集处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test[<span class="string">&#x27;level_1&#x27;</span>] = test[<span class="string">&#x27;level_1&#x27;</span>].apply(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;（&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">test[<span class="string">&#x27;level_2&#x27;</span>] = test[<span class="string">&#x27;level_2&#x27;</span>].apply(<span class="keyword">lambda</span> x:x.split(<span class="string">&#x27;）&#x27;</span>)[-<span class="number">1</span>])</span><br><span class="line">test[<span class="string">&#x27;level_3&#x27;</span>] = test[<span class="string">&#x27;level_3&#x27;</span>].apply(<span class="keyword">lambda</span> x:re.split(<span class="string">&#x27;[0-9]、&#x27;</span>,x)[-<span class="number">1</span>])</span><br><span class="line">test[<span class="string">&#x27;level_4&#x27;</span>] = test[<span class="string">&#x27;level_4&#x27;</span>].apply(<span class="keyword">lambda</span> x:re.split(<span class="string">&#x27;[0-9]、&#x27;</span>,x)[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>工业/危化品类</td>      <td>电气安全</td>      <td>移动用电产品、电动工具及照明</td>      <td>移动使用的用电产品和I类电动工具的绝缘线，必须采用三芯(单相)或四芯(三相)多股铜芯橡套软线。</td>      <td>使用移动手动电动工具,外接线绝缘皮破损,应停止使用.</td>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>消防设施、器材和消防安全标志是否在位、完整；</td>      <td>一般</td>      <td>1</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火检查</td>      <td>重点工种人员以及其他员工消防知识的掌握情况；</td>      <td>消防知识要加强</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>消防设施、器材和消防安全标志是否在位、完整；</td>      <td>消防通道有货物摆放 清理不及时</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>常闭式防火门是否处于关闭状态，防火卷帘下是否堆放物品影响使用；</td>      <td>防火门打开状态</td>      <td>0</td>    </tr>  </tbody></table></div><h2 id="文本拼接"><a href="#文本拼接" class="headerlink" title="文本拼接"></a>文本拼接</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;text&#x27;</span>]=train[<span class="string">&#x27;content&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+train[<span class="string">&#x27;level_1&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+train[<span class="string">&#x27;level_2&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+train[<span class="string">&#x27;level_3&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+train[<span class="string">&#x27;level_4&#x27;</span>]</span><br><span class="line">test[<span class="string">&#x27;text&#x27;</span>]=test[<span class="string">&#x27;content&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+test[<span class="string">&#x27;level_1&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+test[<span class="string">&#x27;level_2&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+test[<span class="string">&#x27;level_3&#x27;</span>]+<span class="string">&#x27;[SEP]&#x27;</span>+test[<span class="string">&#x27;level_4&#x27;</span>]</span><br><span class="line">train.head()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>level_1</th>      <th>level_2</th>      <th>level_3</th>      <th>level_4</th>      <th>content</th>      <th>label</th>      <th>text</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>工业/危化品类</td>      <td>电气安全</td>      <td>移动用电产品、电动工具及照明</td>      <td>移动使用的用电产品和I类电动工具的绝缘线，必须采用三芯(单相)或四芯(三相)多股铜芯橡套软线。</td>      <td>使用移动手动电动工具,外接线绝缘皮破损,应停止使用.</td>      <td>0</td>      <td>使用移动手动电动工具,外接线绝缘皮破损,应停止使用.[SEP]工业/危化品类[SEP]电气安...</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>消防设施、器材和消防安全标志是否在位、完整；</td>      <td>一般</td>      <td>1</td>      <td>一般[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]消防设施、器材和消...</td>    </tr>    <tr>      <th>2</th>      <td>2</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火检查</td>      <td>重点工种人员以及其他员工消防知识的掌握情况；</td>      <td>消防知识要加强</td>      <td>0</td>      <td>消防知识要加强[SEP]工业/危化品类[SEP]消防检查[SEP]防火检查[SEP]重点工种...</td>    </tr>    <tr>      <th>3</th>      <td>3</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>消防设施、器材和消防安全标志是否在位、完整；</td>      <td>消防通道有货物摆放 清理不及时</td>      <td>0</td>      <td>消防通道有货物摆放 清理不及时[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[...</td>    </tr>    <tr>      <th>4</th>      <td>4</td>      <td>工业/危化品类</td>      <td>消防检查</td>      <td>防火巡查</td>      <td>常闭式防火门是否处于关闭状态，防火卷帘下是否堆放物品影响使用；</td>      <td>防火门打开状态</td>      <td>0</td>      <td>防火门打开状态[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]常闭式防...</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;text_len&#x27;</span>]=train[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="built_in">len</span>)</span><br><span class="line">train[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="built_in">len</span>).describe()<span class="comment"># 298-12=286</span></span><br></pre></td></tr></table></figure><pre><code>count    12000.000000mean        80.444500std         21.910859min         43.00000025%         66.00000050%         75.00000075%         92.000000max        298.000000Name: text, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test[<span class="string">&#x27;text&#x27;</span>].<span class="built_in">map</span>(<span class="built_in">len</span>).describe() <span class="comment"># 520-12=518</span></span><br></pre></td></tr></table></figure><pre><code>count    18000.000000mean        80.762611std         22.719823min         43.00000025%         66.00000050%         76.00000075%         92.000000max        520.000000Name: text, dtype: float64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train[<span class="string">&#x27;text_len&#x27;</span>].plot(kind=<span class="string">&#x27;kde&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;AxesSubplot:ylabel=&#39;Density&#39;&gt;</code></pre><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079055167.png" alt="1669079055167"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sum</span>(train[<span class="string">&#x27;text_len&#x27;</span>]&gt;<span class="number">100</span>) <span class="comment"># text文本长度大于100的个数     1878</span></span><br><span class="line"><span class="built_in">sum</span>(train[<span class="string">&#x27;text_len&#x27;</span>]&gt;<span class="number">200</span>) <span class="comment"># text文本长度大于200的个数     11</span></span><br></pre></td></tr></table></figure><h1 id="模型的加载和配置"><a href="#模型的加载和配置" class="headerlink" title="模型的加载和配置"></a>模型的加载和配置</h1><h3 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PRE_TRAINED_MODEL_NAME = <span class="string">&#x27;bert-base-chinese&#x27;</span></span><br><span class="line"><span class="comment"># PRE_TRAINED_MODEL_NAME = &#x27;hfl/chinese-roberta-wwm-ext&#x27;</span></span><br><span class="line"><span class="comment"># PRE_TRAINED_MODEL_NAME = &#x27;hfl/chinese-roberta-wwm&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br></pre></td></tr></table></figure><pre><code>PreTrainedTokenizerFast(name_or_path=&#39;bert-base-chinese&#39;, vocab_size=21128, model_max_len=512, is_fast=True, padding_side=&#39;right&#39;, truncation_side=&#39;right&#39;, special_tokens=&#123;&#39;unk_token&#39;: &#39;[UNK]&#39;, &#39;sep_token&#39;: &#39;[SEP]&#39;, &#39;pad_token&#39;: &#39;[PAD]&#39;, &#39;cls_token&#39;: &#39;[CLS]&#39;, &#39;mask_token&#39;: &#39;[MASK]&#39;&#125;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sample_txt = <span class="string">&#x27;今天早上9点半起床，我在学习预训练模型的使用.&#x27;</span></span><br><span class="line"><span class="built_in">len</span>(sample_txt)</span><br></pre></td></tr></table></figure><pre><code>23</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tokens = tokenizer.tokenize(sample_txt)</span><br><span class="line">token_ids = tokenizer.convert_tokens_to_ids(tokens)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;文本为: <span class="subst">&#123;sample_txt&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;分词的列表为: <span class="subst">&#123;tokens&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;词对应的唯一id: <span class="subst">&#123;token_ids&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>文本为: 今天早上9点半起床，我在学习预训练模型的使用.分词的列表为: [&#39;今&#39;, &#39;天&#39;, &#39;早&#39;, &#39;上&#39;, &#39;9&#39;, &#39;点&#39;, &#39;半&#39;, &#39;起&#39;, &#39;床&#39;, &#39;，&#39;, &#39;我&#39;, &#39;在&#39;, &#39;学&#39;, &#39;习&#39;, &#39;预&#39;, &#39;训&#39;, &#39;练&#39;, &#39;模&#39;, &#39;型&#39;, &#39;的&#39;, &#39;使&#39;, &#39;用&#39;, &#39;.&#39;]词对应的唯一id: [791, 1921, 3193, 677, 130, 4157, 1288, 6629, 2414, 8024, 2769, 1762, 2110, 739, 7564, 6378, 5298, 3563, 1798, 4638, 886, 4500, 119]</code></pre><h3 id="查看特殊的Token"><a href="#查看特殊的Token" class="headerlink" title="查看特殊的Token"></a>查看特殊的Token</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.sep_token,tokenizer.sep_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[SEP]&#39;, 102)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.cls_token,tokenizer.cls_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[CLS]&#39;, 101)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.pad_token,tokenizer.pad_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[PAD]&#39;, 0)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.mask_token,tokenizer.mask_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[MASK]&#39;, 103)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.unk_token,tokenizer.unk_token_id</span><br></pre></td></tr></table></figure><pre><code>(&#39;[UNK]&#39;, 100)</code></pre><h3 id="简单的编码测试"><a href="#简单的编码测试" class="headerlink" title="简单的编码测试"></a>简单的编码测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">encoding=tokenizer.encode_plus(</span><br><span class="line">    sample_txt,</span><br><span class="line">    <span class="comment"># sample_txt_another,</span></span><br><span class="line">    max_length=<span class="number">32</span>,</span><br><span class="line">    add_special_tokens=<span class="literal">True</span>,<span class="comment"># [CLS]和[SEP]</span></span><br><span class="line">    return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">    pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">    return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">    return_tensors=<span class="string">&#x27;pt&#x27;</span>,<span class="comment"># Pytorch tensor张量</span></span><br><span class="line"></span><br><span class="line">)</span><br><span class="line">encoding</span><br></pre></td></tr></table></figure><pre><code>Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.&#123;&#39;input_ids&#39;: tensor([[ 101,  791, 1921, 3193,  677,  130, 4157, 1288, 6629, 2414, 8024, 2769,         1762, 2110,  739, 7564, 6378, 5298, 3563, 1798, 4638,  886, 4500,  119,          102,    0,    0,    0,    0,    0,    0,    0]]), &#39;token_type_ids&#39;: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,         0, 0, 0, 0, 0, 0, 0, 0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,         1, 0, 0, 0, 0, 0, 0, 0]])&#125;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoding[<span class="string">&#x27;attention_mask&#x27;</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><pre><code>tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        1, 0, 0, 0, 0, 0, 0, 0])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">token_lens = []</span><br><span class="line"><span class="comment"># 选的是每一句话的长度</span></span><br><span class="line"><span class="keyword">for</span> txt <span class="keyword">in</span> train.text:</span><br><span class="line"><span class="comment">#     print(txt)</span></span><br><span class="line">    tokens = tokenizer.encode(txt, max_length=<span class="number">512</span>)</span><br><span class="line">    token_lens.append(<span class="built_in">len</span>(tokens))</span><br><span class="line"><span class="comment"># token_lens</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.distplot(token_lens)</span><br><span class="line">plt.xlim([<span class="number">0</span>, <span class="number">256</span>]);</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Token count&#x27;</span>);</span><br></pre></td></tr></table></figure><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079168255.png" alt="1669079168255"></p><p>​    </p><h3 id="通过分析，长度一般都在160之内"><a href="#通过分析，长度一般都在160之内" class="headerlink" title="通过分析，长度一般都在160之内"></a>通过分析，长度一般都在160之内</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAX_LEN = <span class="number">160</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten()</span><br></pre></td></tr></table></figure><pre><code>tensor([ 101,  791, 1921, 3193,  677,  130, 4157, 1288, 6629, 2414, 8024, 2769,        1762, 2110,  739, 7564, 6378, 5298, 3563, 1798, 4638,  886, 4500,  119,         102,    0,    0,    0,    0,    0,    0,    0])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoding[<span class="string">&#x27;input_ids&#x27;</span>]</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 101,  791, 1921, 3193,  677,  130, 4157, 1288, 6629, 2414, 8024, 2769,         1762, 2110,  739, 7564, 6378, 5298, 3563, 1798, 4638,  886, 4500,  119,          102,    0,    0,    0,    0,    0,    0,    0]])</code></pre><h2 id="处理数据"><a href="#处理数据" class="headerlink" title="处理数据"></a>处理数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnterpriseDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,texts,labels,tokenizer,max_len</span>):</span><br><span class="line">        self.texts=texts</span><br><span class="line">        self.labels=labels</span><br><span class="line">        self.tokenizer=tokenizer</span><br><span class="line">        self.max_len=max_len</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.texts)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,item</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        item 为数据索引，迭代取第item条数据</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        text=<span class="built_in">str</span>(self.texts[item])</span><br><span class="line">        label=self.labels[item]</span><br><span class="line">        </span><br><span class="line">        encoding=self.tokenizer.encode_plus(</span><br><span class="line">            text,</span><br><span class="line">            add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">            max_length=self.max_len,</span><br><span class="line">            return_token_type_ids=<span class="literal">True</span>,</span><br><span class="line">            pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">            return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">            return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print(encoding[&#x27;input_ids&#x27;])</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">&#x27;texts&#x27;</span>:text,</span><br><span class="line">            <span class="string">&#x27;input_ids&#x27;</span>:encoding[<span class="string">&#x27;input_ids&#x27;</span>].flatten(),</span><br><span class="line">            <span class="string">&#x27;attention_mask&#x27;</span>:encoding[<span class="string">&#x27;attention_mask&#x27;</span>].flatten(),</span><br><span class="line">            <span class="comment"># toeken_type_ids:0</span></span><br><span class="line">            <span class="string">&#x27;labels&#x27;</span>:torch.tensor(label,dtype=torch.long)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h3 id="分割数据集"><a href="#分割数据集" class="headerlink" title="分割数据集"></a>分割数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_train, df_test = train_test_split(train, test_size=<span class="number">0.1</span>, random_state=RANDOM_SEED)</span><br><span class="line">df_val, df_test = train_test_split(df_test, test_size=<span class="number">0.5</span>, random_state=RANDOM_SEED)</span><br><span class="line">df_train.shape, df_val.shape, df_test.shape</span><br></pre></td></tr></table></figure><pre><code>((10800, 9), (600, 9), (600, 9))</code></pre><h3 id="创建DataLoader"><a href="#创建DataLoader" class="headerlink" title="创建DataLoader"></a>创建DataLoader</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_data_loader</span>(<span class="params">df,tokenizer,max_len,batch_size</span>):</span><br><span class="line">    ds=EnterpriseDataset(</span><br><span class="line">        texts=df[<span class="string">&#x27;text&#x27;</span>].values,</span><br><span class="line">        labels=df[<span class="string">&#x27;label&#x27;</span>].values,</span><br><span class="line">        tokenizer=tokenizer,</span><br><span class="line">        max_len=max_len</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> DataLoader(</span><br><span class="line">        ds,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line"><span class="comment">#         num_workers=4 # windows多线程</span></span><br><span class="line">    )</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)</span><br><span class="line">val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)</span><br><span class="line">test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">next</span>(<span class="built_in">iter</span>(train_data_loader))</span><br></pre></td></tr></table></figure><pre><code>&#123;&#39;texts&#39;: [&#39;指示标识不清楚[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]安全出口、疏散通道是否畅通，安全疏散指示标志、应急照明是否完好；&#39;,  &#39;发现本月有灭火器过期，已安排购买灭火器更换[SEP]商贸服务教文卫类[SEP]消防检查[SEP]防火检查[SEP]灭火器材配置及有效情况。&#39;,  &#39;安全出口标志灯有一个有故障，已买回安装改正。[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]安全出口、疏散通道是否畅通，安全疏散指示标志、应急照明是否完好；&#39;,  &#39;堵了消防通道[SEP]工业/危化品类[SEP]消防检查[SEP]防火巡查[SEP]安全出口、疏散通道是否畅通，安全疏散指示标志、应急照明是否完好；&#39;], &#39;input_ids&#39;: tensor([[ 101, 2900, 4850, 3403, 6399,  679, 3926, 3504,  102, 2339,  689,  120,          1314, 1265, 1501, 5102,  102, 3867, 7344, 3466, 3389,  102, 7344, 4125,          2337, 3389,  102, 2128, 1059, 1139, 1366,  510, 4541, 3141, 6858, 6887,          3221, 1415, 4517, 6858, 8024, 2128, 1059, 4541, 3141, 2900, 4850, 3403,          2562,  510, 2418, 2593, 4212, 3209, 3221, 1415, 2130, 1962, 8039,  102,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0],         [ 101, 1355, 4385, 3315, 3299, 3300, 4127, 4125, 1690, 6814, 3309, 8024,          2347, 2128, 2961, 6579,  743, 4127, 4125, 1690, 3291, 2940,  102, 1555,          6588, 3302, 1218, 3136, 3152, 1310, 5102,  102, 3867, 7344, 3466, 3389,           102, 7344, 4125, 3466, 3389,  102, 4127, 4125, 1690, 3332, 6981, 5390,          1350, 3300, 3126, 2658, 1105,  511,  102,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0],         [ 101, 2128, 1059, 1139, 1366, 3403, 2562, 4128, 3300,  671,  702, 3300,          3125, 7397, 8024, 2347,  743, 1726, 2128, 6163, 3121, 3633,  511,  102,          2339,  689,  120, 1314, 1265, 1501, 5102,  102, 3867, 7344, 3466, 3389,           102, 7344, 4125, 2337, 3389,  102, 2128, 1059, 1139, 1366,  510, 4541,          3141, 6858, 6887, 3221, 1415, 4517, 6858, 8024, 2128, 1059, 4541, 3141,          2900, 4850, 3403, 2562,  510, 2418, 2593, 4212, 3209, 3221, 1415, 2130,          1962, 8039,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0],         [ 101, 1843,  749, 3867, 7344, 6858, 6887,  102, 2339,  689,  120, 1314,          1265, 1501, 5102,  102, 3867, 7344, 3466, 3389,  102, 7344, 4125, 2337,          3389,  102, 2128, 1059, 1139, 1366,  510, 4541, 3141, 6858, 6887, 3221,          1415, 4517, 6858, 8024, 2128, 1059, 4541, 3141, 2900, 4850, 3403, 2562,           510, 2418, 2593, 4212, 3209, 3221, 1415, 2130, 1962, 8039,  102,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,             0,    0,    0,    0]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), &#39;labels&#39;: tensor([0, 0, 0, 0])&#125;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = <span class="built_in">next</span>(<span class="built_in">iter</span>(train_data_loader))</span><br><span class="line">data.keys()</span><br></pre></td></tr></table></figure><pre><code>dict_keys([&#39;texts&#39;, &#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;labels&#39;])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;input_ids&#x27;</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;attention_mask&#x27;</span>].shape)</span><br><span class="line"><span class="built_in">print</span>(data[<span class="string">&#x27;labels&#x27;</span>].shape)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([4, 160])torch.Size([4, 160])torch.Size([4])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert_model = AutoModel.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoding[<span class="string">&#x27;input_ids&#x27;</span>]</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 101,  791, 1921, 3193,  677,  130, 4157, 1288, 6629, 2414, 8024, 2769,         1762, 2110,  739, 7564, 6378, 5298, 3563, 1798, 4638,  886, 4500,  119,          102,    0,    0,    0,    0,    0,    0,    0]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">last_hidden_state, pooled_output = bert_model(</span><br><span class="line">    input_ids=encoding[<span class="string">&#x27;input_ids&#x27;</span>], </span><br><span class="line">    attention_mask=encoding[<span class="string">&#x27;attention_mask&#x27;</span>],</span><br><span class="line">    return_dict = <span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="查看输出结果"><a href="#查看输出结果" class="headerlink" title="查看输出结果"></a>查看输出结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">last_hidden_state[<span class="number">0</span>][<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><pre><code>torch.Size([768])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pooled_output</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.9999,  0.9998,  0.9989,  0.9629,  0.3075, -0.1866, -0.9904,  0.8628,          0.9710, -0.9993,  1.0000,  1.0000,  0.9312, -0.9394,  0.9998, -0.9999,          0.0417,  0.9999,  0.9458,  0.3190,  1.0000, -1.0000, -0.9062, -0.9048,          0.1764,  0.9983,  0.9346, -0.8122, -0.9999,  0.9996,  0.7879,  0.9999,          0.8475, -1.0000, -1.0000,  0.9413, -0.8260,  0.9889, -0.4976, -0.9857,         -0.9955, -0.9580,  0.5833, -0.9996, -0.8932,  0.8563, -1.0000, -0.9999,          0.9719,  0.9999, -0.7430, -0.9993,  0.9756, -0.9754,  0.2991,  0.8933,         -0.9991,  0.9987,  1.0000,  0.4156,  0.9992, -0.9452, -0.8020, -0.9999,          1.0000, -0.9964, -0.9900,  0.4365,  1.0000,  1.0000, -0.9400,  0.8794,          1.0000,  0.9105, -0.6616,  1.0000, -0.9999,  0.6892, -1.0000, -0.9817,          1.0000,  0.9957, -0.8844, -0.8248, -0.9921, -0.9999, -0.9998,  1.0000,          0.5228,  0.1297,  0.9932, -0.9999, -1.0000,  0.9993, -0.9996, -0.9948,         -0.9561,  0.9996, -0.5785, -0.9386, -0.2035,  0.9086, -0.9999, -0.9993,          0.9959,  0.9984,  0.6953, -0.9995,  1.0000,  0.8610, -1.0000, -0.4507,         -1.0000,  0.2384, -0.9812,  0.9998,  0.9504,  0.5421,  0.9995, -0.9998,          0.9320, -0.9941, -0.9718, -0.9910,  0.9822,  1.0000,  0.9997, -0.9990,          1.0000,  1.0000,  0.8608,  0.9964, -0.9997,  0.9799,  0.5985, -0.9098,          0.5329, -0.6345,  1.0000,  0.9872,  0.9970, -0.9719,  0.9988, -0.9933,          1.0000, -0.9999,  0.9973, -1.0000, -0.6550,  0.9996,  0.8899,  1.0000,          0.2969,  0.9999, -0.9983, -0.9991,  0.9906, -0.6590,  0.9872, -1.0000,          0.7658,  0.7876, -0.8556,  0.6304, -1.0000,  1.0000, -0.7938,  1.0000,          0.9898,  0.2216, -0.9942, -0.9969,  0.8345, -0.9998, -0.9779,  0.9914,          0.5227,  0.9992, -0.9893, -0.9889,  0.2325, -0.9887, -0.9999,  0.9885,          0.0340,  0.9284,  0.5197,  0.4143,  0.8315,  0.1585, -0.5348,  1.0000,          0.2361,  0.9985,  0.9999, -0.3446,  0.1012, -0.9924, -1.0000, -0.7542,          0.9999, -0.2807, -0.9999,  0.9490, -1.0000,  0.9906, -0.7288, -0.5263,         -0.9545, -0.9999,  0.9998, -0.9286, -0.9997, -0.5303,  0.8886,  0.5605,         -0.9989, -0.3324,  0.9804, -0.9075,  0.9905, -0.9800, -0.9946,  0.6855,         -0.9393,  0.9929,  0.9874,  1.0000,  0.9997, -0.0714, -0.9440,  1.0000,          0.1676, -1.0000,  0.5573, -0.9611,  0.8835,  0.9999, -0.9980,  0.9294,          1.0000,  0.7968,  1.0000, -0.7065, -0.9793, -0.9997,  1.0000,  0.9922,          0.9999, -0.9984, -0.9995, -0.1701, -0.5426, -1.0000, -1.0000, -0.6334,          0.9969,  0.9999, -0.1620, -0.9818, -0.9921, -0.9994,  1.0000, -0.9759,          1.0000,  0.8570, -0.7434, -0.9164,  0.9438, -0.7311, -0.9986, -0.3936,         -0.9997, -0.9650, -1.0000,  0.9433, -0.9999, -1.0000,  0.6913,  1.0000,          0.8762, -1.0000,  0.9997,  0.9764,  0.7094, -0.9294,  0.9522, -1.0000,          1.0000, -0.9965,  0.9428, -0.9972, -0.9897, -0.7680,  0.9922,  0.9999,         -0.9999, -0.9597, -0.9922, -0.9807, -0.3632,  0.9936, -0.7280,  0.4117,         -0.9498, -0.9666,  0.9545, -0.9957, -0.9970,  0.4028,  1.0000, -0.9798,          1.0000,  0.9941,  1.0000,  0.9202, -0.9942,  0.9996,  0.5352, -0.5836,         -0.8829, -0.9418,  0.9497, -0.0532,  0.6966, -0.9999,  0.9998,  0.9917,          0.9612,  0.7289,  0.0167,  0.3179,  0.9627, -0.9911,  0.9995, -0.9996,         -0.6737,  0.9991,  1.0000,  0.9932,  0.4880, -0.7488,  0.9986, -0.9961,          0.9995, -1.0000,  0.9999, -0.9940,  0.9705, -0.9970, -0.9856,  1.0000,          0.9846, -0.7932,  0.9997, -0.9386,  0.9938,  0.9738,  0.8173,  0.9913,          0.9981,  1.0000, -0.9998, -0.9918, -0.9727, -0.9987, -0.9955, -1.0000,         -0.1038, -1.0000, -0.9874, -0.9287,  0.5109, -0.9056,  0.1022,  0.7864,         -0.8197,  0.5724, -0.5905,  0.2713, -0.7239, -0.9976, -0.9844, -1.0000,         -0.9988,  0.8835,  0.9999, -0.9997,  0.9999, -0.9999, -0.9782,  0.9383,         -0.5609,  0.7721,  0.9999, -1.0000,  0.9585,  0.9987,  1.0000,  0.9960,          0.9993, -0.9741, -0.9999, -0.9989, -0.9999, -1.0000, -0.9998,  0.9343,          0.6337, -1.0000,  0.0902,  0.8980,  1.0000,  0.9964, -0.9985, -0.6136,         -0.9996, -0.8252,  0.9996, -0.0566, -1.0000,  0.9962, -0.8744,  1.0000,         -0.8865,  0.9879,  0.8897,  0.9571,  0.9823, -1.0000,  0.9145,  1.0000,          0.0365, -1.0000, -0.9985, -0.9075, -0.9998,  0.0369,  0.8120,  0.9999,         -1.0000, -0.9155, -0.9975,  0.7988,  0.9922,  0.9998,  0.9982,  0.9267,          0.9165,  0.5368,  0.1464,  0.9998,  0.4663, -0.9989,  0.9996, -0.7952,          0.4527, -1.0000,  0.9998,  0.4073,  0.9999,  0.9159, -0.5480, -0.6821,         -0.9904,  0.9938,  1.0000, -0.4229, -0.4845, -0.9981, -1.0000, -0.9861,         -0.0950, -0.4625, -0.9629, -0.9998,  0.6675, -0.5244,  1.0000,  1.0000,          0.9924, -0.9253, -0.9974,  0.9974, -0.9012,  0.9900, -0.2582, -1.0000,         -0.9919, -0.9986,  1.0000, -0.9716, -0.9262, -0.9911, -0.2593,  0.5919,         -0.9999, -0.4994, -0.9962,  0.9818,  1.0000, -0.9996,  0.9918, -0.9970,          0.7085, -0.1369,  0.8077,  0.9955, -0.3394, -0.5860, -0.6887, -0.9841,          0.9970,  0.9987, -0.9948, -0.8401,  0.9999,  0.0856,  0.9999,  0.5099,          0.9466,  0.9567,  1.0000,  0.8771,  1.0000, -0.0815,  1.0000,  0.9999,         -0.9392,  0.5744,  0.8723, -0.9686,  0.5958,  0.9822,  0.9997,  0.8854,         -0.1952, -0.9967,  0.9994,  1.0000,  1.0000, -0.3391,  0.9883, -0.4452,          0.9252,  0.4495,  0.9870,  0.3479,  0.2266,  0.9942,  0.9990, -0.9999,         -0.9999, -1.0000,  1.0000,  0.9996, -0.6637, -1.0000,  0.9999,  0.4543,          0.7471,  0.9983,  0.3772, -0.9812,  0.9853, -0.9995, -0.3404,  0.9788,          0.9867,  0.7564,  0.9995, -0.9997,  0.7990,  1.0000,  0.0752,  0.9999,          0.2912, -0.9941,  0.9970, -0.9935, -0.9995, -0.9743,  0.9991,  0.9981,         -0.9273, -0.8402,  0.9996, -0.9999,  0.9999, -0.9998,  0.9724, -0.9939,          1.0000, -0.9752, -0.9998, -0.3806,  0.8830,  0.8352, -0.8892,  1.0000,         -0.8875, -0.8107,  0.7083, -0.8909, -0.9931, -0.9630,  0.0800, -1.0000,          0.7777, -0.9611,  0.5867, -0.9947, -0.9999,  1.0000, -0.9084, -0.9414,          0.9999, -0.8838, -1.0000,  0.9549, -0.9999, -0.6522,  0.7967, -0.6850,          0.1524, -1.0000,  0.4800,  0.9999, -0.9998, -0.7089, -0.9129, -0.9864,          0.6220,  0.8855,  0.9855, -0.8651,  0.3988, -0.2548,  0.9793, -0.7212,         -0.2582, -0.9999, -0.8692, -0.6282, -0.9999, -0.9999, -1.0000,  1.0000,          0.9996,  0.9999, -0.5600,  0.7442,  0.9460,  0.9927, -0.9999,  0.4407,         -0.0461,  0.9937, -0.4887, -0.9994, -0.9198, -1.0000, -0.6905,  0.3538,         -0.7728,  0.6622,  1.0000,  0.9999, -0.9999, -0.9994, -0.9995, -0.9979,          0.9998,  0.9999,  0.9996, -0.9072, -0.5844,  0.9997,  0.9689,  0.5231,         -0.9999, -0.9981, -0.9999,  0.7505, -0.9922, -0.9986,  0.9971,  1.0000,          0.8730, -1.0000, -0.9533,  1.0000,  0.9997,  1.0000, -0.7768,  0.9999,         -0.9838,  0.9819, -0.9993,  1.0000, -1.0000,  1.0000,  0.9999,  0.9809,          0.9984, -0.9928,  0.9776, -0.9998, -0.7407,  0.9298, -0.4495, -0.9902,          0.8053,  0.9996, -0.9952,  1.0000,  0.9243, -0.2028,  0.8002,  0.9873,          0.9419, -0.6913, -0.9999,  0.8162,  0.9995,  0.9509,  1.0000,  0.9177,          0.9996, -0.9839, -0.9998,  0.9914, -0.6991, -0.7821, -0.9998,  1.0000,          1.0000, -0.9999, -0.9227,  0.7483,  0.1186,  1.0000,  0.9963,  0.9971,          0.9857,  0.3887,  0.9996, -0.9999,  0.8526, -0.9980, -0.8613,  0.9999,         -0.9899,  0.9999, -0.9981,  1.0000, -0.9858,  0.9944,  0.9989,  0.9684,         -0.9968,  1.0000,  0.8246, -0.9956, -0.8348, -0.9374, -0.9999,  0.7827]],       grad_fn=&lt;TanhBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">last_hidden_state.shape <span class="comment"># 每个token的向量表示</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([1, 32, 768])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert_model.config</span><br></pre></td></tr></table></figure><pre><code>BertConfig &#123;  &quot;_name_or_path&quot;: &quot;bert-base-chinese&quot;,  &quot;architectures&quot;: [    &quot;BertForMaskedLM&quot;  ],  &quot;attention_probs_dropout_prob&quot;: 0.1,  &quot;classifier_dropout&quot;: null,  &quot;directionality&quot;: &quot;bidi&quot;,  &quot;hidden_act&quot;: &quot;gelu&quot;,  &quot;hidden_dropout_prob&quot;: 0.1,  &quot;hidden_size&quot;: 768,  &quot;initializer_range&quot;: 0.02,  &quot;intermediate_size&quot;: 3072,  &quot;layer_norm_eps&quot;: 1e-12,  &quot;max_position_embeddings&quot;: 512,  &quot;model_type&quot;: &quot;bert&quot;,  &quot;num_attention_heads&quot;: 12,  &quot;num_hidden_layers&quot;: 12,  &quot;pad_token_id&quot;: 0,  &quot;pooler_fc_size&quot;: 768,  &quot;pooler_num_attention_heads&quot;: 12,  &quot;pooler_num_fc_layers&quot;: 3,  &quot;pooler_size_per_head&quot;: 128,  &quot;pooler_type&quot;: &quot;first_token_transform&quot;,  &quot;position_embedding_type&quot;: &quot;absolute&quot;,  &quot;transformers_version&quot;: &quot;4.24.0&quot;,  &quot;type_vocab_size&quot;: 2,  &quot;use_cache&quot;: true,  &quot;vocab_size&quot;: 21128&#125;</code></pre><h2 id="bert后面又接了一个全连接层"><a href="#bert后面又接了一个全连接层" class="headerlink" title="bert后面又接了一个全连接层"></a>bert后面又接了一个全连接层</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EnterpriseDangerClassifier</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, n_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(EnterpriseDangerClassifier, self).__init__()</span><br><span class="line">        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)</span><br><span class="line">        self.drop = nn.Dropout(p=<span class="number">0.3</span>)</span><br><span class="line">        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) <span class="comment"># 两个类别</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, attention_mask</span>):</span><br><span class="line">        _, pooled_output = self.bert(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            return_dict = <span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line">        output = self.drop(pooled_output) <span class="comment"># dropout</span></span><br><span class="line">        <span class="keyword">return</span> self.out(output)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class_names=[<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h2 id="将数据和模型送到CUDA"><a href="#将数据和模型送到CUDA" class="headerlink" title="将数据和模型送到CUDA"></a>将数据和模型送到CUDA</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer,BertConfig, AdamW, get_linear_schedule_with_warmup</span><br><span class="line"></span><br><span class="line">model = EnterpriseDangerClassifier(<span class="built_in">len</span>(class_names))</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure><pre><code>Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: [&#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.seq_relationship.weight&#39;]- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">input_ids = data[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">attention_mask = data[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(input_ids.shape) <span class="comment"># batch size x seq length</span></span><br><span class="line"><span class="built_in">print</span>(attention_mask.shape) <span class="comment"># batch size x seq length</span></span><br></pre></td></tr></table></figure><pre><code>torch.Size([4, 160])torch.Size([4, 160])</code></pre><h2 id="得到单个batch的输出token"><a href="#得到单个batch的输出token" class="headerlink" title="得到单个batch的输出token"></a>得到单个batch的输出token</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model(input_ids, attention_mask)</span><br></pre></td></tr></table></figure><pre><code>tensor([[ 0.2120, -0.4050],        [ 0.3156, -0.4160],        [ 0.5127, -0.4634],        [ 0.3168,  0.5057]], device=&#39;cuda:0&#39;, grad_fn=&lt;AddmmBackward0&gt;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.softmax(model(input_ids, attention_mask), dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>tensor([[0.4999, 0.5001],        [0.5258, 0.4742],        [0.5899, 0.4101],        [0.4575, 0.5425]], device=&#39;cuda:0&#39;, grad_fn=&lt;SoftmaxBackward0&gt;)</code></pre><h2 id="训练模型前期配置"><a href="#训练模型前期配置" class="headerlink" title="训练模型前期配置"></a>训练模型前期配置</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">EPOCHS = <span class="number">10</span> <span class="comment"># 训练轮数</span></span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>, correct_bias=<span class="literal">False</span>)</span><br><span class="line">total_steps = <span class="built_in">len</span>(train_data_loader) * EPOCHS</span><br><span class="line"></span><br><span class="line"><span class="comment"># Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,</span></span><br><span class="line"><span class="comment"># 等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳</span></span><br><span class="line">scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">  optimizer,</span><br><span class="line">  num_warmup_steps=<span class="number">0</span>,</span><br><span class="line">  num_training_steps=total_steps</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># optimizer： 优化器</span></span><br><span class="line"><span class="comment"># num_warmup_steps：初始预热步数</span></span><br><span class="line"><span class="comment"># num_training_steps：整个训练过程的总步数</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss().to(device)</span><br></pre></td></tr></table></figure><h2 id="定义模型的训练"><a href="#定义模型的训练" class="headerlink" title="定义模型的训练"></a>定义模型的训练</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params"></span></span><br><span class="line"><span class="params">  model, </span></span><br><span class="line"><span class="params">  data_loader, </span></span><br><span class="line"><span class="params">  loss_fn, </span></span><br><span class="line"><span class="params">  optimizer, </span></span><br><span class="line"><span class="params">  device, </span></span><br><span class="line"><span class="params">  scheduler, </span></span><br><span class="line"><span class="params">  n_examples</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    model = model.train() <span class="comment"># train模式</span></span><br><span class="line">    losses = []</span><br><span class="line">    correct_predictions = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data_loader:</span><br><span class="line">        input_ids = d[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">        attention_mask = d[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">        targets = d[<span class="string">&quot;labels&quot;</span>].to(device)</span><br><span class="line">        outputs = model(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            attention_mask=attention_mask</span><br><span class="line">        )</span><br><span class="line">        _, preds = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line">        loss = loss_fn(outputs, targets)</span><br><span class="line">        correct_predictions += torch.<span class="built_in">sum</span>(preds == targets)</span><br><span class="line">        losses.append(loss.item())</span><br><span class="line">        loss.backward()</span><br><span class="line">        nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># 预热学习率</span></span><br><span class="line">        scheduler.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> correct_predictions.double() / n_examples, np.mean(losses)</span><br></pre></td></tr></table></figure><h2 id="模型的评估函数"><a href="#模型的评估函数" class="headerlink" title="模型的评估函数"></a>模型的评估函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">eval_model</span>(<span class="params">model, data_loader, loss_fn, device, n_examples</span>):</span><br><span class="line">    model = model.<span class="built_in">eval</span>() <span class="comment"># 验证预测模式</span></span><br><span class="line"></span><br><span class="line">    losses = []</span><br><span class="line">    correct_predictions = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data_loader:</span><br><span class="line">            input_ids = d[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">            attention_mask = d[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">            targets = d[<span class="string">&quot;labels&quot;</span>].to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(</span><br><span class="line">                input_ids=input_ids,</span><br><span class="line">                attention_mask=attention_mask</span><br><span class="line">            )</span><br><span class="line">            _, preds = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">            correct_predictions += torch.<span class="built_in">sum</span>(preds == targets)</span><br><span class="line">            losses.append(loss.item())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> correct_predictions.double() / n_examples, np.mean(losses)</span><br></pre></td></tr></table></figure><h2 id="训练模型：10EPOCHS"><a href="#训练模型：10EPOCHS" class="headerlink" title="训练模型：10EPOCHS"></a>训练模型：10EPOCHS</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">history = defaultdict(<span class="built_in">list</span>) <span class="comment"># 记录10轮loss和acc</span></span><br><span class="line">best_accuracy = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(EPOCHS):</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;EPOCHS&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span> * <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    train_acc, train_loss = train_epoch(</span><br><span class="line">        model,</span><br><span class="line">        train_data_loader,</span><br><span class="line">        loss_fn,</span><br><span class="line">        optimizer,</span><br><span class="line">        device,</span><br><span class="line">        scheduler,</span><br><span class="line">        <span class="built_in">len</span>(df_train)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Train loss <span class="subst">&#123;train_loss&#125;</span> accuracy <span class="subst">&#123;train_acc&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    val_acc, val_loss = eval_model(</span><br><span class="line">        model,</span><br><span class="line">        val_data_loader,</span><br><span class="line">        loss_fn,</span><br><span class="line">        device,</span><br><span class="line">        <span class="built_in">len</span>(df_val)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Val   loss <span class="subst">&#123;val_loss&#125;</span> accuracy <span class="subst">&#123;val_acc&#125;</span>&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    history[<span class="string">&#x27;train_acc&#x27;</span>].append(train_acc)</span><br><span class="line">    history[<span class="string">&#x27;train_loss&#x27;</span>].append(train_loss)</span><br><span class="line">    history[<span class="string">&#x27;val_acc&#x27;</span>].append(val_acc)</span><br><span class="line">    history[<span class="string">&#x27;val_loss&#x27;</span>].append(val_loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> val_acc &gt; best_accuracy:</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">&#x27;best_model_state.bin&#x27;</span>)</span><br><span class="line">        best_accuracy = val_acc</span><br></pre></td></tr></table></figure><pre><code>Epoch 1/10----------Train loss 0.4988938277521757 accuracy 0.8899999999999999Val   loss 0.4194765945523977 accuracy 0.9Epoch 2/10----------Train loss 0.4967574527254328 accuracy 0.8905555555555555Val   loss 0.43736912585794924 accuracy 0.9Epoch 3/10----------Train loss 0.49347498720511795 accuracy 0.8905555555555555Val   loss 0.41818931301434836 accuracy 0.9Epoch 4/10----------Train loss 0.4900011462407807 accuracy 0.8905555555555555Val   loss 0.42409916249414287 accuracy 0.9Epoch 5/10----------Train loss 0.4952681002088098 accuracy 0.8888888888888888Val   loss 0.31909402589624125 accuracy 0.9Epoch 6/10----------Train loss 0.2478140213253425 accuracy 0.9463888888888888Val   loss 0.1787985412031412 accuracy 0.9666666666666667Epoch 7/10----------Train loss 0.17434944392257387 accuracy 0.9677777777777777Val   loss 0.15001839348037416 accuracy 0.9700000000000001Epoch 8/10----------Train loss 0.12048366091100939 accuracy 0.9775925925925926Val   loss 0.11547344802587758 accuracy 0.9783333333333334Epoch 9/10----------Train loss 0.10136666681817992 accuracy 0.9813888888888889Val   loss 0.10292303454208498 accuracy 0.9800000000000001Epoch 10/10----------Train loss 0.08721379442805402 accuracy 0.9831481481481481Val   loss 0.12598223814862042 accuracy 0.9766666666666667</code></pre><h2 id="准确率绘图"><a href="#准确率绘图" class="headerlink" title="准确率绘图"></a>准确率绘图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">plt.plot([i.cpu() <span class="keyword">for</span> i <span class="keyword">in</span> history[<span class="string">&#x27;train_acc&#x27;</span>]], label=<span class="string">&#x27;train accuracy&#x27;</span>)</span><br><span class="line">plt.plot([i.cpu() <span class="keyword">for</span> i <span class="keyword">in</span> history[<span class="string">&#x27;val_acc&#x27;</span>]], label=<span class="string">&#x27;validation accuracy&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&#x27;Training history&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Epoch&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">1</span>]);</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079408883.png" alt="1669079408883"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">test_acc, _ = eval_model(</span><br><span class="line">  model,</span><br><span class="line">  test_data_loader,</span><br><span class="line">  loss_fn,</span><br><span class="line">  device,</span><br><span class="line">  <span class="built_in">len</span>(df_test)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_acc.item()</span><br></pre></td></tr></table></figure><pre><code>0.9783333333333334</code></pre><h2 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_predictions</span>(<span class="params">model, data_loader</span>):</span><br><span class="line">    model = model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    raw_texts = []</span><br><span class="line">    predictions = []</span><br><span class="line">    prediction_probs = []</span><br><span class="line">    real_values = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data_loader:</span><br><span class="line">            texts = d[<span class="string">&quot;texts&quot;</span>]</span><br><span class="line">            input_ids = d[<span class="string">&quot;input_ids&quot;</span>].to(device)</span><br><span class="line">            attention_mask = d[<span class="string">&quot;attention_mask&quot;</span>].to(device)</span><br><span class="line">            targets = d[<span class="string">&quot;labels&quot;</span>].to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(</span><br><span class="line">                input_ids=input_ids,</span><br><span class="line">                attention_mask=attention_mask</span><br><span class="line">            )</span><br><span class="line">            _, preds = torch.<span class="built_in">max</span>(outputs, dim=<span class="number">1</span>) <span class="comment"># 类别</span></span><br><span class="line"></span><br><span class="line">            probs = F.softmax(outputs, dim=<span class="number">1</span>) <span class="comment"># 概率</span></span><br><span class="line"></span><br><span class="line">            raw_texts.extend(texts)</span><br><span class="line">            predictions.extend(preds)</span><br><span class="line">            prediction_probs.extend(probs)</span><br><span class="line">            real_values.extend(targets)</span><br><span class="line"></span><br><span class="line">    predictions = torch.stack(predictions).cpu()</span><br><span class="line">    prediction_probs = torch.stack(prediction_probs).cpu()</span><br><span class="line">    real_values = torch.stack(real_values).cpu()</span><br><span class="line">    <span class="keyword">return</span> raw_texts, predictions, prediction_probs, real_values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">y_texts, y_pred, y_pred_probs, y_test = get_predictions(</span><br><span class="line">  model,</span><br><span class="line">  test_data_loader</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred, target_names=[<span class="built_in">str</span>(label) <span class="keyword">for</span> label <span class="keyword">in</span> class_names])) <span class="comment"># 分类报告</span></span><br></pre></td></tr></table></figure><pre><code>              precision    recall  f1-score   support           0       0.99      0.99      0.99       554           1       0.84      0.89      0.86        46    accuracy                           0.98       600   macro avg       0.91      0.94      0.93       600weighted avg       0.98      0.98      0.98       600</code></pre><h2 id="查看混淆矩阵"><a href="#查看混淆矩阵" class="headerlink" title="查看混淆矩阵"></a>查看混淆矩阵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_confusion_matrix</span>(<span class="params">confusion_matrix</span>):</span><br><span class="line">    hmap = sns.heatmap(confusion_matrix, annot=<span class="literal">True</span>, fmt=<span class="string">&quot;d&quot;</span>, cmap=<span class="string">&quot;Blues&quot;</span>)</span><br><span class="line">    hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=<span class="number">0</span>, ha=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line">    hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=<span class="number">30</span>, ha=<span class="string">&#x27;right&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;True label&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Predicted label&#x27;</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line">df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)</span><br><span class="line">show_confusion_matrix(df_cm)</span><br></pre></td></tr></table></figure><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079467197.png" alt="1669079467197">    </p><h2 id="评估单条数据"><a href="#评估单条数据" class="headerlink" title="评估单条数据"></a>评估单条数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">idx = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">sample_text = y_texts[idx]</span><br><span class="line">true_label = y_test[idx]</span><br><span class="line">pred_df = pd.DataFrame(&#123;</span><br><span class="line">  <span class="string">&#x27;class_names&#x27;</span>: class_names,</span><br><span class="line">  <span class="string">&#x27;values&#x27;</span>: y_pred_probs[idx]</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred_d</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>class_names</th>      <th>values</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>0.999889</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>0.000111</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>.join(wrap(sample_text)))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;True label: <span class="subst">&#123;class_names[true_label]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>小A班应急照明灯坏[SEP]商贸服务教文卫类[SEP]消防检查[SEP]防火巡查[SEP]安全出口、疏散通道是否畅通，安全疏散指示标志、应急照明是否完好。True label: 0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sns.barplot(x=<span class="string">&#x27;values&#x27;</span>, y=<span class="string">&#x27;class_names&#x27;</span>, data=pred_df, orient=<span class="string">&#x27;h&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;sentiment&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;probability&#x27;</span>)</span><br><span class="line">plt.xlim([<span class="number">0</span>, <span class="number">1</span>]);</span><br></pre></td></tr></table></figure><p><img src="C:\Users\dupeibo\AppData\Roaming\Typora\typora-user-images\1669079499324.png" alt="1669079499324"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_text = <span class="string">&#x27; &#x27;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">encoded_text = tokenizer.encode_plus(</span><br><span class="line">  sample_text,</span><br><span class="line">  max_length=MAX_LEN,</span><br><span class="line">  add_special_tokens=<span class="literal">True</span>,</span><br><span class="line">  return_token_type_ids=<span class="literal">False</span>,</span><br><span class="line">  pad_to_max_length=<span class="literal">True</span>,</span><br><span class="line">  return_attention_mask=<span class="literal">True</span>,</span><br><span class="line">  return_tensors=<span class="string">&#x27;pt&#x27;</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">input_ids = encoded_text[<span class="string">&#x27;input_ids&#x27;</span>].to(device)</span><br><span class="line">attention_mask = encoded_text[<span class="string">&#x27;attention_mask&#x27;</span>].to(device)</span><br><span class="line"></span><br><span class="line">output = model(input_ids, attention_mask)</span><br><span class="line">_, prediction = torch.<span class="built_in">max</span>(output, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Sample text: <span class="subst">&#123;sample_text&#125;</span>&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Danger label  : <span class="subst">&#123;class_names[prediction]&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>Sample text:  Danger label  : 1</code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;通过智能化手段识别其中是否存在“虚报、假报”的情况&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="文本分类" scheme="https://du2279664786.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
    <category term="BERT" scheme="https://du2279664786.github.io/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>Transformer总结和梳理</title>
    <link href="https://du2279664786.github.io/2022/11/20/2022-11-20Transformer%E6%80%BB%E7%BB%93%E5%92%8C%E6%A2%B3%E7%90%86/"/>
    <id>https://du2279664786.github.io/2022/11/20/2022-11-20Transformer%E6%80%BB%E7%BB%93%E5%92%8C%E6%A2%B3%E7%90%86/</id>
    <published>2022-11-20T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.768Z</updated>
    
    <content type="html"><![CDATA[<p>Transformer最终总结版，超级详细！</p><span id="more"></span><p>首先来看一下Transformer结构的结构：<br><img src="https://img-blog.csdnimg.cn/de74182b27f24a84a28fdd5f7204f0cd.png" alt="在这里插入图片描述"><br>Transformer是由Encoder和Decoder两大部分组成，首先我们先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构。<br>        Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，然后又做了一个ADD&amp;Norm，再通过Feed Forward进行输出，最后又做了一个ADD&amp;Norm，这就是Decoder。<br>        Decoder和Encoder有很多相同的地方，Decoder首先也是需要进行 Input Embedding 和 Positional Embedding 求作为输入，并且Decoder中的第一个Attention模块为Mask Attention，还有一个就是Decoder的K和V分别均来自Encoder。<br>        接下来看一下每个模块的具体理解：</p><h1 id="Positional-encoding"><a href="#Positional-encoding" class="headerlink" title="Positional encoding"></a>Positional encoding</h1><p>首先对于文本特征，需要进行Embedding，由于transformer抛弃了Rnn的结构，不能捕捉到序列的信息，交换单词位置，得到相应的attention也会发生交换，并不会发生数值上的改变，所以要对input进行Positional Encoding。</p><p>Positional encoding和input embedding是同等维度的，所以可以将两者进行相加，的到输入向量<br><img src="https://img-blog.csdnimg.cn/be30b27838dd411c89d793432ff72582.png" alt="在这里插入图片描述"><br>接下来看一些Positional Encoding的计算公式：<br><img src="https://img-blog.csdnimg.cn/6e9a80e756b94a70aeef8a79097eb7a6.png" alt="在这里插入图片描述"><br>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值，也就是说：对于单个token的d_model维度的词向量，奇数位置取cos，偶数位置取sin，最终的到一个维度和word embedding维度一样的矩阵，接下来可以看一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_positional_encoding</span>(<span class="params">max_seq_len, embed_dim</span>):</span><br><span class="line">    <span class="comment"># 初始化一个positional encoding</span></span><br><span class="line">    <span class="comment"># embed_dim: 字嵌入的维度</span></span><br><span class="line">    <span class="comment"># max_seq_len: 最大的序列长度</span></span><br><span class="line">    positional_encoding = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * i / embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embed_dim)] <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(embed_dim) <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len)])</span><br><span class="line"></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i 偶数</span></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1 奇数</span></span><br><span class="line">    <span class="keyword">return</span> positional_encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">positional_encoding = get_positional_encoding(max_seq_len=<span class="number">100</span>, embed_dim=<span class="number">16</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">sns.heatmap(positional_encoding)</span><br><span class="line">plt.title(<span class="string">&quot;Sinusoidal Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;hidden dimension&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;sequence length&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>首先求初始向量：positional_encoding，然后对其奇数列求sin，偶数列求cos：<br><img src="https://img-blog.csdnimg.cn/2da3445d7953426394ab2e46d8820baf.png" alt="在这里插入图片描述"><br>最终得到positional encoding之后的数据可视化：<br><img src="https://img-blog.csdnimg.cn/507cc7ca0ba34f8fb2ec87216689857c.png" alt="在这里插入图片描述"></p><h1 id="Self-attention"><a href="#Self-attention" class="headerlink" title="Self-attention"></a>Self-attention</h1><p>何为self-attention？首先我们要明白什么是attention，对于传统的seq2seq任务，例如中-英文翻译，输入中文，得到英文，即source是中文句子（x1 x2 x3）,英文句子是target（y1 y2 y3）<br><img src="https://img-blog.csdnimg.cn/296c379fd77c475ebf77c06fa8e42e59.png" alt="在这里插入图片描述"><br>attention机制发生在target的元素和source中的所有元素之间。简单的将就是attention机制中的权重计算需要target参与，即在上述Encoder-Decoder模型中，Encoder和Decoder两部分都需要参与运算。</p><p>而对于self-attention，它不需要Decoder的参与，而是source内部元素之间发生的运算，对于输入向量X，对其做线性变换，分别得到Q、K、V矩阵<br><img src="https://img-blog.csdnimg.cn/17358bbb1cf641d78117625fb5a00d31.png" alt="在这里插入图片描述"><br>然后去计算attention，Q、K点乘得到初步的权重因子，并对Q、K点乘结果进行放缩，除以sqrt（dk），Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差，最终再加一个softmax就得到了self attention的输出。<br><img src="https://img-blog.csdnimg.cn/07fdbd802d114b7193c70e9fc451ba22.png" alt="在这里插入图片描述"></p><h2 id="Multi–head-attention"><a href="#Multi–head-attention" class="headerlink" title="Multi–head-attention"></a>Multi–head-attention</h2><p>Multi–head-attention使用了多个头进行运算，捕捉到了更多的信息，多头的数量用h表示，一般h&#x3D;8，表示8个头<br><img src="https://img-blog.csdnimg.cn/7cad1e37120b4d40ad64140677452ec1.png" alt="在这里插入图片描述"><br>在输入每个self-attention之前，我们需将输入X均分的分到h个头中，得到Z1-Z7八个头的输出结果。<br><img src="https://img-blog.csdnimg.cn/d5763f726a5c48f292a140f84c0e5200.png" alt="在这里插入图片描述"><br>对于每个头计算相应的attention score，将其进行拼接，再与W0进行一个线性变换，就得到最终输出的Z。<br><img src="https://img-blog.csdnimg.cn/05bfb9eb87bb4f3b8066c8190c0dff0f.png" alt="在这里插入图片描述"></p><h1 id="Add-amp-Norm"><a href="#Add-amp-Norm" class="headerlink" title="Add&amp;Norm"></a>Add&amp;Norm</h1><h2 id="Add操作"><a href="#Add操作" class="headerlink" title="Add操作"></a>Add操作</h2><p>首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，再通过Feed Forward进行输出。</p><p>由下图可以看出：在Encoder层和Decoder层中都用到了Add&amp;Norm操作，即残差连接和层归一化操作。<br><img src="https://img-blog.csdnimg.cn/5ef722ad3c5b407482ac132b0883c59c.png" alt="在这里插入图片描述"><br>什么是残差连接呢？残差连接就是把网络的输入和输出相加，即网络的输出为F(x)+x，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就变成了F(x)+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项‘1’，有效解决了梯度消失问题。</p><h2 id="Norm操作"><a href="#Norm操作" class="headerlink" title="Norm操作"></a>Norm操作</h2><p>首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇<a href="https://mp.weixin.qq.com/s/HNCl6MPS_hjTVHNt7UkYyw">文章</a>，才明白了Norm是什么。</p><p>假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]</span><br><span class="line">[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]]</span><br></pre></td></tr></table></figure><p>如果是在做BatchNorm（BN）的话，其计算过程如下：BN1&#x3D;(w11+w12+w13+w14+w41+<br>w42+w43+w44)&#x2F;8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean</p><p>如果是在做LayerNorm（LN）的话，则会进如下计算：LN1&#x3D;(w11+w12+w13+w14+w21+<br>w22+w23+w24+w31+w32+w33+w34)&#x2F;12，同理会得到LN2，最终得到[LN1,LN2]两个mean</p><p>如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1&#x3D;(w11+w12+w13+w14)&#x2F;4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]<br>下图完美的揭示了，这几种Norm<br><img src="https://img-blog.csdnimg.cn/a143d6b41e654fa1849f44580401110c.png" alt="在这里插入图片描述"><br>接下来我们来看一下Transformer中的Norm：首先生成[2,3,4]形状的数据，使用原始的编码方式进行编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> InstanceNorm2d</span><br><span class="line">random_seed = <span class="number">123</span></span><br><span class="line">torch.manual_seed(random_seed)</span><br><span class="line"></span><br><span class="line">batch_size, seq_size, dim = <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">embedding = torch.randn(batch_size, seq_size, dim)</span><br><span class="line"></span><br><span class="line">layer_norm = torch.nn.LayerNorm(dim, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y: &quot;</span>, layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y:  tensor([[[ <span class="number">1.5524</span>,  <span class="number">0.0155</span>, -<span class="number">0.3596</span>, -<span class="number">1.2083</span>],</span><br><span class="line">         [ <span class="number">0.5851</span>,  <span class="number">1.3263</span>, -<span class="number">0.7660</span>, -<span class="number">1.1453</span>],</span><br><span class="line">         [ <span class="number">0.2864</span>,  <span class="number">0.0185</span>,  <span class="number">1.2388</span>, -<span class="number">1.5437</span>]],</span><br><span class="line">        [[ <span class="number">1.1119</span>, -<span class="number">0.3988</span>,  <span class="number">0.7275</span>, -<span class="number">1.4406</span>],</span><br><span class="line">         [-<span class="number">0.4144</span>, -<span class="number">1.1914</span>,  <span class="number">0.0548</span>,  <span class="number">1.5510</span>],</span><br><span class="line">         [ <span class="number">0.3914</span>, -<span class="number">0.5591</span>,  <span class="number">1.4105</span>, -<span class="number">1.2428</span>]]])</span><br></pre></td></tr></table></figure><p>接下来手动去进行一下编码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以发现和LayerNorm的结果是一样的，也就是说明Norm是对d_model进行的Norm，会给我们[batch,sqe_length]形状的平均值。<br>加下来进行batch_norm,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_norm = torch.nn.LayerNorm([seq_size,dim], elementwise_affine = <span class="literal">False</span>)</span><br><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1822</span>,  <span class="number">0.4419</span>, -<span class="number">0.3196</span>, -<span class="number">1.9889</span>],</span><br><span class="line">         [-<span class="number">0.6677</span>, -<span class="number">0.2537</span>, -<span class="number">0.8151</span>,  <span class="number">1.5143</span>],</span><br><span class="line">         [ <span class="number">0.7174</span>,  <span class="number">1.2147</span>, -<span class="number">0.0852</span>, -<span class="number">0.9403</span>]],</span><br><span class="line">        [[-<span class="number">0.0138</span>,  <span class="number">1.5666</span>, -<span class="number">2.1726</span>,  <span class="number">1.0590</span>],</span><br><span class="line">         [ <span class="number">0.6646</span>,  <span class="number">0.6852</span>, -<span class="number">0.8706</span>, -<span class="number">0.0442</span>],</span><br><span class="line">         [-<span class="number">0.1163</span>,  <span class="number">0.1389</span>,  <span class="number">0.4454</span>, -<span class="number">1.3423</span>]]])</span><br></pre></td></tr></table></figure><p>可以看到BN的计算的mean形状为[2, 1, 1]，并且Norm结果也和上面的两个不一样，这就充分说明了Norm是在对最后一个维度求平均。<br>那么什么又是Instancenorm呢？接下来再来实现一下instancenorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instance_norm = InstanceNorm2d(<span class="number">3</span>, affine=<span class="literal">False</span>)</span><br><span class="line">output = instance_norm(embedding.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)) <span class="comment">#InstanceNorm2D需要(N,C,H,W)的shape作为输入</span></span><br><span class="line">layer_norm = torch.nn.LayerNorm(<span class="number">4</span>, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br></pre></td></tr></table></figure><p>可以看出无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><h1 id="FeedForward"><a href="#FeedForward" class="headerlink" title="FeedForward"></a>FeedForward</h1><p>接下来是FeedForward，FeedForward是Multi-Head Attention的输出做了残差连接和Norm之后得数据，然后FeedForward做了两次线性线性变换，为的是更加深入的提取特征。<br><img src="https://img-blog.csdnimg.cn/b976d7add795475fac7bbc6c5f01121f.png" alt="在这里插入图片描述"><br>可以看出在每次线性变换都引入了非线性激活函数Relu，在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强，FeedForward的计算公式如下：max相当于Relu<br><img src="https://img-blog.csdnimg.cn/43b6886add22435795f3f8a7a889c58e.png" alt="在这里插入图片描述"></p><p>所以FeedForward的作用是：通过线性变换，先将数据映射到高纬度的空间再映射到低纬度的空间，提取了更深层次的特征</p><h1 id="MASK"><a href="#MASK" class="headerlink" title="MASK"></a>MASK</h1><p>最后是MASK介绍，Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分</p><h2 id="Padding-Masked"><a href="#Padding-Masked" class="headerlink" title="Padding Masked"></a>Padding Masked</h2><p>对于Transformer而言，每次的输入为：[batch_size,seq_length,d_module]结构，由于句子一般是长短不一的，而输入的数据需要是固定的格式，所以要对句子进行处理。<br>通常会把每个句子按照最大长度进行补齐，所以当句子不够长时，需要进行补0操作，以保证输入数据结构的完整性<br>但是在计算注意力机制时的Softmax函数时，就会出现问题，Padding数值为0的话，仍然会影响到Softmax的计算结果，即无效数据参加了运算。<br>为了不让Padding数据产生影响，通常会将Padding数据变为负无穷，这样的话就不会影响Softmax函数了</p><h2 id="Self-Attention-Masked"><a href="#Self-Attention-Masked" class="headerlink" title="Self-Attention Masked"></a>Self-Attention Masked</h2><p>Self-Attention Masked只发生在Decoder操作中，在Decoder中，我们的预测是一个一个进行的，即输入一个token，输出下一个token，在网上看到一个很好的解释如下：<br>假设我们当前在进行机器翻译<br>    输入：我很好<br>    输出：I am fine<br>接下来是Decoder执行步骤<br>第一步：<br>    ·初始输入： 起始符</s> + Positional Encoding（位置编码）<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“I”<br>第二步：<br>    ·初始输入：起始符</s> + “I”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“am”<br>第三步：<br>    ·初始输入：起始符</s> + “I”+ “am”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“fine”<br>上面就是Decoder的执行过程，在预测出“I”之前，我们是不可能知道‘am’的，所以要将数进行Mask，防止看到当前值后面的值，如下图所示：当我们仅知道start的时候，后面的关系是不知道的，所以start和“I”以及其它单词的Score都为负无穷，当预测出“I”之后，再去预测“am”，最终得到下面第三个得分矩阵。<br><img src="https://img-blog.csdnimg.cn/8df2f100c0a447909a8f9d99cbf86a1d.png"></p><p>最后经过Softmax处理之后，得到最终的得分矩阵<br><img src="https://img-blog.csdnimg.cn/90d840807bf04b89bf8e32dd6bf19e85.png"></p><p>最后不要忘了Decoder中依然采用的是Masked Multi-Head Attention，即多次进行Mask机制</p><p>写在最后：笔者也是刚入门深度学习，对知识也是初步的认识，如果文章有错，请大佬们斧正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Transformer最终总结版，超级详细！&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>什么是Warmup</title>
    <link href="https://du2279664786.github.io/2022/11/16/2022-11-16Warmup/"/>
    <id>https://du2279664786.github.io/2022/11/16/2022-11-16Warmup/</id>
    <published>2022-11-16T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.767Z</updated>
    
    <content type="html"><![CDATA[<p>Warmup可以逐渐地将学习率从一个小的值提升到一个大的值</p><span id="more"></span><h1 id="Warmup"><a href="#Warmup" class="headerlink" title="Warmup"></a>Warmup</h1><h3 id="什么是预热学习率-Warmup-？"><a href="#什么是预热学习率-Warmup-？" class="headerlink" title="什么是预热学习率(Warmup)？"></a>什么是预热学习率(Warmup)？</h3><p>先来看一下预热的定义：预热指的是为防止急热，焊接前先对材料预热。</p><p>而预热学习率指的是对学习率进行控制，使学习率缓慢的变化，而不是直接变为设定值。</p><p>下面我们通过一个具体的例子来展示Warmup：</p><p>设置初始化lr &#x3D; 0.1,假设所有步数num_step &#x3D; 20000,warmup_step &#x3D; 5000，即当步数达到5000时，达到所设置的学习率</p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">warmup_steps = <span class="number">5000</span></span><br><span class="line">init_lr = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 模拟训练20000步</span></span><br><span class="line">max_steps = <span class="number">20000</span></span><br><span class="line">a = []</span><br><span class="line"><span class="keyword">for</span> train_steps <span class="keyword">in</span> <span class="built_in">range</span>(max_steps):</span><br><span class="line">    <span class="keyword">if</span> warmup_steps <span class="keyword">and</span> train_steps &lt; warmup_steps:</span><br><span class="line">        warmup_percent_done = train_steps / warmup_steps</span><br><span class="line">        warmup_learning_rate = init_lr * warmup_percent_done  <span class="comment">#gradual warmup_lr</span></span><br><span class="line">        learning_rate = warmup_learning_rate</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#learning_rate = np.sin(learning_rate)  #预热学习率结束后,学习率呈sin衰减</span></span><br><span class="line">        learning_rate = learning_rate**<span class="number">1.0001</span> <span class="comment">#预热学习率结束后,学习率呈指数衰减(近似模拟指数衰减)</span></span><br><span class="line">    <span class="keyword">if</span> (train_steps+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">             <span class="built_in">print</span>(<span class="string">&quot;train_steps:%.3f--warmup_steps:%.3f--learning_rate:%.3f&quot;</span> % (</span><br><span class="line">                 train_steps+<span class="number">1</span>,warmup_steps,learning_rate))</span><br><span class="line">    a.append(learning_rate)</span><br><span class="line">plt.plot(a)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可得学习率变化如下：</p><p><img src="https://img-blog.csdnimg.cn/ddd9decebde845009dbf4aad2a84ac4a.png"></p><h3 id="为什么要进行预热学习率"><a href="#为什么要进行预热学习率" class="headerlink" title="为什么要进行预热学习率"></a>为什么要进行预热学习率</h3><p>由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Warmup可以逐渐地将学习率从一个小的值提升到一个大的值&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中的MASK理解</title>
    <link href="https://du2279664786.github.io/2022/11/15/2022-11-14%E5%AF%B9Transformer%E4%B8%AD%E7%9A%84MASK%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/11/15/2022-11-14%E5%AF%B9Transformer%E4%B8%AD%E7%9A%84MASK%E7%90%86%E8%A7%A3/</id>
    <published>2022-11-15T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.771Z</updated>
    
    <content type="html"><![CDATA[<p>Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分</p><span id="more"></span><p>上一篇文章我们介绍了<a href="https://blog.csdn.net/weixin_51756104/article/details/127250190?spm=1001.2014.3001.5501">对Transformer中FeedForward层的理解</a>，今天我们来介绍一下对MASK的理解<br>老规矩，还是先放一张Transformer的图片<br><img src="https://img-blog.csdnimg.cn/de74182b27f24a84a28fdd5f7204f0cd.png" alt="在这里插入图片描述"><br>Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，而所谓的MASK在Encoder和Decoder两个结构中都有使用。</p><p>Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分</p><h1 id="Padding-Masked"><a href="#Padding-Masked" class="headerlink" title="Padding Masked"></a>Padding Masked</h1><p>对于Transformer而言，每次的输入为：[batch_size,seq_length,d_module]结构，由于句子一般是长短不一的，而输入的数据需要是固定的格式，所以要对句子进行处理。<br>通常会把每个句子按照最大长度进行补齐，所以当句子不够长时，需要进行补0操作，以保证输入数据结构的完整性<br>但是在计算注意力机制时的Softmax函数时，就会出现问题，Padding数值为0的话，仍然会影响到Softmax的计算结果，即无效数据参加了运算。<br>为了不让Padding数据产生影响，通常会将Padding数据变为负无穷，这样的话就不会影响Softmax函数了</p><h1 id="Sequence-Masked"><a href="#Sequence-Masked" class="headerlink" title="Sequence Masked"></a>Sequence Masked</h1><p>Sequence Masked只发生在Decoder操作中，在Decoder中，我们的预测是一个一个进行的，即输入一个token，输出下一个token，在网上看到一个很好的解释如下：<br>假设我们当前在进行机器翻译<br>    输入：我很好<br>    输出：I am fine<br>接下来是Decoder执行步骤<br>第一步：<br>    ·初始输入： 起始符</s> + Positional Encoding（位置编码）<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“I”<br>第二步：<br>    ·初始输入：起始符</s> + “I”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“am”<br>第三步：<br>    ·初始输入：起始符</s> + “I”+ “am”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“fine”<br>上面就是Decoder的执行过程，在预测出“I”之前，我们是不可能知道‘am’的，所以要将数进行Mask，防止看到当前值后面的值，如下图所示：当我们仅知道start的时候，后面的关系是不知道的，所以start和“I”以及其它单词的Score都为负无穷，当预测出“I”之后，再去预测“am”，最终得到下面第三个得分矩阵。<br><img src="https://img-blog.csdnimg.cn/8df2f100c0a447909a8f9d99cbf86a1d.png"></p><p>最后经过Softmax处理之后，得到最终的得分矩阵<br><img src="https://img-blog.csdnimg.cn/90d840807bf04b89bf8e32dd6bf19e85.png"></p><p>最后不要忘了Decoder中依然采用的是Masked Multi-Head Attention，即多次进行Mask机制</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
    <category term="Mask" scheme="https://du2279664786.github.io/tags/Mask/"/>
    
  </entry>
  
  <entry>
    <title>第五届“泰迪杯”数据分析技能赛</title>
    <link href="https://du2279664786.github.io/2022/11/13/2022-11-13%E7%AC%AC%E4%BA%94%E5%B1%8A%E2%80%9C%E6%B3%B0%E8%BF%AA%E6%9D%AF%E2%80%9D%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD%E8%B5%9B/"/>
    <id>https://du2279664786.github.io/2022/11/13/2022-11-13%E7%AC%AC%E4%BA%94%E5%B1%8A%E2%80%9C%E6%B3%B0%E8%BF%AA%E6%9D%AF%E2%80%9D%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD%E8%B5%9B/</id>
    <published>2022-11-13T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.770Z</updated>
    
    <content type="html"><![CDATA[<p>第五届“泰迪杯”数据分析技能赛复盘</p><span id="more"></span><p>今天刚结束了第五届“泰迪杯”数据分析技能赛，我们选择的B题做，昨天的A题太伤心了，做不出来A题（时间太短，再加上太麻烦，晚上直接跑路去操场看70周年校庆了）</p><p>先上代码：<a href="https://github.com/du2279664786/2022-taidi-competition">https://github.com/du2279664786/2022-taidi-competition</a>，直接在前面仓库可以看到代码和相关数据</p><h1 id="竞赛日程；"><a href="#竞赛日程；" class="headerlink" title="竞赛日程；"></a>竞赛日程；</h1><p>报名起始时间：2022年9月5日-11月10日<br>赛前指导时间：2022年9月13日-11月10日<br>A题竞赛时间：2022年11月12日 8:00-21:00（8:00:00公布赛题）<br>B题竞赛时间：2022年11月13日 8:00-20:00（8:00:00公布赛题）<br>视频答辩时间：2022年12月上旬，具体时间另行通知<br>成绩公示时间：2022年12月中下旬<br>最终成绩公布时间：2022年12月中下旬</p><h1 id="赛题基本情况"><a href="#赛题基本情况" class="headerlink" title="赛题基本情况"></a>赛题基本情况</h1><p>A题基本如下：<br><img src="https://img-blog.csdnimg.cn/fc0e1942da0840e581af2f1a07f92294.png"><br>B题基本如下：<br><img src="https://img-blog.csdnimg.cn/60fe3adf19504c08858bb6e0335dae6c.png"><br><img src="https://img-blog.csdnimg.cn/0bd891ee6bc648db963bbb5be25ff832.png"><br><img src="https://img-blog.csdnimg.cn/32ce46c165fb4f959267a3ee388aa68e.png"><br><img src="https://img-blog.csdnimg.cn/528ac1e435114bd781e6ad1f5de35bb8.png"></p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>昨天做A题，今天做B题，每个题的做题时间是8:00-20:00（8:00:00公布赛题），共12个小时的竞赛时间，完成一篇论文</p><p>可以看出，A题和我们人工智能专业相关性不大，所以昨天我和两个队友基本出于摆烂状态，也没有提交论文，基本放弃了A题，而转身去看建校70周年的节目去了</p><p>然后今天早晨又看了B题，发现可以冲，于是我们三个就去201教室开干，一共写了20页，差不多从9点开干，一直到晚上8点，难度不大，但也需要一定的思考逻辑，模型部分如下图展示：<br><img src="https://img-blog.csdnimg.cn/492c0617f3ce49c68980a8c3d4e23b36.png"><br><img src="https://img-blog.csdnimg.cn/4b25af9f99db44c195bb9ce3f6079b28.png"><br><img src="https://img-blog.csdnimg.cn/b7c75b831b574d3fb7c78f100c022287.png"></p><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>做的还可以，基本都是按照题目要求做的，美中不足就是正第一页排版不大好，其他的都还可以，希望能够摸一个奖</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第五届“泰迪杯”数据分析技能赛复盘&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据挖掘" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数据分析" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>第三届“大湾区杯”粤港澳金融数学建模竞赛</title>
    <link href="https://du2279664786.github.io/2022/11/08/2022-11-08%E7%AC%AC%E4%B8%89%E5%B1%8A%E2%80%9C%E5%A4%A7%E6%B9%BE%E5%8C%BA%E6%9D%AF%E2%80%9D%E7%B2%A4%E6%B8%AF%E6%BE%B3%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B/"/>
    <id>https://du2279664786.github.io/2022/11/08/2022-11-08%E7%AC%AC%E4%B8%89%E5%B1%8A%E2%80%9C%E5%A4%A7%E6%B9%BE%E5%8C%BA%E6%9D%AF%E2%80%9D%E7%B2%A4%E6%B8%AF%E6%BE%B3%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B/</id>
    <published>2022-11-08T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.765Z</updated>
    
    <content type="html"><![CDATA[<p>2022年第三届“大湾区杯”粤港澳金融数学建模竞赛复盘</p><span id="more"></span><p>老规矩先上代码仓库：<a href="https://github.com/du2279664786/2022-JinRong-Mathematics-modeling">https://github.com/du2279664786/2022-JinRong-Mathematics-modeling</a></p><h1 id="竞赛日程"><a href="#竞赛日程" class="headerlink" title="竞赛日程"></a>竞赛日程</h1><p>报名时间：10月12日10:00:00-10月30日22:00:00<br>竞赛时间：11月1日10:00:00-11月8日14:00:00</p><h1 id="赛题基本情况"><a href="#赛题基本情况" class="headerlink" title="赛题基本情况"></a>赛题基本情况</h1><p>A题如下<br><img src="https://img-blog.csdnimg.cn/4d314192ae304e3ab220fddc93379418.png"><br>B题如下<br><img src="https://img-blog.csdnimg.cn/1c6b8f2fcc844fc891edbf488f9078df.png"><br>AB两个题都是比较偏向金融的（题目就叫‘金融’数学建模）</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>我们三个都是计算机专业的，对金融一窍不通，刚开始选择的是A题，因为我们感觉A比较偏代码，实践性更强一些，但是当我们做了两天之后发现：A题根本没有头绪，对他们所给的数据也是不会处理，11月3号，还是没有思路，于是我们果断换成了B题，容易水论文<br><img src="https://img-blog.csdnimg.cn/83574b6bd9624a229fb935074e4c808a.png"></p><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>整体还行，至少尽力做了，对金融领域不大了解，这也是我们的短处，希望能摸一个奖！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年第三届“大湾区杯”粤港澳金融数学建模竞赛复盘&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数学建模" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
    <category term="金融" scheme="https://du2279664786.github.io/tags/%E9%87%91%E8%9E%8D/"/>
    
  </entry>
  
  <entry>
    <title>中国大学生计算机设计大赛复盘</title>
    <link href="https://du2279664786.github.io/2022/10/27/2022-10-27%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/"/>
    <id>https://du2279664786.github.io/2022/10/27/2022-10-27%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/</id>
    <published>2022-10-27T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.764Z</updated>
    
    <content type="html"><![CDATA[<p>2022年中国大学生计算机设计大赛-人工智能挑战赛-国家二等奖</p><span id="more"></span><p>此篇文章写于2022年10月27日，为了复盘、回顾上一次的计算机设计大赛<br>中国大学生计算机设计大赛</p><h1 id="日程"><a href="#日程" class="headerlink" title="日程"></a>日程</h1><p>5-6月份开始初赛省赛，我和我的两位队友努力的写文档，整理代码，提交了相关资料，由于初赛（省赛）没有答辩，所以差不多等到六月多收到获得省二的通知<br>7月份多得之进了国赛<br>7月底-8月底就开始修改文档，修改代码，录制相关视频，等待国赛的答辩</p><h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><p>项目背景：在数字经济蓬勃发展的时代背景下， “数字化”“智慧化”成为了企业管理转型升级的核心引擎。传统建管理模式已不再符合可持续发展的要求，迫切需要利用以信息技术为代表的现代科技手段，实现中国企业管理转型升级与跨越式发展。为了更好的解决时代问题的痛点，并在一定程度上节约人力成本，本项目设计了一个基于云端和硬件的人工智能多场景实时物体监测平台，将多场景实时监测与报警系统进行了融合创新，做到了监控、检测、分类、识别四位一体的平台建设。<br>整体布局：<br><img src="https://img-blog.csdnimg.cn/d652a2b775ee475f951a36c928d2054e.png" alt="在这里插入图片描述"></p><p>算法介绍：<br>移动物体检测：本项目采用OpenCV（Open Source Computer Vision Library）算法，使用灰度转化、图片缩放和高斯滤波等相关操作，对图像进行预处理，增强了移动物体的可检测性。</p><p>物体识别：为提高精确度并且处理时间尽可能短，本项目采用了运算速度比较快的 Yolov3 算法，它是基于深度学习框架Darknet的目标检测开源项目，不仅可以充分发挥多核处理器和 GPU 并行运算的功能，还可以基于预训练模型进行实时目标检测,预期效果如下：<br><img src="https://img-blog.csdnimg.cn/d1d870e51929499fb0637ed6a3ba3703.png" alt="在这里插入图片描述"><br>以及安全帽检测、疲劳监测、口罩检测<br>具体代码仓库如下：<a href="https://github.com/du2279664786/Chinese-undergraduate-computer-design-contest">https://github.com/du2279664786/Chinese-undergraduate-computer-design-contest</a></p><p>最终获得证书：一个省二证书、一个国二证书、外加一枚金匾<br><img src="https://img-blog.csdnimg.cn/736f6b9f339841fc832f0180d06ab5d8.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/99c107c9a863454e8ee0d84169935b90.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年中国大学生计算机设计大赛-人工智能挑战赛-国家二等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="机器视觉" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>数学建模省一思路及其代码</title>
    <link href="https://du2279664786.github.io/2022/10/14/2022-10-14%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%9C%81%E4%B8%80%E6%80%9D%E8%B7%AF%E5%8F%8A%E5%85%B6%E4%BB%A3%E7%A0%81/"/>
    <id>https://du2279664786.github.io/2022/10/14/2022-10-14%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%9C%81%E4%B8%80%E6%80%9D%E8%B7%AF%E5%8F%8A%E5%85%B6%E4%BB%A3%E7%A0%81/</id>
    <published>2022-10-14T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.762Z</updated>
    
    <content type="html"><![CDATA[<p>2022年全国大学生数学建模竞赛山东省一等奖</p><span id="more"></span><p>连肝三天，撸了三天代码，9.17早-9.18晚日夜不停，终于和队友拿下了2022年全国大学生数学建模竞赛山东省一等奖<br>基本思路如下：<br><img src="https://img-blog.csdnimg.cn/c6128ba97db94cf489182b2fe05ee6ec.jpeg"><br>下面是代码部分，仓库连接如下：<a href="https://github.com/du2279664786/CUMCM">https://github.com/du2279664786/CUMCM</a><br><img src="https://img-blog.csdnimg.cn/1b6a9e5ed0054967b790696d20f287ce.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年全国大学生数学建模竞赛山东省一等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数学建模" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中FeedForward层的理解</title>
    <link href="https://du2279664786.github.io/2022/10/10/2022-10-10%E5%AF%B9Transformer%E4%B8%ADFeedForward%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/10/2022-10-10%E5%AF%B9Transformer%E4%B8%ADFeedForward%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-10T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.761Z</updated>
    
    <content type="html"><![CDATA[<p>在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强</p><span id="more"></span><p>上一篇我们介绍了<a href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/">对Add&amp;Norm层的理解</a>，有不大熟悉的可以看一下上篇文章。</p><p>今天来说一下Transformer中FeedForward层，首先还是先来回顾一下Transformer的基本结构：首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，然后又做了一个ADD&amp;Norm，再通过Feed Forward进行输出。<br><img src="https://img-blog.csdnimg.cn/4af25c021fd14b70aa2648a925fadf54.png"><br>FeedForward的输入是什么呢？是Multi-Head Attention的输出做了残差连接和Norm之后得数据，然后FeedForward做了两次线性线性变换，为的是更加深入的提取特征。<br><img src="https://img-blog.csdnimg.cn/b976d7add795475fac7bbc6c5f01121f.png" alt="在这里插入图片描述"><br>可以看出在每次线性变换都引入了非线性激活函数Relu，在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强，FeedForward的计算公式如下：max相当于Relu<br><img src="https://img-blog.csdnimg.cn/43b6886add22435795f3f8a7a889c58e.png" alt="在这里插入图片描述"></p><p>所以FeedForward的作用是：通过线性变换，先将数据映射到高纬度的空间再映射到低纬度的空间，提取了更深层次的特征</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Add&amp;Norm层的理解</title>
    <link href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&amp;Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&amp;Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-09T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.759Z</updated>
    
    <content type="html"><![CDATA[<p>无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><span id="more"></span><h1 id="Add操作"><a href="#Add操作" class="headerlink" title="Add操作"></a>Add操作</h1><p>首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，再通过Feed Forward进行输出。</p><p>由下图可以看出：在Encoder层和Decoder层中都用到了Add&amp;Norm操作，即残差连接和层归一化操作。<br><img src="https://img-blog.csdnimg.cn/5ef722ad3c5b407482ac132b0883c59c.png" alt="在这里插入图片描述"><br>什么是残差连接呢？残差连接就是把网络的输入和输出相加，即网络的输出为F(x)+x，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就变成了F(x)+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项‘1’，有效解决了梯度消失问题。</p><h1 id="Norm操作"><a href="#Norm操作" class="headerlink" title="Norm操作"></a>Norm操作</h1><p>首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇<a href="https://mp.weixin.qq.com/s/HNCl6MPS_hjTVHNt7UkYyw">文章</a>，才明白了Norm是什么。</p><p>假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]</span><br><span class="line">[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]]</span><br></pre></td></tr></table></figure><p>如果是在做BatchNorm（BN）的话，其计算过程如下：BN1&#x3D;(w11+w12+w13+w14+w41+<br>w42+w43+w44)&#x2F;8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean</p><p>如果是在做LayerNorm（LN）的话，则会进如下计算：LN1&#x3D;(w11+w12+w13+w14+w21+<br>w22+w23+w24+w31+w32+w33+w34)&#x2F;12，同理会得到LN2，最终得到[LN1,LN2]两个mean</p><p>如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1&#x3D;(w11+w12+w13+w14)&#x2F;4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]<br>下图完美的揭示了，这几种Norm<br><img src="https://img-blog.csdnimg.cn/a143d6b41e654fa1849f44580401110c.png" alt="在这里插入图片描述"><br>接下来我们来看一下Transformer中的Norm：首先生成[2,3,4]形状的数据，使用原始的编码方式进行编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> InstanceNorm2d</span><br><span class="line">random_seed = <span class="number">123</span></span><br><span class="line">torch.manual_seed(random_seed)</span><br><span class="line"></span><br><span class="line">batch_size, seq_size, dim = <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">embedding = torch.randn(batch_size, seq_size, dim)</span><br><span class="line"></span><br><span class="line">layer_norm = torch.nn.LayerNorm(dim, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y: &quot;</span>, layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y:  tensor([[[ <span class="number">1.5524</span>,  <span class="number">0.0155</span>, -<span class="number">0.3596</span>, -<span class="number">1.2083</span>],</span><br><span class="line">         [ <span class="number">0.5851</span>,  <span class="number">1.3263</span>, -<span class="number">0.7660</span>, -<span class="number">1.1453</span>],</span><br><span class="line">         [ <span class="number">0.2864</span>,  <span class="number">0.0185</span>,  <span class="number">1.2388</span>, -<span class="number">1.5437</span>]],</span><br><span class="line">        [[ <span class="number">1.1119</span>, -<span class="number">0.3988</span>,  <span class="number">0.7275</span>, -<span class="number">1.4406</span>],</span><br><span class="line">         [-<span class="number">0.4144</span>, -<span class="number">1.1914</span>,  <span class="number">0.0548</span>,  <span class="number">1.5510</span>],</span><br><span class="line">         [ <span class="number">0.3914</span>, -<span class="number">0.5591</span>,  <span class="number">1.4105</span>, -<span class="number">1.2428</span>]]])</span><br></pre></td></tr></table></figure><p>接下来手动去进行一下编码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以发现和LayerNorm的结果是一样的，也就是说明Norm是对d_model进行的Norm，会给我们[batch,sqe_length]形状的平均值。<br>加下来进行batch_norm,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_norm = torch.nn.LayerNorm([seq_size,dim], elementwise_affine = <span class="literal">False</span>)</span><br><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1822</span>,  <span class="number">0.4419</span>, -<span class="number">0.3196</span>, -<span class="number">1.9889</span>],</span><br><span class="line">         [-<span class="number">0.6677</span>, -<span class="number">0.2537</span>, -<span class="number">0.8151</span>,  <span class="number">1.5143</span>],</span><br><span class="line">         [ <span class="number">0.7174</span>,  <span class="number">1.2147</span>, -<span class="number">0.0852</span>, -<span class="number">0.9403</span>]],</span><br><span class="line">        [[-<span class="number">0.0138</span>,  <span class="number">1.5666</span>, -<span class="number">2.1726</span>,  <span class="number">1.0590</span>],</span><br><span class="line">         [ <span class="number">0.6646</span>,  <span class="number">0.6852</span>, -<span class="number">0.8706</span>, -<span class="number">0.0442</span>],</span><br><span class="line">         [-<span class="number">0.1163</span>,  <span class="number">0.1389</span>,  <span class="number">0.4454</span>, -<span class="number">1.3423</span>]]])</span><br></pre></td></tr></table></figure><p>可以看到BN的计算的mean形状为[2, 1, 1]，并且Norm结果也和上面的两个不一样，这就充分说明了Norm是在对最后一个维度求平均。<br>那么什么又是Instancenorm呢？接下来再来实现一下instancenorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instance_norm = InstanceNorm2d(<span class="number">3</span>, affine=<span class="literal">False</span>)</span><br><span class="line">output = instance_norm(embedding.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)) <span class="comment">#InstanceNorm2D需要(N,C,H,W)的shape作为输入</span></span><br><span class="line">layer_norm = torch.nn.LayerNorm(<span class="number">4</span>, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br></pre></td></tr></table></figure><p>可以看出无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><p>如果喜欢文章请点个赞，笔者也是一个刚入门Transformer的小白，一起学习，共同努力。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
    <category term="Add&amp;Norm" scheme="https://du2279664786.github.io/tags/Add-Norm/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中self-attention的理解</title>
    <link href="https://du2279664786.github.io/2022/10/08/2022-10-08%E5%AF%B9Transformer%E4%B8%ADself-attention%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/08/2022-10-08%E5%AF%B9Transformer%E4%B8%ADself-attention%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-08T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.755Z</updated>
    
    <content type="html"><![CDATA[<p>Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差</p><span id="more"></span><h1 id="什么是self-attention"><a href="#什么是self-attention" class="headerlink" title="什么是self-attention"></a>什么是self-attention</h1><p>首先我们来看一下Transformer架构：对于input输出，首先进行input embedding，然后再进行positional encoding，将两者相加作为Encoder的输入，也就是输如X<img src="https://img-blog.csdnimg.cn/65ef7c5730514e2cac88d332e2588423.png" alt="在这里插入图片描述"><br>何为self-attention？首先我们要明白什么是attention，对于传统的seq2seq任务，例如中-英文翻译，输入中文，得到英文，即source是中文句子（x1 x2 x3）,英文句子是target（y1 y2 y3）<br><img src="https://img-blog.csdnimg.cn/296c379fd77c475ebf77c06fa8e42e59.png" alt="在这里插入图片描述"><br>attention机制发生在target的元素和source中的所有元素之间。简单的将就是attention机制中的权重计算需要target参与，即在上述Encoder-Decoder模型中，Encoder和Decoder两部分都需要参与运算。</p><p>而对于self-attention，它不需要Decoder的参与，而是source内部元素之间发生的运算，对于输入向量X，对其做线性变换，分别得到Q、K、V矩阵<br><img src="https://img-blog.csdnimg.cn/17358bbb1cf641d78117625fb5a00d31.png" alt="在这里插入图片描述"><br>然后去计算attention，Q、K点乘得到初步的权重因子，并对Q、K点乘结果进行放缩，除以sqrt（dk），Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差，最终再加一个softmax就得到了self attention的输出。<br><img src="https://img-blog.csdnimg.cn/07fdbd802d114b7193c70e9fc451ba22.png" alt="在这里插入图片描述"></p><h1 id="Multi–head-attention"><a href="#Multi–head-attention" class="headerlink" title="Multi–head-attention"></a>Multi–head-attention</h1><p>Multi–head-attention使用了多个头进行运算，捕捉到了更多的信息，多头的数量用h表示，一般h&#x3D;8，表示8个头<br><img src="https://img-blog.csdnimg.cn/7cad1e37120b4d40ad64140677452ec1.png" alt="在这里插入图片描述"><br>在输入每个self-attention之前，我们需将输入X均分的分到h个头中，得到Z1-Z7八个头的输出结果。<br><img src="https://img-blog.csdnimg.cn/d5763f726a5c48f292a140f84c0e5200.png" alt="在这里插入图片描述"><br>对于每个头计算相应的attention score，将其进行拼接，再与W0进行一个线性变换，就得到最终输出的Z。<br><img src="https://img-blog.csdnimg.cn/05bfb9eb87bb4f3b8066c8190c0dff0f.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Positional Encoding的理解</title>
    <link href="https://du2279664786.github.io/2022/10/07/2022-10-07%E5%AF%B9Transformer%E4%B8%ADPositional%20Encoding%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/07/2022-10-07%E5%AF%B9Transformer%E4%B8%ADPositional%20Encoding%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-07T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.757Z</updated>
    
    <content type="html"><![CDATA[<p>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值</p><span id="more"></span><p>首先来看一下Transformer结构的结构：<br><img src="https://img-blog.csdnimg.cn/c7ce14349b734567a014c4cf679398fe.png" alt="在这里插入图片描述"><br>Transformer是由Encoder和Decoder两大部分组成，首先对于文本特征，需要进行Embedding，由于transformer抛弃了Rnn的结构，不能捕捉到序列的信息，交换单词位置，得到相应的attention也会发生交换，并不会发生数值上的改变，所以要对input进行Positional Encoding。</p><p>Positional encoding和input embedding是同等维度的，所以可以将两者进行相加，的到输入向量<br><img src="https://img-blog.csdnimg.cn/be30b27838dd411c89d793432ff72582.png" alt="在这里插入图片描述"><br>接下来看一些Positional Encoding的计算公式：<br><img src="https://img-blog.csdnimg.cn/6e9a80e756b94a70aeef8a79097eb7a6.png" alt="在这里插入图片描述"><br>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值，也就是说：对于单个token的d_model维度的词向量，奇数位置取cos，偶数位置取sin，最终的到一个维度和word embedding维度一样的矩阵，接下来可以看一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_positional_encoding</span>(<span class="params">max_seq_len, embed_dim</span>):</span><br><span class="line">    <span class="comment"># 初始化一个positional encoding</span></span><br><span class="line">    <span class="comment"># embed_dim: 字嵌入的维度</span></span><br><span class="line">    <span class="comment"># max_seq_len: 最大的序列长度</span></span><br><span class="line">    positional_encoding = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * i / embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embed_dim)] <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(embed_dim) <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len)])</span><br><span class="line"></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i 偶数</span></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1 奇数</span></span><br><span class="line">    <span class="keyword">return</span> positional_encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">positional_encoding = get_positional_encoding(max_seq_len=<span class="number">100</span>, embed_dim=<span class="number">16</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">sns.heatmap(positional_encoding)</span><br><span class="line">plt.title(<span class="string">&quot;Sinusoidal Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;hidden dimension&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;sequence length&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>首先求初始向量：positional_encoding，然后对其奇数列求sin，偶数列求cos：<br><img src="https://img-blog.csdnimg.cn/2da3445d7953426394ab2e46d8820baf.png" alt="在这里插入图片描述"><br>最终得到positional encoding之后的数据可视化：<br><img src="https://img-blog.csdnimg.cn/507cc7ca0ba34f8fb2ec87216689857c.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Positional Encoding" scheme="https://du2279664786.github.io/tags/Positional-Encoding/"/>
    
    <category term="Transformer" scheme="https://du2279664786.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>怎么理解预训练模型？</title>
    <link href="https://du2279664786.github.io/2022/10/06/2022-10-06%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F/"/>
    <id>https://du2279664786.github.io/2022/10/06/2022-10-06%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F/</id>
    <published>2022-10-06T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.752Z</updated>
    
    <content type="html"><![CDATA[<p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性……</p><span id="more"></span><h1 id="什么是预训练"><a href="#什么是预训练" class="headerlink" title="什么是预训练"></a>什么是预训练</h1><p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性，然后将其中的共性“移植”到特定任务的模型中，再使用相关特定领域的少量标注数据进行“微调”，这样的话，模型只需要从”共性“出发，去“学习”该特定任务的“特殊”部分即可。</p><h1 id="预训练的思想"><a href="#预训练的思想" class="headerlink" title="预训练的思想"></a>预训练的思想</h1><p>预训练的思想是：模型的参数不再是随机初始化的，而是通过一些任务进行预先训练，得到一套模型参数，然后用这套参数对模型进行初始化，再进行训练</p><h1 id="CV领域的预训练"><a href="#CV领域的预训练" class="headerlink" title="CV领域的预训练"></a>CV领域的预训练</h1><p>首先对于CV领域图片分类任务，常用的深度学习模型是卷积视神经网络，对于多层的卷积神经网络来说，不同的层学到的特征是不同的，为了捕获更多的特征，浅层的感受野较小，所以浅层学到的特征往往是更加通用的，包含更多的像素点的信息，比如一些细粒度的信息：颜色、纹理、边缘等。<br>通常在大规模图片数据上预先获得‘通用特征’，然后再去做下游任务：<br><img src="https://img-blog.csdnimg.cn/455a209bb08941ff8d390ede716556b4.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性……&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="预训练" scheme="https://du2279664786.github.io/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>HuggingFace的安装和编码</title>
    <link href="https://du2279664786.github.io/2022/10/05/2022-10-05HuggingFace%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A0%81/"/>
    <id>https://du2279664786.github.io/2022/10/05/2022-10-05HuggingFace%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A0%81/</id>
    <published>2022-10-05T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.753Z</updated>
    
    <content type="html"><![CDATA[<p>模型的加载和编码以及基本的使用功能</p><span id="more"></span><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>使用以下命令安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></table></figure><h1 id="模型的加载"><a href="#模型的加载" class="headerlink" title="模型的加载"></a>模型的加载</h1><p>导入包：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from transformers import BertTokenizer</span><br></pre></td></tr></table></figure><p>加载预训练模型bert-base-chinese，初次加载可能需要较长的时间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#加载预训练字典和分词方法</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(</span><br><span class="line"># 下载或者从本地加载模型</span><br><span class="line">    pretrained_model_name_or_path=&#x27;bert-base-chinese&#x27;,</span><br><span class="line">    force_download=False,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>接下来就可以看到tokenizer的内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br><span class="line"></span><br><span class="line">#print:(name_or_path=&#x27;bert-base-chinese&#x27;, vocab_size=21128, model_max_len=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;unk_token&#x27;: &#x27;[UNK]&#x27;, &#x27;sep_token&#x27;: &#x27;[SEP]&#x27;, &#x27;pad_token&#x27;: &#x27;[PAD]&#x27;, &#x27;cls_token&#x27;: &#x27;[CLS]&#x27;, &#x27;mask_token&#x27;: &#x27;[MASK]&#x27;&#125;)</span><br></pre></td></tr></table></figure><h1 id="进行编码"><a href="#进行编码" class="headerlink" title="进行编码"></a>进行编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sents = [</span><br><span class="line">    &#x27;我在一所学院上大三。&#x27;,</span><br><span class="line">    &#x27;今天天气好，我来图书馆学习。&#x27;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">res= tokenizer.encode(</span><br><span class="line">    # 可以一次编码两个句子，即text和text_pair</span><br><span class="line">    text = sents[0],</span><br><span class="line">    text_pair=sents[1],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    return_tensors=None,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以打印出res的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 101为CLS    102为SEP    0为PAD</span><br><span class="line">#print(res):[101, 2769, 1762, 671, 2792, 2110, 7368, 677, 1920, 676, 511, 102, 791, 1921, 1921, 3698, 1962, 8024, 2769, 3341, 1745, 741, 7667, 2110, 739, 511, 102, 0, 0, 0]</span><br></pre></td></tr></table></figure><p>也可以查看编码之后的结果:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decode(res)</span><br><span class="line"># [CLS] 我 在 一 所 学 院 上 大 三 。 [SEP] 今 天 天 气 好 ， 我 来 图 书 馆 学 习 。 [SEP] [PAD] [PAD] [PAD]</span><br></pre></td></tr></table></figure><h1 id="多功能编码"><a href="#多功能编码" class="headerlink" title="多功能编码"></a>多功能编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">res = tokenizer.encode_plus(</span><br><span class="line">    # 可以一次编码两个句子</span><br><span class="line">    text = sents[0],</span><br><span class="line">    text_pair=sents[1],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    </span><br><span class="line">    # 返回值的类型：tf  pt  np</span><br><span class="line">    return_tensors=None,</span><br><span class="line">    </span><br><span class="line">    # 返回token_type_ids</span><br><span class="line">    return_token_type_ids=True,</span><br><span class="line">    </span><br><span class="line">    # 返回attention_mask</span><br><span class="line">    return_attention_mask = True,</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # 返回length标识长度</span><br><span class="line">    return_length = True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看编码的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for k,v in res.items():</span><br><span class="line">    print(k,&#x27;:&#x27;,v)</span><br><span class="line">    </span><br><span class="line">tokenizer.decode(res[&#x27;input_ids&#x27;])</span><br><span class="line"></span><br><span class="line"># input_ids:编码后的句子</span><br><span class="line"># token_type_ids:第一个句子和特殊符号的位置是0，第二个句子的位置是1</span><br><span class="line"># attention_mask:pad的位置是0，其它位置是1</span><br><span class="line"># length:返回句子的长度</span><br></pre></td></tr></table></figure><h1 id="将句子批量进行编码"><a href="#将句子批量进行编码" class="headerlink" title="将句子批量进行编码"></a>将句子批量进行编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">out = tokenizer.batch_encode_plus(</span><br><span class="line">    batch_text_or_text_pairs=[sents[0],sents[1]],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    </span><br><span class="line">    # 返回值的类型：tf  pt  np</span><br><span class="line">    return_tensors=None,</span><br><span class="line">    </span><br><span class="line">    # 返回token_type_ids</span><br><span class="line">    return_token_type_ids=True,</span><br><span class="line">    </span><br><span class="line">    # 返回attention_mask</span><br><span class="line">    return_attention_mask = True,</span><br><span class="line">    </span><br><span class="line">    # 返回length标识长度</span><br><span class="line">    return_length = True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;模型的加载和编码以及基本的使用功能&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>深度学习课程回顾</title>
    <link href="https://du2279664786.github.io/2022/07/10/2022-07-10%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/"/>
    <id>https://du2279664786.github.io/2022/07/10/2022-07-10%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/</id>
    <published>2022-07-10T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.750Z</updated>
    
    <content type="html"><![CDATA[<p>大二下学期深度学习课程回顾</p><span id="more"></span><h1 id="第1章：深度学习概述"><a href="#第1章：深度学习概述" class="headerlink" title="第1章：深度学习概述"></a>第1章：深度学习概述</h1><p>BP神经网络的反向传播（《深度学习08_小复习.pptx》，链式法则：损失函数，激活函数、信号强度；每多一层多一个sigma符号：也是网络不能太深的原因之一）<br>    参数确定（权值个数和偏置个数）</p><h1 id="第2章：Pytorch简介"><a href="#第2章：Pytorch简介" class="headerlink" title="第2章：Pytorch简介"></a>第2章：Pytorch简介</h1><p>Pytorch安装要点：装python、装CUDA、装gpu版的torch<br>Linux编译安装Python的主要步骤：下载、解压、配置(.&#x2F;configure)、编译安装(make &amp;&amp; make install)<br>Tensor的基本知识：数据方面就是numpy，计算图方面是张量的本质。</p><h1 id="第3章：Pytorch计算图"><a href="#第3章：Pytorch计算图" class="headerlink" title="第3章：Pytorch计算图"></a>第3章：Pytorch计算图</h1><p>《深度学习08_小复习.pptx》<br>给几个公式，可以画出计算图，给计算图，可以复原出公式。<br>给计算图，可以向上算数据，可以算回传梯度<br>Pytorch的计算图结构：1、是否所有的叶子节点都可以设置为需要计算梯度，是否所有的叶子节点都需要计算梯度。2、是否默认所有的计算图都可以回传多次。3、计算图是否一个有向无环图。4、是否默认可以让中间节点保存梯度。5、非叶子节点的导函数是怎么来的？</p><h1 id="第4章：线性回归"><a href="#第4章：线性回归" class="headerlink" title="第4章：线性回归"></a>第4章：线性回归</h1><p>分类与回归，线性回归的基本概念，可用的解法。<br>小批量随机梯度下降法。<br>看网络写方程，看方程画网络。<br>数据加载、损失函数。</p><h1 id="第5章：Softmax回归"><a href="#第5章：Softmax回归" class="headerlink" title="第5章：Softmax回归"></a>第5章：Softmax回归</h1><p>Softmax和交叉熵，代码、计算、业务场景，<br>权值、偏置初始化<br>view()的业务意义</p><h1 id="第6章：多层感知机"><a href="#第6章：多层感知机" class="headerlink" title="第6章：多层感知机"></a>第6章：多层感知机</h1><p>单层和多层的区别（非线性激活函数的意义）<br>权值、偏置初始化，MLP的完备性<br>Relu激活函数</p><h1 id="第7章：模型训练与深度学习计算"><a href="#第7章：模型训练与深度学习计算" class="headerlink" title="第7章：模型训练与深度学习计算"></a>第7章：模型训练与深度学习计算</h1><p>为什么网络不能过深<br>Wd的原因和表现<br>Dropout的原理和实现<br>模型的读写<br>GPU计算的特点和历史<br>常用的框架代码：比如参数初始化、损失函数定义、优化器定义</p><h1 id="第8章：卷积"><a href="#第8章：卷积" class="headerlink" title="第8章：卷积"></a>第8章：卷积</h1><p>卷积的思想基础，能够解释局部性和平移不变性<br>对于某个像素的信息，应当只与其附近的像素有关系，超出一定的距离以后则无关<br>对某个区域进行的特征提取所得到的输出并不会由于平移操作而发生变化<br>卷积、池化的代码、计算<br>步幅、填充、多通道<br>    卷积和池化是否可以用相同的步幅、填充、输入通道、输出通道<br>1*1卷积</p><h1 id="第9章：机器视觉初步"><a href="#第9章：机器视觉初步" class="headerlink" title="第9章：机器视觉初步"></a>第9章：机器视觉初步</h1><p>LeNet、AlexNet、Vgg、NiN、GoogLeNet、ResNet<br>LeNet、典型的VGG块、inception块、Res块<br>过拟与DA，常用的DA技术<br>每个网络的名字的意义，历史脉络、网络深了是更容易过拟还是会减轻过拟。</p><h1 id="第10章：循环神经网络"><a href="#第10章：循环神经网络" class="headerlink" title="第10章：循环神经网络"></a>第10章：循环神经网络</h1><p>N元语法、词向量，两种常用的向量训练方法。<br>二阶马尔可夫展开<br>RNN、LSTM、GRU</p><h1 id="论述："><a href="#论述：" class="headerlink" title="论述："></a>论述：</h1><p>计算图、框架的作用<br>深度学习、卷积神经网络：简史、意义、动力<br>深度学习与机器学习的联系与区别<br>机器学习的核心理念<br>三个以上的观点，可以是自己的合理的观点，言之有物，自圆其说，字数达标，每个关键观点可酌情给予3-4分。<br>纯网络、教材内容不会得到高分。<br>如果有雷同答案，将判为抄袭，本题不得分，不区分抄袭与被抄袭者，欢迎你和同学讨论，但不要将你的完成文稿给别人，如果你已经给了别人，建议你自己再写一份。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大二下学期深度学习课程回顾&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="总结复盘" scheme="https://du2279664786.github.io/tags/%E6%80%BB%E7%BB%93%E5%A4%8D%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>机器学习课程回顾</title>
    <link href="https://du2279664786.github.io/2022/07/08/2022-07-08%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/"/>
    <id>https://du2279664786.github.io/2022/07/08/2022-07-08%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/</id>
    <published>2022-07-08T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.748Z</updated>
    
    <content type="html"><![CDATA[<p>大二上学期机器学习课程回顾</p><span id="more"></span><h1 id="1-基本概念部分"><a href="#1-基本概念部分" class="headerlink" title="1.基本概念部分"></a>1.基本概念部分</h1><p>1、统计学习方法可以概括如下：……..<br>2、什么是有监督学习、无监督学习、半监督学习<br>3、有监督的学习的三要素两过程<br>4、生成式模型和判别式模型是什么意思，常见的代表模型有哪几个<br>5、什么叫过拟合、欠拟合，常用的减轻拟合的方法<br>6、如果clf是一个模拟的对象，则一般clf.train(X, y), clf.fit(X, y), clf.predict(test)是什么意思，执行后的结果或改变是什么<br>7、Precision, Recall, F1, Accuracy, AUC of ROC。上面这几个概念的定义、意义、计算。给定正负例的信号强度，能画出ROC<br>8、训练集、验证集、测试集的作用是什么，S折交叉验证是怎么回事<br>9、什么叫回归，什么叫聚类，什么叫分类</p><h1 id="2-Knn"><a href="#2-Knn" class="headerlink" title="2.Knn"></a>2.Knn</h1><p>中英文名字、算法理念、算法过程、算法伪代码，算法代码实现</p><h1 id="3-Kmeans"><a href="#3-Kmeans" class="headerlink" title="3.Kmeans"></a>3.Kmeans</h1><p>中英文名字、算法理念、算法过程、算法伪代码，算法代码实现</p><h1 id="4-最优化问题"><a href="#4-最优化问题" class="headerlink" title="4.最优化问题"></a>4.最优化问题</h1><p>最优化问题，迭代最优化问题，梯度下降法都是什么意思。<br>梯度下降法的算法理念、算法过程、算法伪代码、停机条件<br>给定函数、当前自变量、学习率，可算出下一次迭代的自变量<br>给定函数，能求出argmin和min</p><h1 id="5-感知机"><a href="#5-感知机" class="headerlink" title="5.感知机"></a>5.感知机</h1><p>算法理念、算法过程、算法伪代码，算法代码实现<br>感知机解的情况和业务意义，感知机的局限，感知机在机器学习中的地位</p><h1 id="6-线性回归"><a href="#6-线性回归" class="headerlink" title="6.线性回归"></a>6.线性回归</h1><p>线性回归的定义，解法，解的情况，广义线性回归<br>会手算简单的线性回归(单变量)</p><h1 id="7-逻辑回归"><a href="#7-逻辑回归" class="headerlink" title="7.逻辑回归"></a>7.逻辑回归</h1><p>线性回归的定义，解法，解的情况<br>Sigmoid函数及求导，求解最大似然估计</p><h1 id="8-朴素贝叶斯"><a href="#8-朴素贝叶斯" class="headerlink" title="8.朴素贝叶斯"></a>8.朴素贝叶斯</h1><p>给定一个小规模数据集，可以手算朴素贝叶斯</p><h1 id="9-决策树"><a href="#9-决策树" class="headerlink" title="9.决策树"></a>9.决策树</h1><p>决策树的基本算法<br>熵、基尼、熵增益、固有值、熵增益比的定义和业务意义<br>ID3、C4.5、Cart算法基本思路和伪代码</p><h1 id="10-提升方法"><a href="#10-提升方法" class="headerlink" title="10.提升方法"></a>10.提升方法</h1><p>Bagging和随机森林<br>能说清楚GBDT的脉络即：<br>adaboost的理念，加法模型，前向加法模型，提升树，回归树对残差的拟合，以及对梯度的拟合。</p><h1 id="11-SVM"><a href="#11-SVM" class="headerlink" title="11.SVM"></a>11.SVM</h1><p>线性可分支持向量机的基本脉络<br>松弛变量、核函数的业务背景和操作方法<br>SMO算法的大致过程</p><h1 id="12-NN"><a href="#12-NN" class="headerlink" title="12.NN"></a>12.NN</h1><p>说清楚神经网络学习和预测的过程<br>了解常见的神经网络，及中英文名称<br>对于多层神经网络，可以计算其待定参数的个数，并能说明BP算法如何更新网络参数</p><h1 id="13-Numpy"><a href="#13-Numpy" class="headerlink" title="13.Numpy"></a>13.Numpy</h1><p>基本的向量化运算，使用numpy常见的方法</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大二上学期机器学习课程回顾&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="总结复盘" scheme="https://du2279664786.github.io/tags/%E6%80%BB%E7%BB%93%E5%A4%8D%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>Python切换源，快速下载文件</title>
    <link href="https://du2279664786.github.io/2022/06/05/2022-06-05Python%E5%88%87%E6%8D%A2%E6%BA%90%EF%BC%8C%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/"/>
    <id>https://du2279664786.github.io/2022/06/05/2022-06-05Python%E5%88%87%E6%8D%A2%E6%BA%90%EF%BC%8C%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/</id>
    <published>2022-06-05T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.745Z</updated>
    
    <content type="html"><![CDATA[<p>快速下载文件</p><span id="more"></span><h1 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h1><p>进入电脑路径’C:\Users\user_name\AppData\Roaming\pip’文件夹下，找到pip.ini文件<br><img src="https://img-blog.csdnimg.cn/d02a90ee68bd40248c2b39ea89154681.png" alt="在这里插入图片描述"><br>若没有此文件则可以创建一个pip文件，后缀为ini，，然后打开文件，将文件内容替换为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">global</span>]</span><br><span class="line"></span><br><span class="line">index-url=http://pypi.douban.com/simple</span><br><span class="line"></span><br><span class="line">[install]</span><br><span class="line"></span><br><span class="line">trusted-host=pypi.douban.com</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>保存文件即可<br><img src="https://img-blog.csdnimg.cn/435b149734414ae49fd8cb8cfbe6f124.png" alt="在这里插入图片描述"><br>下载速度惊人<br><img src="https://img-blog.csdnimg.cn/e904985a074142faaab9e22a7e273cc5.png" alt="在这里插入图片描述"><br>如果文章对您有帮助，请点赞+收藏</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;快速下载文件&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SOFTMAX回归模型</title>
    <link href="https://du2279664786.github.io/2022/05/08/2022-05-08SOFTMAX%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>https://du2279664786.github.io/2022/05/08/2022-05-08SOFTMAX%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-05-08T14:55:10.000Z</published>
    <updated>2022-11-24T06:54:45.743Z</updated>
    
    <content type="html"><![CDATA[<p>SOFTMAX函数的脉络梳理</p><span id="more"></span><p>2022-05-08SOFTMAX回归模型</p><h2 id="什么是SOFTMAX回归函数"><a href="#什么是SOFTMAX回归函数" class="headerlink" title="什么是SOFTMAX回归函数"></a>什么是SOFTMAX回归函数</h2><p>·softmax回归跟线性回归⼀样将输⼊特征与权᯿做线性叠加<br>·与线性回归的⼀个主要不同在于，softmax回归的输出值个数等于标签⾥的类别数<br>·SOFTMAX是一个单层的神经网络<br>结构图如下：<br><img src="https://img-blog.csdnimg.cn/1eef7cdde9b04e1d866471e374b3a80b.png" alt="在这里插入图片描述"><br>运算过程如下：<br><img src="https://img-blog.csdnimg.cn/4e928316894e4e298b9e71c18598fbd1.png" alt="在这里插入图片描述"><br>即我们通过神经网络预测，然后得到相应的一个分数，此时我们希望我们得到的分数是一个概率：<br><img src="https://img-blog.csdnimg.cn/c16934676c254f119005ddefbb0d5164.png" alt="在这里插入图片描述"><br>此时我们就应该选取一个合适的方案，来将预测的分数来转化为标签的概率，那么最合适的肯定是softmax了，softmax公式：<br><img src="https://img-blog.csdnimg.cn/df4b36ec76f44413b348ad2917c4d399.png" alt="在这里插入图片描述"><br>即将得到的分数都进行exp，然后求每个标签占比(概率)，最终我们得到的是预测概率最大的标签：<br><img src="https://img-blog.csdnimg.cn/656cab1ab05b4020ac06c14f0ea28951.png" alt="在这里插入图片描述"><br>很显然，我们只关注预测概率最大的标签（单标签预测）<br>总结了一下特性：<br>    ·结果都为正数：即将得分为负数的进行转化<br>    ·所有求和为1：所有概率相加等于1<br>    ·平移不变性：所有得分平移得到的结果不受影响<br>    ·最大–&gt;最大：预测得分最大的概率也大</p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>在这里我们使用的是交叉熵损失函数（Cross Entropy）<br><img src="https://img-blog.csdnimg.cn/eded5e651bfd471dada59239dc2b6c9d.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/cf23b1b754ab4f3eae57f49f5f12b0f1.png" alt="其中带下标的是向量 中⾮0即1的元素"></p><h2 id="实现代码："><a href="#实现代码：" class="headerlink" title="实现代码："></a>实现代码：</h2><p>·导入包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>若出现没有‘d2lzh_pytorch’这个包，<a href="https://blog.csdn.net/weixin_51756104/article/details/124626354?spm=1001.2014.3001.5501">点击此处离线安装</a><br>·导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>·定义网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, 1, 28, 28)</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"> </span><br><span class="line">net = LinearNet(num_inputs,num_outputs)</span><br></pre></td></tr></table></figure><p>·初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>·定义损失函数和优化器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>·训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;SOFTMAX函数的脉络梳理&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="SOFTMAX" scheme="https://du2279664786.github.io/tags/SOFTMAX/"/>
    
  </entry>
  
</feed>
