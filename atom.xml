<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>江东的笔记</title>
  
  <subtitle>Be overcome difficulties is victory</subtitle>
  <link href="https://du2279664786.github.io/atom.xml" rel="self"/>
  
  <link href="https://du2279664786.github.io/"/>
  <updated>2022-10-16T13:51:48.053Z</updated>
  <id>https://du2279664786.github.io/</id>
  
  <author>
    <name>江东</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="https://du2279664786.github.io/2022/10/15/hello-world/"/>
    <id>https://du2279664786.github.io/2022/10/15/hello-world/</id>
    <published>2022-10-15T03:00:57.930Z</published>
    <updated>2022-10-16T13:51:48.053Z</updated>
    
    <content type="html"><![CDATA[<p>大家好，这是我的第一篇文章，欢迎查看！</p><span id="more"></span><div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["青青陵上柏，磊磊涧中石。", "人生天地间，忽如远行客。","斗酒相娱乐，聊厚不为薄。", "驱车策驽马，游戏宛与洛。","洛中何郁郁，冠带自相索。","长衢罗夹巷，王侯多第宅。","两宫遥相望，双阙百余尺。","极宴娱心意，戚戚何所迫？"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大家好，这是我的第一篇文章，欢迎查看！&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>对Transformer中FeedForward层的理解</title>
    <link href="https://du2279664786.github.io/2022/10/10/2022-10-10%E5%AF%B9Transformer%E4%B8%ADFeedForward%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/10/2022-10-10%E5%AF%B9Transformer%E4%B8%ADFeedForward%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-10T14:55:10.000Z</published>
    <updated>2022-10-19T03:19:02.492Z</updated>
    
    <content type="html"><![CDATA[<p>在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强</p><span id="more"></span><p>上一篇我们介绍了<a href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/">对Add&amp;Norm层的理解</a>，有不大熟悉的可以看一下上篇文章。</p><p>今天来说一下Transformer中FeedForward层，首先还是先来回顾一下Transformer的基本结构：首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，然后又做了一个ADD&amp;Norm，再通过Feed Forward进行输出。<br><img src="https://img-blog.csdnimg.cn/4af25c021fd14b70aa2648a925fadf54.png" alt="在这里插入图片描述"><br>FeedForward的输入是什么呢？是Multi-Head Attention的输出做了残差连接和Norm之后得数据，然后FeedForward做了两次线性线性变换，为的是更加深入的提取特征。<br><img src="https://img-blog.csdnimg.cn/b976d7add795475fac7bbc6c5f01121f.png" alt="在这里插入图片描述"><br>可以看出在每次线性变换都引入了非线性激活函数Relu，在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强，FeedForward的计算公式如下：max相当于Relu<br><img src="https://img-blog.csdnimg.cn/43b6886add22435795f3f8a7a889c58e.png" alt="在这里插入图片描述"></p><p>所以FeedForward的作用是：通过线性变换，先将数据映射到高纬度的空间再映射到低纬度的空间，提取了更深层次的特征</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Add&amp;Norm层的理解</title>
    <link href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&amp;Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&amp;Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-09T14:55:10.000Z</published>
    <updated>2022-10-18T11:40:29.638Z</updated>
    
    <content type="html"><![CDATA[<p>无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><span id="more"></span><h1 id="Add操作"><a href="#Add操作" class="headerlink" title="Add操作"></a>Add操作</h1><p>首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，再通过Feed Forward进行输出。</p><p>由下图可以看出：在Encoder层和Decoder层中都用到了Add&amp;Norm操作，即残差连接和层归一化操作。<br><img src="https://img-blog.csdnimg.cn/5ef722ad3c5b407482ac132b0883c59c.png" alt="在这里插入图片描述"><br>什么是残差连接呢？残差连接就是把网络的输入和输出相加，即网络的输出为F(x)+x，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就变成了F(x)+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项‘1’，有效解决了梯度消失问题。</p><h1 id="Norm操作"><a href="#Norm操作" class="headerlink" title="Norm操作"></a>Norm操作</h1><p>首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇<a href="https://mp.weixin.qq.com/s/HNCl6MPS_hjTVHNt7UkYyw">文章</a>，才明白了Norm是什么。</p><p>假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]</span><br><span class="line">[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]]</span><br></pre></td></tr></table></figure><p>如果是在做BatchNorm（BN）的话，其计算过程如下：BN1&#x3D;(w11+w12+w13+w14+w41+<br>w42+w43+w44)&#x2F;8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean</p><p>如果是在做LayerNorm（LN）的话，则会进如下计算：LN1&#x3D;(w11+w12+w13+w14+w21+<br>w22+w23+w24+w31+w32+w33+w34)&#x2F;12，同理会得到LN2，最终得到[LN1,LN2]两个mean</p><p>如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1&#x3D;(w11+w12+w13+w14)&#x2F;4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]<br>下图完美的揭示了，这几种Norm<br><img src="https://img-blog.csdnimg.cn/a143d6b41e654fa1849f44580401110c.png" alt="在这里插入图片描述"><br>接下来我们来看一下Transformer中的Norm：首先生成[2,3,4]形状的数据，使用原始的编码方式进行编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> InstanceNorm2d</span><br><span class="line">random_seed = <span class="number">123</span></span><br><span class="line">torch.manual_seed(random_seed)</span><br><span class="line"></span><br><span class="line">batch_size, seq_size, dim = <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">embedding = torch.randn(batch_size, seq_size, dim)</span><br><span class="line"></span><br><span class="line">layer_norm = torch.nn.LayerNorm(dim, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y: &quot;</span>, layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y:  tensor([[[ <span class="number">1.5524</span>,  <span class="number">0.0155</span>, -<span class="number">0.3596</span>, -<span class="number">1.2083</span>],</span><br><span class="line">         [ <span class="number">0.5851</span>,  <span class="number">1.3263</span>, -<span class="number">0.7660</span>, -<span class="number">1.1453</span>],</span><br><span class="line">         [ <span class="number">0.2864</span>,  <span class="number">0.0185</span>,  <span class="number">1.2388</span>, -<span class="number">1.5437</span>]],</span><br><span class="line">        [[ <span class="number">1.1119</span>, -<span class="number">0.3988</span>,  <span class="number">0.7275</span>, -<span class="number">1.4406</span>],</span><br><span class="line">         [-<span class="number">0.4144</span>, -<span class="number">1.1914</span>,  <span class="number">0.0548</span>,  <span class="number">1.5510</span>],</span><br><span class="line">         [ <span class="number">0.3914</span>, -<span class="number">0.5591</span>,  <span class="number">1.4105</span>, -<span class="number">1.2428</span>]]])</span><br></pre></td></tr></table></figure><p>接下来手动去进行一下编码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以发现和LayerNorm的结果是一样的，也就是说明Norm是对d_model进行的Norm，会给我们[batch,sqe_length]形状的平均值。<br>加下来进行batch_norm,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_norm = torch.nn.LayerNorm([seq_size,dim], elementwise_affine = <span class="literal">False</span>)</span><br><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1822</span>,  <span class="number">0.4419</span>, -<span class="number">0.3196</span>, -<span class="number">1.9889</span>],</span><br><span class="line">         [-<span class="number">0.6677</span>, -<span class="number">0.2537</span>, -<span class="number">0.8151</span>,  <span class="number">1.5143</span>],</span><br><span class="line">         [ <span class="number">0.7174</span>,  <span class="number">1.2147</span>, -<span class="number">0.0852</span>, -<span class="number">0.9403</span>]],</span><br><span class="line">        [[-<span class="number">0.0138</span>,  <span class="number">1.5666</span>, -<span class="number">2.1726</span>,  <span class="number">1.0590</span>],</span><br><span class="line">         [ <span class="number">0.6646</span>,  <span class="number">0.6852</span>, -<span class="number">0.8706</span>, -<span class="number">0.0442</span>],</span><br><span class="line">         [-<span class="number">0.1163</span>,  <span class="number">0.1389</span>,  <span class="number">0.4454</span>, -<span class="number">1.3423</span>]]])</span><br></pre></td></tr></table></figure><p>可以看到BN的计算的mean形状为[2, 1, 1]，并且Norm结果也和上面的两个不一样，这就充分说明了Norm是在对最后一个维度求平均。<br>那么什么又是Instancenorm呢？接下来再来实现一下instancenorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instance_norm = InstanceNorm2d(<span class="number">3</span>, affine=<span class="literal">False</span>)</span><br><span class="line">output = instance_norm(embedding.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)) <span class="comment">#InstanceNorm2D需要(N,C,H,W)的shape作为输入</span></span><br><span class="line">layer_norm = torch.nn.LayerNorm(<span class="number">4</span>, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br></pre></td></tr></table></figure><p>可以看出无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><p>如果喜欢文章请点个赞，笔者也是一个刚入门Transformer的小白，一起学习，共同努力。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Add&amp;Norm" scheme="https://du2279664786.github.io/tags/Add-Norm/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中self-attention的理解</title>
    <link href="https://du2279664786.github.io/2022/10/08/2022-10-08%E5%AF%B9Transformer%E4%B8%ADself-attention%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/08/2022-10-08%E5%AF%B9Transformer%E4%B8%ADself-attention%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-08T14:55:10.000Z</published>
    <updated>2022-10-18T11:38:45.094Z</updated>
    
    <content type="html"><![CDATA[<p>Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差</p><span id="more"></span><h1 id="什么是self-attention"><a href="#什么是self-attention" class="headerlink" title="什么是self-attention"></a>什么是self-attention</h1><p>首先我们来看一下Transformer架构：对于input输出，首先进行input embedding，然后再进行positional encoding，将两者相加作为Encoder的输入，也就是输如X<img src="https://img-blog.csdnimg.cn/65ef7c5730514e2cac88d332e2588423.png" alt="在这里插入图片描述"><br>何为self-attention？首先我们要明白什么是attention，对于传统的seq2seq任务，例如中-英文翻译，输入中文，得到英文，即source是中文句子（x1 x2 x3）,英文句子是target（y1 y2 y3）<br><img src="https://img-blog.csdnimg.cn/296c379fd77c475ebf77c06fa8e42e59.png" alt="在这里插入图片描述"><br>attention机制发生在target的元素和source中的所有元素之间。简单的将就是attention机制中的权重计算需要target参与，即在上述Encoder-Decoder模型中，Encoder和Decoder两部分都需要参与运算。</p><p>而对于self-attention，它不需要Decoder的参与，而是source内部元素之间发生的运算，对于输入向量X，对其做线性变换，分别得到Q、K、V矩阵<br><img src="https://img-blog.csdnimg.cn/17358bbb1cf641d78117625fb5a00d31.png" alt="在这里插入图片描述"><br>然后去计算attention，Q、K点乘得到初步的权重因子，并对Q、K点乘结果进行放缩，除以sqrt（dk），Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差，最终再加一个softmax就得到了self attention的输出。<br><img src="https://img-blog.csdnimg.cn/07fdbd802d114b7193c70e9fc451ba22.png" alt="在这里插入图片描述"></p><h1 id="Multi–head-attention"><a href="#Multi–head-attention" class="headerlink" title="Multi–head-attention"></a>Multi–head-attention</h1><p>Multi–head-attention使用了多个头进行运算，捕捉到了更多的信息，多头的数量用h表示，一般h&#x3D;8，表示8个头<br><img src="https://img-blog.csdnimg.cn/7cad1e37120b4d40ad64140677452ec1.png" alt="在这里插入图片描述"><br>在输入每个self-attention之前，我们需将输入X均分的分到h个头中，得到Z1-Z7八个头的输出结果。<br><img src="https://img-blog.csdnimg.cn/d5763f726a5c48f292a140f84c0e5200.png" alt="在这里插入图片描述"><br>对于每个头计算相应的attention score，将其进行拼接，再与W0进行一个线性变换，就得到最终输出的Z。<br><img src="https://img-blog.csdnimg.cn/05bfb9eb87bb4f3b8066c8190c0dff0f.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Positional Encoding的理解</title>
    <link href="https://du2279664786.github.io/2022/10/07/2022-10-07%E5%AF%B9Transformer%E4%B8%ADPositional%20Encoding%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/07/2022-10-07%E5%AF%B9Transformer%E4%B8%ADPositional%20Encoding%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-07T14:55:10.000Z</published>
    <updated>2022-10-19T03:21:15.466Z</updated>
    
    <content type="html"><![CDATA[<p>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值</p><span id="more"></span><p>首先来看一下Transformer结构的结构：<br><img src="https://img-blog.csdnimg.cn/c7ce14349b734567a014c4cf679398fe.png" alt="在这里插入图片描述"><br>Transformer是由Encoder和Decoder两大部分组成，首先对于文本特征，需要进行Embedding，由于transformer抛弃了Rnn的结构，不能捕捉到序列的信息，交换单词位置，得到相应的attention也会发生交换，并不会发生数值上的改变，所以要对input进行Positional Encoding。</p><p>Positional encoding和input embedding是同等维度的，所以可以将两者进行相加，的到输入向量<br><img src="https://img-blog.csdnimg.cn/be30b27838dd411c89d793432ff72582.png" alt="在这里插入图片描述"><br>接下来看一些Positional Encoding的计算公式：<br><img src="https://img-blog.csdnimg.cn/6e9a80e756b94a70aeef8a79097eb7a6.png" alt="在这里插入图片描述"><br>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值，也就是说：对于单个token的d_model维度的词向量，奇数位置取cos，偶数位置取sin，最终的到一个维度和word embedding维度一样的矩阵，接下来可以看一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_positional_encoding</span>(<span class="params">max_seq_len, embed_dim</span>):</span><br><span class="line">    <span class="comment"># 初始化一个positional encoding</span></span><br><span class="line">    <span class="comment"># embed_dim: 字嵌入的维度</span></span><br><span class="line">    <span class="comment"># max_seq_len: 最大的序列长度</span></span><br><span class="line">    positional_encoding = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * i / embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embed_dim)] <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(embed_dim) <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len)])</span><br><span class="line"></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i 偶数</span></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1 奇数</span></span><br><span class="line">    <span class="keyword">return</span> positional_encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">positional_encoding = get_positional_encoding(max_seq_len=<span class="number">100</span>, embed_dim=<span class="number">16</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">sns.heatmap(positional_encoding)</span><br><span class="line">plt.title(<span class="string">&quot;Sinusoidal Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;hidden dimension&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;sequence length&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>首先求初始向量：positional_encoding，然后对其奇数列求sin，偶数列求cos：<br><img src="https://img-blog.csdnimg.cn/2da3445d7953426394ab2e46d8820baf.png" alt="在这里插入图片描述"><br>最终得到positional encoding之后的数据可视化：<br><img src="https://img-blog.csdnimg.cn/507cc7ca0ba34f8fb2ec87216689857c.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Positional Encoding" scheme="https://du2279664786.github.io/tags/Positional-Encoding/"/>
    
  </entry>
  
  <entry>
    <title>怎么理解预训练模型？</title>
    <link href="https://du2279664786.github.io/2022/10/06/2022-10-06%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F/"/>
    <id>https://du2279664786.github.io/2022/10/06/2022-10-06%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F/</id>
    <published>2022-10-06T14:55:10.000Z</published>
    <updated>2022-10-19T03:22:28.472Z</updated>
    
    <content type="html"><![CDATA[<p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性……</p><span id="more"></span><h1 id="什么是预训练"><a href="#什么是预训练" class="headerlink" title="什么是预训练"></a>什么是预训练</h1><p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性，然后将其中的共性“移植”到特定任务的模型中，再使用相关特定领域的少量标注数据进行“微调”，这样的话，模型只需要从”共性“出发，去“学习”该特定任务的“特殊”部分即可。</p><h1 id="预训练的思想"><a href="#预训练的思想" class="headerlink" title="预训练的思想"></a>预训练的思想</h1><p>预训练的思想是：模型的参数不再是随机初始化的，而是通过一些任务进行预先训练，得到一套模型参数，然后用这套参数对模型进行初始化，再进行训练</p><h1 id="CV领域的预训练"><a href="#CV领域的预训练" class="headerlink" title="CV领域的预训练"></a>CV领域的预训练</h1><p>首先对于CV领域图片分类任务，常用的深度学习模型是卷积视神经网络，对于多层的卷积神经网络来说，不同的层学到的特征是不同的，为了捕获更多的特征，浅层的感受野较小，所以浅层学到的特征往往是更加通用的，包含更多的像素点的信息，比如一些细粒度的信息：颜色、纹理、边缘等。<br>通常在大规模图片数据上预先获得‘通用特征’，然后再去做下游任务：<br><img src="https://img-blog.csdnimg.cn/455a209bb08941ff8d390ede716556b4.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性……&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="预训练" scheme="https://du2279664786.github.io/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>HuggingFace的安装和编码</title>
    <link href="https://du2279664786.github.io/2022/10/05/2022-10-05HuggingFace%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A0%81/"/>
    <id>https://du2279664786.github.io/2022/10/05/2022-10-05HuggingFace%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A0%81/</id>
    <published>2022-10-05T14:55:10.000Z</published>
    <updated>2022-10-18T11:36:57.563Z</updated>
    
    <content type="html"><![CDATA[<p>模型的加载和编码以及基本的使用功能</p><span id="more"></span><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>使用以下命令安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></table></figure><h1 id="模型的加载"><a href="#模型的加载" class="headerlink" title="模型的加载"></a>模型的加载</h1><p>导入包：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from transformers import BertTokenizer</span><br></pre></td></tr></table></figure><p>加载预训练模型bert-base-chinese，初次加载可能需要较长的时间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#加载预训练字典和分词方法</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(</span><br><span class="line"># 下载或者从本地加载模型</span><br><span class="line">    pretrained_model_name_or_path=&#x27;bert-base-chinese&#x27;,</span><br><span class="line">    force_download=False,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>接下来就可以看到tokenizer的内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br><span class="line"></span><br><span class="line">#print:(name_or_path=&#x27;bert-base-chinese&#x27;, vocab_size=21128, model_max_len=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;unk_token&#x27;: &#x27;[UNK]&#x27;, &#x27;sep_token&#x27;: &#x27;[SEP]&#x27;, &#x27;pad_token&#x27;: &#x27;[PAD]&#x27;, &#x27;cls_token&#x27;: &#x27;[CLS]&#x27;, &#x27;mask_token&#x27;: &#x27;[MASK]&#x27;&#125;)</span><br></pre></td></tr></table></figure><h1 id="进行编码"><a href="#进行编码" class="headerlink" title="进行编码"></a>进行编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sents = [</span><br><span class="line">    &#x27;我在一所学院上大三。&#x27;,</span><br><span class="line">    &#x27;今天天气好，我来图书馆学习。&#x27;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">res= tokenizer.encode(</span><br><span class="line">    # 可以一次编码两个句子，即text和text_pair</span><br><span class="line">    text = sents[0],</span><br><span class="line">    text_pair=sents[1],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    return_tensors=None,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以打印出res的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 101为CLS    102为SEP    0为PAD</span><br><span class="line">#print(res):[101, 2769, 1762, 671, 2792, 2110, 7368, 677, 1920, 676, 511, 102, 791, 1921, 1921, 3698, 1962, 8024, 2769, 3341, 1745, 741, 7667, 2110, 739, 511, 102, 0, 0, 0]</span><br></pre></td></tr></table></figure><p>也可以查看编码之后的结果:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decode(res)</span><br><span class="line"># [CLS] 我 在 一 所 学 院 上 大 三 。 [SEP] 今 天 天 气 好 ， 我 来 图 书 馆 学 习 。 [SEP] [PAD] [PAD] [PAD]</span><br></pre></td></tr></table></figure><h1 id="多功能编码"><a href="#多功能编码" class="headerlink" title="多功能编码"></a>多功能编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">res = tokenizer.encode_plus(</span><br><span class="line">    # 可以一次编码两个句子</span><br><span class="line">    text = sents[0],</span><br><span class="line">    text_pair=sents[1],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    </span><br><span class="line">    # 返回值的类型：tf  pt  np</span><br><span class="line">    return_tensors=None,</span><br><span class="line">    </span><br><span class="line">    # 返回token_type_ids</span><br><span class="line">    return_token_type_ids=True,</span><br><span class="line">    </span><br><span class="line">    # 返回attention_mask</span><br><span class="line">    return_attention_mask = True,</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # 返回length标识长度</span><br><span class="line">    return_length = True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看编码的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for k,v in res.items():</span><br><span class="line">    print(k,&#x27;:&#x27;,v)</span><br><span class="line">    </span><br><span class="line">tokenizer.decode(res[&#x27;input_ids&#x27;])</span><br><span class="line"></span><br><span class="line"># input_ids:编码后的句子</span><br><span class="line"># token_type_ids:第一个句子和特殊符号的位置是0，第二个句子的位置是1</span><br><span class="line"># attention_mask:pad的位置是0，其它位置是1</span><br><span class="line"># length:返回句子的长度</span><br></pre></td></tr></table></figure><h1 id="将句子批量进行编码"><a href="#将句子批量进行编码" class="headerlink" title="将句子批量进行编码"></a>将句子批量进行编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">out = tokenizer.batch_encode_plus(</span><br><span class="line">    batch_text_or_text_pairs=[sents[0],sents[1]],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    </span><br><span class="line">    # 返回值的类型：tf  pt  np</span><br><span class="line">    return_tensors=None,</span><br><span class="line">    </span><br><span class="line">    # 返回token_type_ids</span><br><span class="line">    return_token_type_ids=True,</span><br><span class="line">    </span><br><span class="line">    # 返回attention_mask</span><br><span class="line">    return_attention_mask = True,</span><br><span class="line">    </span><br><span class="line">    # 返回length标识长度</span><br><span class="line">    return_length = True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;模型的加载和编码以及基本的使用功能&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Python切换源，快速下载文件</title>
    <link href="https://du2279664786.github.io/2022/09/05/2022-09-05Python%E5%88%87%E6%8D%A2%E6%BA%90%EF%BC%8C%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/"/>
    <id>https://du2279664786.github.io/2022/09/05/2022-09-05Python%E5%88%87%E6%8D%A2%E6%BA%90%EF%BC%8C%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/</id>
    <published>2022-09-05T14:55:10.000Z</published>
    <updated>2022-10-18T09:17:15.868Z</updated>
    
    <content type="html"><![CDATA[<p>快速下载文件</p><span id="more"></span><h1 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h1><p>进入电脑路径’C:\Users\user_name\AppData\Roaming\pip’文件夹下，找到pip.ini文件<br><img src="https://img-blog.csdnimg.cn/d02a90ee68bd40248c2b39ea89154681.png" alt="在这里插入图片描述"><br>若没有此文件则可以创建一个pip文件，后缀为ini，，然后打开文件，将文件内容替换为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">global</span>]</span><br><span class="line"></span><br><span class="line">index-url=http://pypi.douban.com/simple</span><br><span class="line"></span><br><span class="line">[install]</span><br><span class="line"></span><br><span class="line">trusted-host=pypi.douban.com</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>保存文件即可<br><img src="https://img-blog.csdnimg.cn/435b149734414ae49fd8cb8cfbe6f124.png" alt="在这里插入图片描述"><br>下载速度惊人<br><img src="https://img-blog.csdnimg.cn/e904985a074142faaab9e22a7e273cc5.png" alt="在这里插入图片描述"><br>如果文章对您有帮助，请点赞+收藏</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;快速下载文件&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SOFTMAX回归模型</title>
    <link href="https://du2279664786.github.io/2022/05/08/2022-05-08SOFTMAX%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>https://du2279664786.github.io/2022/05/08/2022-05-08SOFTMAX%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-05-08T14:55:10.000Z</published>
    <updated>2022-10-18T09:14:53.067Z</updated>
    
    <content type="html"><![CDATA[<p>SOFTMAX函数的脉络梳理</p><span id="more"></span><p>2022-05-08SOFTMAX回归模型</p><h2 id="什么是SOFTMAX回归函数"><a href="#什么是SOFTMAX回归函数" class="headerlink" title="什么是SOFTMAX回归函数"></a>什么是SOFTMAX回归函数</h2><p>·softmax回归跟线性回归⼀样将输⼊特征与权᯿做线性叠加<br>·与线性回归的⼀个主要不同在于，softmax回归的输出值个数等于标签⾥的类别数<br>·SOFTMAX是一个单层的神经网络<br>结构图如下：<br><img src="https://img-blog.csdnimg.cn/1eef7cdde9b04e1d866471e374b3a80b.png" alt="在这里插入图片描述"><br>运算过程如下：<br><img src="https://img-blog.csdnimg.cn/4e928316894e4e298b9e71c18598fbd1.png" alt="在这里插入图片描述"><br>即我们通过神经网络预测，然后得到相应的一个分数，此时我们希望我们得到的分数是一个概率：<br><img src="https://img-blog.csdnimg.cn/c16934676c254f119005ddefbb0d5164.png" alt="在这里插入图片描述"><br>此时我们就应该选取一个合适的方案，来将预测的分数来转化为标签的概率，那么最合适的肯定是softmax了，softmax公式：<br><img src="https://img-blog.csdnimg.cn/df4b36ec76f44413b348ad2917c4d399.png" alt="在这里插入图片描述"><br>即将得到的分数都进行exp，然后求每个标签占比(概率)，最终我们得到的是预测概率最大的标签：<br><img src="https://img-blog.csdnimg.cn/656cab1ab05b4020ac06c14f0ea28951.png" alt="在这里插入图片描述"><br>很显然，我们只关注预测概率最大的标签（单标签预测）<br>总结了一下特性：<br>    ·结果都为正数：即将得分为负数的进行转化<br>    ·所有求和为1：所有概率相加等于1<br>    ·平移不变性：所有得分平移得到的结果不受影响<br>    ·最大–&gt;最大：预测得分最大的概率也大</p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>在这里我们使用的是交叉熵损失函数（Cross Entropy）<br><img src="https://img-blog.csdnimg.cn/eded5e651bfd471dada59239dc2b6c9d.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/cf23b1b754ab4f3eae57f49f5f12b0f1.png" alt="其中带下标的是向量 中⾮0即1的元素"></p><h2 id="实现代码："><a href="#实现代码：" class="headerlink" title="实现代码："></a>实现代码：</h2><p>·导入包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>若出现没有‘d2lzh_pytorch’这个包，<a href="https://blog.csdn.net/weixin_51756104/article/details/124626354?spm=1001.2014.3001.5501">点击此处离线安装</a><br>·导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>·定义网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, 1, 28, 28)</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"> </span><br><span class="line">net = LinearNet(num_inputs,num_outputs)</span><br></pre></td></tr></table></figure><p>·初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>·定义损失函数和优化器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>·训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;SOFTMAX函数的脉络梳理&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="SOFTMAX" scheme="https://du2279664786.github.io/tags/SOFTMAX/"/>
    
  </entry>
  
  <entry>
    <title>d2lzh_pytorch包离线安装</title>
    <link href="https://du2279664786.github.io/2022/05/07/2022-05-07d2lzh_pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/"/>
    <id>https://du2279664786.github.io/2022/05/07/2022-05-07d2lzh_pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/</id>
    <published>2022-05-07T14:55:10.000Z</published>
    <updated>2022-10-18T09:12:51.207Z</updated>
    
    <content type="html"><![CDATA[<p>线上安装经常出错，所以可以选择离线安装</p><span id="more"></span><p>在导入d2lzh_pytorch包时，一般会报错：<br><img src="https://img-blog.csdnimg.cn/56f5ff1ee876479e9707f06a234a04dd.png" alt="在这里插入图片描述"><br>我们可以离线下载包：<br>链接：<a href="https://pan.baidu.com/share/init?surl=iXyFqY8uM5PGhrthL_-9xQ#list/path=/">点击此处</a>，提取码：1314<br>下载后：进入到我们要使用的环境，按照如下位置安放包<br><img src="https://img-blog.csdnimg.cn/79fcfef8196a43c8bb122b712b2dce9d.png" alt="在这里插入图片描述"><br>最后就可以导入了：<br><img src="https://img-blog.csdnimg.cn/4c3b27ce6aa9488494f97a336101d894.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;线上安装经常出错，所以可以选择离线安装&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="d2lzh_pytorch包离线安装" scheme="https://du2279664786.github.io/tags/d2lzh-pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>线性回归的简洁实现</title>
    <link href="https://du2279664786.github.io/2022/04/28/2022-04-28%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://du2279664786.github.io/2022/04/28/2022-04-28%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-04-28T14:55:10.000Z</published>
    <updated>2022-10-18T09:11:02.360Z</updated>
    
    <content type="html"><![CDATA[<p>创建单层神经网络</p><span id="more"></span><p>线性回归详细实现，<a href="https://blog.csdn.net/weixin_51756104/article/details/124334225">请点击此处</a></p><h3 id="导入包："><a href="#导入包：" class="headerlink" title="导入包："></a>导入包：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data    <span class="comment"># 数据读取</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init     <span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim   <span class="comment"># 优化器</span></span><br></pre></td></tr></table></figure><h3 id="数据集的生成"><a href="#数据集的生成" class="headerlink" title="数据集的生成"></a>数据集的生成</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span>     <span class="comment"># 2个维度</span></span><br><span class="line">num_examples = <span class="number">1000</span>     <span class="comment"># 1000调数据</span></span><br><span class="line"><span class="comment"># 标准的参数</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples,num_inputs)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">label = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] +true_b</span><br><span class="line">label += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>,size=label.size()), dtype=torch.<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure><h3 id="数据的读取"><a href="#数据的读取" class="headerlink" title="数据的读取"></a>数据的读取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">30</span></span><br><span class="line">dataset = Data.TensorDataset(features,label)</span><br><span class="line">data_it = Data.DataLoader(dataset,batch_size,shuffle =<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>定义模型有多种方法：<br>方法一：继承nn.Module</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,n_feature</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(n_feature,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br></pre></td></tr></table></figure><p>方法二：nn.Sequential</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line"> )</span><br></pre></td></tr></table></figure><p>方法三：nn.Sequential()+add_module</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>方法四：导入OrderedDict</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))]))</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias,val=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="MSE损失函数"><a href="#MSE损失函数" class="headerlink" title="MSE损失函数"></a>MSE损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure><h3 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure><h3 id="模型的优化"><a href="#模型的优化" class="headerlink" title="模型的优化"></a>模型的优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_it:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output, y.view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(epoch, l.item())</span><br></pre></td></tr></table></figure><p>最后输出epoch和loss：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">0.3572668433189392</span></span><br><span class="line"><span class="number">2</span> <span class="number">0.005662666633725166</span></span><br><span class="line"><span class="number">3</span> <span class="number">0.00011592111695790663</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建单层神经网络&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>史上最详细的Pytorch+CUDA+CUDNN的安装(GPU版)</title>
    <link href="https://du2279664786.github.io/2022/04/27/2022-04-27%E5%8F%B2%E4%B8%8A%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84Pytorch+CUDA+CUDNN%E7%9A%84%E5%AE%89%E8%A3%85(GPU%E7%89%88)/"/>
    <id>https://du2279664786.github.io/2022/04/27/2022-04-27%E5%8F%B2%E4%B8%8A%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84Pytorch+CUDA+CUDNN%E7%9A%84%E5%AE%89%E8%A3%85(GPU%E7%89%88)/</id>
    <published>2022-04-27T14:55:10.000Z</published>
    <updated>2022-10-18T09:09:30.930Z</updated>
    
    <content type="html"><![CDATA[<p>炒鸡详细的pytorch GPU安装版本，从0搭建！</p><span id="more"></span><p>CPU版本的教程<a href="https://blog.csdn.net/weixin_51756104/article/details/124222546">请点击此处查看</a></p><h3 id="首先看一下自己的驱动："><a href="#首先看一下自己的驱动：" class="headerlink" title="首先看一下自己的驱动："></a>首先看一下自己的驱动：</h3><p>·如果驱动不支持CUDA11的话就要先更新驱动<br>·打开命令行win+r，输入cmd，在命令行输入：nvidia-smi   查看信息<br><img src="https://img-blog.csdnimg.cn/f1d12b1f1cb24e11a5e70143ef5c1195.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>这里可以看到我的驱动是512.2，根据下图可以看到驱动只要大于451.22就支持CUDA11，,pytorch最新本已经不支持CUDA10,如果驱动版本低于451,可以升级驱动，<a href="https://www.nvidia.com/en-us/geforce/drivers/">点击此处下载驱动</a>，下面是CUDA和显卡驱动对应的版本：<br><img src="https://img-blog.csdnimg.cn/ea404a8d657e418eaa0ab409ba8386f7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="安装Pytorch"><a href="#安装Pytorch" class="headerlink" title="安装Pytorch"></a>安装Pytorch</h3><p>此处使用的是本地安装(因为pip安装和conda安装本人都没有成功，可能是网络问题),<a href="https://download.pytorch.org/whl/torch_stable.html">点击此处</a>进行Pytorch的下载：可以看到我的CUDA是11.6版本：<br><img src="https://img-blog.csdnimg.cn/06c6e268cdfa4a2da9224cb2ba7f9b48.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>我们进入下载pytorch的网站，发现还没有CUDA11.6版本，我们可以下载CUDA11.5版本，<br>cu115代表CUDA11.5版本，cp38代表python的版本，选择合适的进行下载，我下载的是CUDA11.5版本，Python版本3.8,所以我们选择：<br><img src="https://img-blog.csdnimg.cn/ac64598e145c4b3b943c9516ba7df188.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>同理我们再选择torchvison和torchaudio的下载，下载完成后进行本地安装：使用pip install+安装包的路径安装，我的在D盘：<br><img src="https://img-blog.csdnimg.cn/5e018a995bc3415faf2ccdb8be245aa7.png" alt="："><br>此时我们就可以检测一下是否安装成功：<br><img src="https://img-blog.csdnimg.cn/2c27cd5cb5084ae3a502032a12bd7316.png" alt="在这里插入图片描述">可以看到已经成功了！！！</p><h3 id="CUDA安装"><a href="#CUDA安装" class="headerlink" title="CUDA安装"></a>CUDA安装</h3><p><a href="https://developer.nvidia.com/cuda-toolkit-archive">点击此处</a>，进入下载，选择合适自己的版本：<br><img src="https://img-blog.csdnimg.cn/91fb78d977294a21ae0068b7f1eb8234.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>选择好信息开始下载：<br><img src="https://img-blog.csdnimg.cn/5b0e8251a48640f9b1490f335f916d84.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>点击安装包，一路默认安装就行</p><h3 id="CUDNN安装"><a href="#CUDNN安装" class="headerlink" title="CUDNN安装"></a>CUDNN安装</h3><p><a href="https://developer.nvidia.com/rdp/cudnn-download">点击此处</a>，自行注册账号</p><p><img src="https://img-blog.csdnimg.cn/fcc85c8aeb184e60bf7f1899a3d3e681.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后点击下载：需要填写调查问卷，点击提交<br><img src="https://img-blog.csdnimg.cn/55be6915942542f2b7e61f587a44e4fa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>找到相应的版本：<br><img src="https://img-blog.csdnimg.cn/84f7be9ba3e74a5bb4566b39b57ea958.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>这里我的是windows CUDA11.6，所以我下载windows版本的压缩包<br><img src="https://img-blog.csdnimg.cn/cb5883133ef64418adefffb76c9aebfa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>下载完进行解压：<br><img src="https://img-blog.csdnimg.cn/69864e29e15c46afa22218a9edb63b6e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后找到我们CUDA11.6的位置，默认安装的在：C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA<br><img src="https://img-blog.csdnimg.cn/70e4ce0c5ba84be48fd10eecd82524af.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后我们找到刚刚解压的cudnn文件夹<br><img src="https://img-blog.csdnimg.cn/87f93e9696584e929c591d283fc883c2.png" alt="在这里插入图片描述"><br>将bin，include，lib文件夹下里面的‘文件’分别复制到CUDA相应的文件夹里面（复制的是里面的的文件，不是文件夹）：<br><img src="https://img-blog.csdnimg.cn/69d6624d180c40d999de7c7aa76bc6dc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h3><p>·查看CUDA，在命令行输入：nvcc -V，出现以下代表成功：<br><img src="https://img-blog.csdnimg.cn/2bf5e7e797224b03aa842149bab8c4cf.png" alt="在这里插入图片描述"><br>·查看cudnn，我们在命令行进入安装cuda的目录，我的是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11<span class="number">.6</span>\extras\demo_suite</span><br></pre></td></tr></table></figure><p>然后在命令行进入文件夹：<br><img src="https://img-blog.csdnimg.cn/3be6ef8c961743acad34f275013dcd1d.png" alt="在这里插入图片描述"><br>输入：bandwidthTest.exe<br><img src="https://img-blog.csdnimg.cn/c5722a063f564e788682cd13491f759a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>输入：deviceQuery.exe<br><img src="https://img-blog.csdnimg.cn/cc1b0f43a5944f03b12627fd3738dc74.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>表示安装成功！！！<br>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;炒鸡详细的pytorch GPU安装版本，从0搭建！&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="CUDA" scheme="https://du2279664786.github.io/tags/CUDA/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch线性回归的详细实现</title>
    <link href="https://du2279664786.github.io/2022/04/26/2022-04-26Pytorch%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%9E%E7%8E%B0/"/>
    <id>https://du2279664786.github.io/2022/04/26/2022-04-26Pytorch%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-04-26T14:55:10.000Z</published>
    <updated>2022-10-18T11:29:12.034Z</updated>
    
    <content type="html"><![CDATA[<p>创建单层神经⽹络</p><span id="more"></span><h2 id="线性回归-单层神经网络"><a href="#线性回归-单层神经网络" class="headerlink" title="线性回归-单层神经网络"></a>线性回归-单层神经网络</h2><p>线性回归是⼀个单层神经⽹络<br><img src="https://img-blog.csdnimg.cn/590a40befa164375b9ceb587cda58445.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&amp;emsp;输⼊分别为x1和x2，因此输⼊层的输⼊个数为2,输⼊个数也叫特征数或<br>特征向量维度,输出层的输出个数为1,输出层中的神经元和输⼊层中各个输⼊完全连<br>接,因此，这⾥的输出层⼜叫全连接层,即一个简单地线性回归。<br>&amp;emsp;假设我们有三个预测数据：<br><img src="https://img-blog.csdnimg.cn/e73f69360b1d4f4b90360de495dac343.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>转化为矩阵运算：<br><img src="https://img-blog.csdnimg.cn/eec385599fc3439c90c79ebce9737f30.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>即<br><img src="https://img-blog.csdnimg.cn/10cf121677624495bc8ad82f4f203933.png" alt="在这里插入图片描述"></p><h2 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h2><p>首先导入所需要的包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure><p>生成数据集：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_input = <span class="number">2</span></span><br><span class="line">num_example = <span class="number">1000</span>   <span class="comment"># 1000条样本</span></span><br><span class="line"><span class="comment"># 定义标准的参数</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]   </span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">np.random.seed(<span class="number">2012</span>)</span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>,<span class="number">1</span>,(<span class="number">1000</span>,<span class="number">2</span>)))</span><br><span class="line"><span class="comment"># 构造标签</span></span><br><span class="line">labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] +true_b</span><br><span class="line">labels += torch.from_numpy(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>,size=labels.size()))</span><br><span class="line"><span class="built_in">print</span>(features,labels)</span><br></pre></td></tr></table></figure><p>数据的读取：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_item</span>(<span class="params">bach_size,features,labels</span>):</span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices) <span class="comment"># 样本的读取顺序是随机的</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, bach_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: <span class="built_in">min</span>(i + bach_size,num_examples)]) <span class="comment"># 最后⼀次可能不⾜⼀个batch</span></span><br><span class="line">        <span class="keyword">yield</span> features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br></pre></td></tr></table></figure><p>随机初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_input, <span class="number">1</span>)),dtype=torch.double) </span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.double)</span><br><span class="line">w.requires_grad = <span class="literal">True</span>     <span class="comment"># 定义为可求梯度</span></span><br><span class="line">b.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>定义线性回归函数，使⽤ mm 函数(矩阵相乘)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params">x,w,b</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.mm(x,w)+b</span><br></pre></td></tr></table></figure><p>定义损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">y_hat, y</span>): <span class="comment"># 本函数已保存在d2lzh_pytorch包中⽅便以后使⽤</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure><p>定义优化函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">params, lr, batch_size</span>): <span class="comment"># 本函数已保存在d2lzh_pytorch包中⽅便以后使⽤</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size <span class="comment"># 修改的的param.data</span></span><br></pre></td></tr></table></figure><p>训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">bach_size = <span class="number">30</span></span><br><span class="line">net = linear</span><br><span class="line">loss = loss</span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> data_item(bach_size=bach_size,features=features,labels=labels):</span><br><span class="line">        los = loss(linear(x,w,b),y).<span class="built_in">sum</span>()</span><br><span class="line">        los.backward()</span><br><span class="line">        </span><br><span class="line">        SGD([w,b],lr=lr,batch_size=bach_size)</span><br><span class="line"><span class="comment">#         print(b)</span></span><br><span class="line">        w.grad.zero_()</span><br><span class="line">        b.grad.zero_()</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br><span class="line"><span class="built_in">print</span>(true_w, <span class="string">&#x27;\n&#x27;</span>, w)</span><br><span class="line"><span class="built_in">print</span>(true_b, <span class="string">&#x27;\n&#x27;</span>, b)</span><br></pre></td></tr></table></figure><p>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建单层神经⽹络&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="线性回归" scheme="https://du2279664786.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch自动求梯度</title>
    <link href="https://du2279664786.github.io/2022/04/19/2022-04-19Pytorch%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6/"/>
    <id>https://du2279664786.github.io/2022/04/19/2022-04-19Pytorch%E8%87%AA%E5%8A%A8%E6%B1%82%E6%A2%AF%E5%BA%A6/</id>
    <published>2022-04-19T14:55:10.000Z</published>
    <updated>2022-10-18T09:06:00.494Z</updated>
    
    <content type="html"><![CDATA[<p>创建Tensor的几种方式</p><span id="more"></span><h1 id="微分"><a href="#微分" class="headerlink" title="微分"></a>微分</h1><p>通常我们见到的微分方法有两种：<br>·符号微分法：<br><img src="https://img-blog.csdnimg.cn/bbaa997b4ed34967a69068ebfa46d97d.png" alt="在这里插入图片描述"><br>·数值微分法：<br><img src="https://img-blog.csdnimg.cn/ac854fda494b48c1825b1c860dc258f8.png" alt="∂f(x)/∂x=lim┬ℎ→0f(x+ℎ)−f(x)/ℎ"></p><h1 id="Pytorch自动微分"><a href="#Pytorch自动微分" class="headerlink" title="Pytorch自动微分"></a>Pytorch自动微分</h1><p>对于一个Tensor，如果它的属性requires_grad 设置为 True，它将开始追<br>踪(track)在其上的所有操作<br>我们定义一个初始的tensor并且requires_grad 设置为 True：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>此时，我们在x的基础上进行运算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line"><span class="comment"># grad_fn属性代表y是否由运算得来</span></span><br></pre></td></tr></table></figure><p>此时我们就可以进一步运算：out &#x3D;（x+2）**2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z, out)</span><br></pre></td></tr></table></figure><p>反向传播：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out.backward() <span class="comment"># 等价于 out.backward(torch.tensor(1.))</span></span><br></pre></td></tr></table></figure><p>此时我们就可以输出x的梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p>注意：grad在反向传播过程中是累加的(accumulated)，这意味着每⼀次运⾏反向传播，梯度都会累加之前的梯度，所以⼀般在反向传播之前需把梯度清零</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure><p>为什么会输出这个值呢？接下来看一下过程：<br>我们可以写出out的等式：<br><img src="https://img-blog.csdnimg.cn/8aee7ec7069c4ea5b5e54a8fcaf7d344.png" alt="在这里插入图片描述"><br>此时我们求o关于x的偏导：<br><img src="https://img-blog.csdnimg.cn/d01a2a217b0146faa398d2a078b6f8df.png" alt="在这里插入图片描述"><br>那么我们在在进行求梯度时为什么要求out的梯度呢？为什么最后要z.mean()呢？<br>很显然我们直接y.backward()会报错<img src="https://img-blog.csdnimg.cn/812ca92f731f4290909e9d92c40502a0.png" alt="在这里插入图片描述"></p><p>这是因为：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传⼊任何参数；否则，需要传⼊⼀个与 y 同形的 Tensor 。<br>在pytorch中：不允许张量对张量求导，只允许标量对张量求导，求导结果是和⾃变量同形的张量。所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量<br>接下来看一个实际的栗子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>], requires_grad=<span class="literal">True</span>) </span><br><span class="line">y = <span class="number">2</span> * x </span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>, <span class="number">8.</span>], grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure><p>此时我们直接y.backward()会报错，因为y不是标量，所以我们按照要求应该传入一个同形的张量，作为权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">y.backward(t)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br></pre></td></tr></table></figure><p>d(y) &#x3D; 2<br>求导的同时也应该乘以相应的权重t &#x3D; torch.tensor([1,2,3,4])，所以最后输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2.</span>, <span class="number">4.</span>, <span class="number">6.</span>, <span class="number">8.</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建Tensor的几种方式&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="梯度" scheme="https://du2279664786.github.io/tags/%E6%A2%AF%E5%BA%A6/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中Tensor的创建</title>
    <link href="https://du2279664786.github.io/2022/04/17/2022-04-17Pytorch%E4%B8%ADTensor%E7%9A%84%E5%88%9B%E5%BB%BA/"/>
    <id>https://du2279664786.github.io/2022/04/17/2022-04-17Pytorch%E4%B8%ADTensor%E7%9A%84%E5%88%9B%E5%BB%BA/</id>
    <published>2022-04-17T14:55:10.000Z</published>
    <updated>2022-10-18T09:04:24.850Z</updated>
    
    <content type="html"><![CDATA[<p>创建Tensor的几种方式</p><span id="more"></span><h1 id="创建Tensor"><a href="#创建Tensor" class="headerlink" title="创建Tensor"></a>创建Tensor</h1><h3 id="创建一个5行3列未初始化的tensor"><a href="#创建一个5行3列未初始化的tensor" class="headerlink" title="创建一个5行3列未初始化的tensor"></a>创建一个5行3列未初始化的tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.0194e-38</span>, <span class="number">1.0469e-38</span>, <span class="number">1.0010e-38</span>],</span><br><span class="line">        [<span class="number">8.9081e-39</span>, <span class="number">8.9082e-39</span>, <span class="number">5.9694e-39</span>],</span><br><span class="line">        [<span class="number">8.9082e-39</span>, <span class="number">1.0194e-38</span>, <span class="number">9.1837e-39</span>],</span><br><span class="line">        [<span class="number">4.6837e-39</span>, <span class="number">9.2755e-39</span>, <span class="number">1.0837e-38</span>],</span><br><span class="line">        [<span class="number">8.4490e-39</span>, <span class="number">1.1112e-38</span>, <span class="number">1.0194e-38</span>]])</span><br></pre></td></tr></table></figure><h3 id="创建一个5行3列随机初始化的tensor："><a href="#创建一个5行3列随机初始化的tensor：" class="headerlink" title="创建一个5行3列随机初始化的tensor："></a>创建一个5行3列随机初始化的tensor：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.5911</span>, <span class="number">0.9191</span>, <span class="number">0.9826</span>],</span><br><span class="line">        [<span class="number">0.4801</span>, <span class="number">0.1648</span>, <span class="number">0.8578</span>],</span><br><span class="line">        [<span class="number">0.9937</span>, <span class="number">0.8051</span>, <span class="number">0.6952</span>],</span><br><span class="line">        [<span class="number">0.9682</span>, <span class="number">0.1975</span>, <span class="number">0.1151</span>],</span><br><span class="line">        [<span class="number">0.2434</span>, <span class="number">0.2917</span>, <span class="number">0.7866</span>]])</span><br></pre></td></tr></table></figure><h3 id="创建Tensor还可以指定数据类型：创建一个5行3列的类型为long的全0数据"><a href="#创建Tensor还可以指定数据类型：创建一个5行3列的类型为long的全0数据" class="headerlink" title="创建Tensor还可以指定数据类型：创建一个5行3列的类型为long的全0数据"></a>创建Tensor还可以指定数据类型：创建一个5行3列的类型为long的全0数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure><h3 id="直接输入数据进行创建tensor："><a href="#直接输入数据进行创建tensor：" class="headerlink" title="直接输入数据进行创建tensor："></a>直接输入数据进行创建tensor：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure><h3 id="根据现有的tensor创建新的tensor："><a href="#根据现有的tensor创建新的tensor：" class="headerlink" title="根据现有的tensor创建新的tensor："></a>根据现有的tensor创建新的tensor：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn_like(x,dtype = torch.float64)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-<span class="number">0.6669</span>,  <span class="number">0.5308</span>,  <span class="number">1.5981</span>],</span><br><span class="line">        [ <span class="number">1.2061</span>,  <span class="number">0.6624</span>, -<span class="number">0.4535</span>],</span><br><span class="line">        [-<span class="number">0.5667</span>, -<span class="number">0.8755</span>, -<span class="number">2.1078</span>],</span><br><span class="line">        [-<span class="number">3.0560</span>, -<span class="number">0.6035</span>,  <span class="number">0.7990</span>],</span><br><span class="line">        [-<span class="number">0.3979</span>, -<span class="number">1.3582</span>, -<span class="number">0.8427</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure><h3 id="2根据numpy创建新的tensor："><a href="#2根据numpy创建新的tensor：" class="headerlink" title="2根据numpy创建新的tensor："></a>2根据numpy创建新的tensor：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensor转化为array</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">b = a.numpy()</span><br><span class="line">b</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">       [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]], dtype=float32)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将array转化为tensor</span></span><br><span class="line">x = torch.from_numpy(b)</span><br><span class="line">x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure><p>不过要注意的是：无论是array转化为tensor，还是tensor转化为array，他们都是和原来的数据共享内存的，例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">b+=<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(b)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]</span><br><span class="line"> [<span class="number">2.</span> <span class="number">2.</span> <span class="number">2.</span>]]</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>]])</span><br><span class="line"><span class="comment"># b的数值发生变化，x的数值也发生变化，需要注意</span></span><br></pre></td></tr></table></figure><p>更多详情请查看<a href="https://pytorch.org/docs/stable/tensors.html">官方文档</a><br>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建Tensor的几种方式&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>使用anacond prompt安装pytorch(CPU版)</title>
    <link href="https://du2279664786.github.io/2022/04/16/2022-04-16%E4%BD%BF%E7%94%A8anacond%20prompt%E5%AE%89%E8%A3%85pytorch(CPU%E7%89%88)/"/>
    <id>https://du2279664786.github.io/2022/04/16/2022-04-16%E4%BD%BF%E7%94%A8anacond%20prompt%E5%AE%89%E8%A3%85pytorch(CPU%E7%89%88)/</id>
    <published>2022-04-16T14:55:10.000Z</published>
    <updated>2022-10-18T09:02:26.645Z</updated>
    
    <content type="html"><![CDATA[<p>利用anaconda搭建一个新环境并安装CPU版本的pytorch</p><span id="more"></span><h1 id="使用anacond-prompt安装pytorch"><a href="#使用anacond-prompt安装pytorch" class="headerlink" title="使用anacond prompt安装pytorch"></a>使用anacond prompt安装pytorch</h1><p>GPU版本的教程<a href="https://blog.csdn.net/weixin_51756104/article/details/124398722?spm=1001.2014.3001.5501">请点击此处查看</a><br>·首先去anaconda<a href="https://www.anaconda.com/">官网</a>安装anaconda，然后打开anaconda prompt（pycharm配置环境略过）：<br>在命令行输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conda create –n pytorch python=<span class="number">3.8</span></span><br></pre></td></tr></table></figure><p>来创建一个关于pytorch的单独环境<br>这里的‘pytorch’是环境名称，python&#x3D;3.8是版本，都可以根据需求自行修改<br>然后就可以进入我们创建好的虚拟环境：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate pytorch</span><br></pre></td></tr></table></figure><p>然后我们可以切换一下源：切换源能够更快的下载 包<br>配置清华源，在命令行输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls yes</span><br></pre></td></tr></table></figure><p>然后一切准备工作做好之后，接下来开始安装pytorch，在命令行输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch torchvision torchaudio cpuonly -c pytorch   </span><br></pre></td></tr></table></figure><p>输入之后等待一会再输入‘y’（确认下载）<br>最后在命令行输入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="built_in">list</span></span><br></pre></td></tr></table></figure><p>然后会列出这个环境所有的包，可以查看有没有‘torch’<br><img src="https://img-blog.csdnimg.cn/dc71bab57e4141bab8c166d8686d310d.png" alt="休闲就"><br>出现torch就代表成功了！<br>欢迎关注作者，有什么问题可以一起讨论！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;利用anaconda搭建一个新环境并安装CPU版本的pytorch&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>快速安装python包</title>
    <link href="https://du2279664786.github.io/2021/12/08/2021-12-08%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85python%E5%8C%85/"/>
    <id>https://du2279664786.github.io/2021/12/08/2021-12-08%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85python%E5%8C%85/</id>
    <published>2021-12-08T14:55:10.000Z</published>
    <updated>2022-10-18T08:58:45.854Z</updated>
    
    <content type="html"><![CDATA[<p>使用源快速安装python包</p><span id="more"></span><p>使用以下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple    </span><br></pre></td></tr></table></figure><p>在最后加上你要安装的包，就OK了<br>例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple pandas</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;使用源快速安装python包&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="逻辑回归" scheme="https://du2279664786.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>Python修改论文的字体及其大小</title>
    <link href="https://du2279664786.github.io/2021/12/01/2021-12-1Python%E4%BF%AE%E6%94%B9%E8%AE%BA%E6%96%87%E7%9A%84%E5%AD%97%E4%BD%93%E5%8F%8A%E5%85%B6%E5%A4%A7%E5%B0%8F/"/>
    <id>https://du2279664786.github.io/2021/12/01/2021-12-1Python%E4%BF%AE%E6%94%B9%E8%AE%BA%E6%96%87%E7%9A%84%E5%AD%97%E4%BD%93%E5%8F%8A%E5%85%B6%E5%A4%A7%E5%B0%8F/</id>
    <published>2021-12-01T14:55:10.000Z</published>
    <updated>2022-10-18T08:57:16.043Z</updated>
    
    <content type="html"><![CDATA[<p>使用python读取数据并修改文章字体相关格式和大小</p><span id="more"></span><h2 id="对标题的格式修改"><a href="#对标题的格式修改" class="headerlink" title="对标题的格式修改"></a>对标题的格式修改</h2><p>·首先是导入包和读取word文档</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> docx <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> docx.shared <span class="keyword">import</span> Pt, RGBColor  <span class="comment"># 字号，颜色</span></span><br><span class="line"><span class="keyword">from</span> docx.oxml.ns <span class="keyword">import</span> qn  <span class="comment"># 中文字体</span></span><br><span class="line"></span><br><span class="line">file = Document(<span class="string">&quot;E:\\File\\大一\\大一下学期/马克思.docx&quot;</span>)</span><br></pre></td></tr></table></figure><p>然后对字体进行修改</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> file.paragraphs:</span><br><span class="line">    <span class="keyword">if</span> re.<span class="keyword">match</span>(<span class="string">&#x27;^Heading \d+$&#x27;</span>, run.style.name):   <span class="comment"># 找出所有标题</span></span><br><span class="line">        <span class="keyword">for</span> kuai <span class="keyword">in</span> run.runs:</span><br><span class="line">            kuai._element.rPr.rFonts.<span class="built_in">set</span>(qn(<span class="string">&#x27;w:eastAsia&#x27;</span>), <span class="string">&#x27;黑体&#x27;</span>)</span><br><span class="line">            kuai.font.size = Pt(<span class="number">42</span>)   <span class="comment"># 修改字号</span></span><br><span class="line">            kuai.font.bold = <span class="literal">True</span>  <span class="comment"># 加粗</span></span><br></pre></td></tr></table></figure><h2 id="对文章内容对修改"><a href="#对文章内容对修改" class="headerlink" title="对文章内容对修改"></a>对文章内容对修改</h2><p>·对正文的修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> file.paragraphs:</span><br><span class="line">    <span class="keyword">if</span> run.style.name == <span class="string">&quot;Normal&quot;</span>:</span><br><span class="line">        <span class="keyword">for</span> kuai <span class="keyword">in</span> run.runs:</span><br><span class="line">            kuai._element.rPr.rFonts.<span class="built_in">set</span>(qn(<span class="string">&#x27;w:eastAsia&#x27;</span>), <span class="string">&#x27;黑体&#x27;</span>)</span><br><span class="line">            kuai.font.size = Pt(<span class="number">42</span>)</span><br><span class="line">            <span class="built_in">print</span>(run.text)</span><br></pre></td></tr></table></figure><p>由于是对整篇论文进行修改，所以又改变了一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">title = []    <span class="comment"># 存入非段落内容数据</span></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> file.paragraphs:</span><br><span class="line">    <span class="keyword">if</span> re.<span class="keyword">match</span>(<span class="string">&#x27;^Heading \d+$&#x27;</span>, run.style.name):</span><br><span class="line">        title.append(run.text)</span><br><span class="line">    <span class="keyword">elif</span> run.style.name == <span class="string">&quot;Normal&quot;</span>:</span><br><span class="line">        title.append(run.text)</span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> file.paragraphs:</span><br><span class="line">    <span class="keyword">if</span> run.text <span class="keyword">not</span> <span class="keyword">in</span> title:</span><br><span class="line">        <span class="keyword">for</span> kuai <span class="keyword">in</span> run.runs:</span><br><span class="line">            <span class="keyword">if</span> kuai.text <span class="keyword">not</span> <span class="keyword">in</span> title:</span><br><span class="line">                <span class="built_in">print</span>(kuai)</span><br><span class="line">                <span class="comment"># kuai.font.size = Pt(42)</span></span><br><span class="line">                kuai.font.color.rgb = RGBColor(<span class="number">200</span>, <span class="number">100</span> , <span class="number">200</span>)</span><br><span class="line">                <span class="comment"># kuai.font.name = &#x27;Arial&#x27;</span></span><br><span class="line">                <span class="comment"># kuai._element.rPr.rFonts.set(qn(&#x27;w:eastAsia&#x27;),&#x27;黑体&#x27;)</span></span><br></pre></td></tr></table></figure><p>最后不要忘记保存文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file.save(<span class="string">&quot;E:/aa.docx&quot;</span>)</span><br></pre></td></tr></table></figure><p>(在设计全文数据库系统项目中，将论文进行最后的标准化)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;使用python读取数据并修改文章字体相关格式和大小&lt;/p&gt;</summary>
    
    
    
    <category term="Python办公自动化" scheme="https://du2279664786.github.io/categories/Python%E5%8A%9E%E5%85%AC%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
    <category term="Python办公自动化" scheme="https://du2279664786.github.io/tags/Python%E5%8A%9E%E5%85%AC%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>python读取word文档</title>
    <link href="https://du2279664786.github.io/2021/11/29/2021-11-29python%E8%AF%BB%E5%8F%96word%E6%96%87%E6%A1%A3/"/>
    <id>https://du2279664786.github.io/2021/11/29/2021-11-29python%E8%AF%BB%E5%8F%96word%E6%96%87%E6%A1%A3/</id>
    <published>2021-11-29T14:55:10.000Z</published>
    <updated>2022-10-18T11:28:45.245Z</updated>
    
    <content type="html"><![CDATA[<p>读取文章并输出各级标题</p><span id="more"></span><h2 id="读取文件内容"><a href="#读取文件内容" class="headerlink" title="读取文件内容"></a>读取文件内容</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> docx <span class="keyword">import</span> Document</span><br><span class="line">file = Document(<span class="string">&quot;E:\\File\\大一\\大一下学期/马克思.docx&quot;</span>)</span><br></pre></td></tr></table></figure><p>我们直接输出文章内容是不可以的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(file)</span><br><span class="line"><span class="comment"># &lt;docx.document.Document object at 0x000002686EE048C0&gt;</span></span><br></pre></td></tr></table></figure><p>我们可以使用循环的方式进行输出text文本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出内容</span></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> file.paragraphs:</span><br><span class="line">    <span class="built_in">print</span>(run.text)</span><br></pre></td></tr></table></figure><h2 id="输出文章的标题"><a href="#输出文章的标题" class="headerlink" title="输出文章的标题"></a>输出文章的标题</h2><h3 id="输出文章的1级标题"><a href="#输出文章的1级标题" class="headerlink" title="输出文章的1级标题"></a>输出文章的1级标题</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> file.paragraphs:</span><br><span class="line">    <span class="keyword">if</span> run.style.name == <span class="string">&quot;Heading 1&quot;</span>:    <span class="comment"># &#x27;Heading 2&#x27; 表示二级标题...</span></span><br><span class="line">        <span class="built_in">print</span>(run.text)</span><br></pre></td></tr></table></figure><h3 id="输出文章的多级标题"><a href="#输出文章的多级标题" class="headerlink" title="输出文章的多级标题"></a>输出文章的多级标题</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出所有标题</span></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> file.paragraphs:</span><br><span class="line">    <span class="keyword">if</span> re.<span class="keyword">match</span>(<span class="string">&#x27;^Heading \d+$&#x27;</span>, run.style.name):</span><br><span class="line">        <span class="built_in">print</span>(run.text)</span><br></pre></td></tr></table></figure><h2 id="输出正文"><a href="#输出正文" class="headerlink" title="输出正文"></a>输出正文</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出正文</span></span><br><span class="line"><span class="keyword">for</span> run <span class="keyword">in</span> file.paragraphs:</span><br><span class="line">    <span class="keyword">if</span> run.style.name == <span class="string">&quot;Normal&quot;</span>:</span><br><span class="line">        <span class="built_in">print</span>(run.text)</span><br></pre></td></tr></table></figure><h2 id="输出段落内容："><a href="#输出段落内容：" class="headerlink" title="输出段落内容："></a>输出段落内容：</h2><h3 id="输出一段的内容"><a href="#输出一段的内容" class="headerlink" title="输出一段的内容"></a>输出一段的内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">file.paragraphs[<span class="number">0</span>].text</span><br><span class="line"><span class="comment"># 如果该段为空格或者其他非段落内容，则输出这一行</span></span><br></pre></td></tr></table></figure><h3 id="输出所有段落的内容"><a href="#输出所有段落的内容" class="headerlink" title="输出所有段落的内容"></a>输出所有段落的内容</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(file.paragraphs)):</span><br><span class="line">    <span class="built_in">print</span>(i, file.paragraphs[<span class="number">0</span>].text)</span><br></pre></td></tr></table></figure><h2 id="保存文章"><a href="#保存文章" class="headerlink" title="保存文章"></a>保存文章</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file.save(<span class="string">&quot;E:/aa.docx&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;读取文章并输出各级标题&lt;/p&gt;</summary>
    
    
    
    <category term="Python办公自动化" scheme="https://du2279664786.github.io/categories/Python%E5%8A%9E%E5%85%AC%E8%87%AA%E5%8A%A8%E5%8C%96/"/>
    
    
    <category term="Python" scheme="https://du2279664786.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>KNN实现手写字体的识别</title>
    <link href="https://du2279664786.github.io/2021/11/28/2021-11-28KNN%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E5%AD%97%E4%BD%93%E7%9A%84%E8%AF%86%E5%88%AB/"/>
    <id>https://du2279664786.github.io/2021/11/28/2021-11-28KNN%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E5%AD%97%E4%BD%93%E7%9A%84%E8%AF%86%E5%88%AB/</id>
    <published>2021-11-28T14:55:10.000Z</published>
    <updated>2022-10-19T03:25:24.001Z</updated>
    
    <content type="html"><![CDATA[<p>KNN实现对digits数据集分类</p><span id="more"></span><h2 id="KNN算法介绍："><a href="#KNN算法介绍：" class="headerlink" title="KNN算法介绍："></a>KNN算法介绍：</h2><p><a href="http://localhost:4000/2021/09/20/2021-09-20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95-KNN/">点击这里查看KNN算法代码及其介绍</a></p><h2 id="数据的导入："><a href="#数据的导入：" class="headerlink" title="数据的导入："></a>数据的导入：</h2><h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h3 id="导入数据集"><a href="#导入数据集" class="headerlink" title="导入数据集"></a>导入数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 手写字体的数据集导入</span></span><br><span class="line">digtis = datasets.load_digits()</span><br><span class="line">target = digtis.target</span><br><span class="line">data = digtis.data</span><br></pre></td></tr></table></figure><h2 id="数据集介绍："><a href="#数据集介绍：" class="headerlink" title="数据集介绍："></a>数据集介绍：</h2><p>数据集情况：1797条数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.shape, target.shape</span><br><span class="line"><span class="comment"># (1797, 64), (1797,))</span></span><br></pre></td></tr></table></figure><p>对于导入的数据集data里面的每个数据的形状是(64,)，我们可以将其转化为8X8像素的数据，将第一个数据进行可视化展示：<br>形状转换：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ima = data[<span class="number">0</span>].reshape(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">Out：</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">5.</span>, <span class="number">13.</span>,  <span class="number">9.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>, <span class="number">13.</span>, <span class="number">15.</span>, <span class="number">10.</span>, <span class="number">15.</span>,  <span class="number">5.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">3.</span>, <span class="number">15.</span>,  <span class="number">2.</span>,  <span class="number">0.</span>, <span class="number">11.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">4.</span>, <span class="number">12.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">8.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">5.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">9.</span>,  <span class="number">8.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">4.</span>, <span class="number">11.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>, <span class="number">12.</span>,  <span class="number">7.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">2.</span>, <span class="number">14.</span>,  <span class="number">5.</span>, <span class="number">10.</span>, <span class="number">12.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">6.</span>, <span class="number">13.</span>, <span class="number">10.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure><p>可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.imshow(ima)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/9266273c20ce45b6baf6ddc299ce7eb9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5bCP55qu6bq76Iqx,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="数据集的分割："><a href="#数据集的分割：" class="headerlink" title="数据集的分割："></a>数据集的分割：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="定义KNN函数："><a href="#定义KNN函数：" class="headerlink" title="定义KNN函数："></a>定义KNN函数：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">knn_code</span>(<span class="params">loc, k=<span class="number">5</span>, order=<span class="number">2</span></span>):  <span class="comment"># k order是超参</span></span><br><span class="line">    <span class="comment"># print(order)</span></span><br><span class="line">    diff_loc = x_train - loc</span><br><span class="line">    dis_loc = np.linalg.norm(diff_loc, <span class="built_in">ord</span>=order, axis=<span class="number">1</span>)  <span class="comment"># 没有axis得到一个数，矩阵的泛数。axis=0，得到两个数</span></span><br><span class="line">    knn = y_train[dis_loc.argsort()[:k]]</span><br><span class="line">    counts = np.bincount(knn)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(counts)</span><br></pre></td></tr></table></figure><h2 id="评估准确率："><a href="#评估准确率：" class="headerlink" title="评估准确率："></a>评估准确率：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">res = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_test:</span><br><span class="line">    res.append(knn_code(i))</span><br><span class="line"></span><br><span class="line">acc = ((y_test == pd.Series(res))==<span class="literal">True</span>).<span class="built_in">sum</span>()/<span class="built_in">len</span>(y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率：&quot;</span>, acc)</span><br><span class="line"><span class="comment"># 准确率： 0.9944444444444445</span></span><br></pre></td></tr></table></figure><h2 id="完整代码："><a href="#完整代码：" class="headerlink" title="完整代码："></a>完整代码：</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># datetime:2021/11/22 22:54</span></span><br><span class="line"><span class="comment"># software: PyCharm</span></span><br><span class="line"><span class="comment"># 导入包</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写字体的数据集导入</span></span><br><span class="line">digtis = datasets.load_digits()</span><br><span class="line">target = digtis.target</span><br><span class="line">data = digtis.data</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化展示</span></span><br><span class="line">ima = data[<span class="number">0</span>].reshape(<span class="number">8</span>, <span class="number">8</span>)</span><br><span class="line">plt.imshow(ima)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集分割</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KNN函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">knn_code</span>(<span class="params">loc, k=<span class="number">5</span>, order=<span class="number">2</span></span>):  <span class="comment"># k order是超参</span></span><br><span class="line">    <span class="comment"># print(order)</span></span><br><span class="line">    diff_loc = x_train - loc</span><br><span class="line">    dis_loc = np.linalg.norm(diff_loc, <span class="built_in">ord</span>=order, axis=<span class="number">1</span>)  <span class="comment"># 没有axis得到一个数，矩阵的泛数。axis=0，得到两个数</span></span><br><span class="line">    knn = y_train[dis_loc.argsort()[:k]]</span><br><span class="line">    counts = np.bincount(knn)</span><br><span class="line">    <span class="keyword">return</span> np.argmax(counts)</span><br><span class="line"></span><br><span class="line"><span class="comment"># acc</span></span><br><span class="line">res = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x_test:</span><br><span class="line">    res.append(knn_code(i))</span><br><span class="line"></span><br><span class="line">acc = ((y_test == pd.Series(res))==<span class="literal">True</span>).<span class="built_in">sum</span>()/<span class="built_in">len</span>(y_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;准确率：&quot;</span>, acc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;KNN实现对digits数据集分类&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="KNN" scheme="https://du2279664786.github.io/tags/KNN/"/>
    
  </entry>
  
</feed>
