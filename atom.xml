<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>江东的笔记</title>
  
  <subtitle>Be overcome difficulties is victory</subtitle>
  <link href="https://du2279664786.github.io/atom.xml" rel="self"/>
  
  <link href="https://du2279664786.github.io/"/>
  <updated>2022-11-17T06:44:32.842Z</updated>
  <id>https://du2279664786.github.io/</id>
  
  <author>
    <name>江东</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="https://du2279664786.github.io/2022/11/17/hello-world/"/>
    <id>https://du2279664786.github.io/2022/11/17/hello-world/</id>
    <published>2022-11-17T14:55:10.000Z</published>
    <updated>2022-11-17T06:44:32.842Z</updated>
    
    <content type="html"><![CDATA[<div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["大家好，这是我的第一篇文章，欢迎查看"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><span id="more"></span><div id="binft"></div>  <script>    var binft = function (r) {      function t() {        return b[Math.floor(Math.random() * b.length)]      }        function e() {        return String.fromCharCode(94 * Math.random() + 33)      }      function n(r) {        for (var n = document.createDocumentFragment(), i = 0; r > i; i++) {          var l = document.createElement("span");          l.textContent = e(), l.style.color = t(), n.appendChild(l)        }        return n      }      function i() {        var t = o[c.skillI];        c.step ? c.step-- : (c.step = g, c.prefixP < l.length ? (c.prefixP >= 0 && (c.text += l[c.prefixP]), c.prefixP++) : "forward" === c.direction ? c.skillP < t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = "backward", c.delay = a) : c.skillP > 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = "forward")), r.textContent = c.text, r.appendChild(n(c.prefixP < l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)      }      var l = "",      o = ["青青陵上柏，磊磊涧中石。", "人生天地间，忽如远行客。","斗酒相娱乐，聊厚不为薄。", "驱车策驽马，游戏宛与洛。","洛中何郁郁，冠带自相索。","长衢罗夹巷，王侯多第宅。","两宫遥相望，双阙百余尺。","极宴娱心意，戚戚何所迫？"].map(function (r) {      return r + ""      }),      a = 2,      g = 1,      s = 5,      d = 75,      b = ["rgb(110,64,170)", "rgb(150,61,179)", "rgb(191,60,175)", "rgb(228,65,157)", "rgb(254,75,131)", "rgb(255,94,99)", "rgb(255,120,71)", "rgb(251,150,51)", "rgb(226,183,47)", "rgb(198,214,60)", "rgb(175,240,91)", "rgb(127,246,88)", "rgb(82,246,103)", "rgb(48,239,130)", "rgb(29,223,163)", "rgb(26,199,194)", "rgb(35,171,216)", "rgb(54,140,225)", "rgb(76,110,219)", "rgb(96,84,200)"],      c = {        text: "",        prefixP: -s,        skillI: 0,        skillP: 0,        direction: "forward",        delay: a,        step: g      };      i()      };      binft(document.getElementById('binft'));  </script><h1 id="欢迎查看我的第一篇文章"><a href="#欢迎查看我的第一篇文章" class="headerlink" title="欢迎查看我的第一篇文章"></a>欢迎查看我的第一篇文章</h1><p>写在前面：这是我的Github，欢迎star：<a href="https://github.com/du2279664786">https://github.com/du2279664786</a></p><p>大一的时候搭建过一次博客，但是由于长时间不使用，导致配置文件丢失<br>所以在大三上学期我又重新搭建了一下，将去年（大二）所学的知识进行了“简单”的汇总（PS：李明虎老师讲的实在是太丰富了，我只整理了简单的）</p><p>本博客为学习博客，旨在记录自己的学习经历和知识回顾</p><p>转眼间已经大三了，回顾过去两年：<br>        ·大一上：人工智能导论<br>        ·大一下：爬虫<br>        ·大二上：机器学习<br>        ·大二下：深度学习和神经网络<br>….. …..</p><p>希望考研可以成功上岸！</p>]]></content>
    
    
    <summary type="html">&lt;div id=&quot;binft&quot;&gt;&lt;/div&gt;
  &lt;script&gt;
    var binft = function (r) {
      function t() {
        return b[Math.floor(Math.random() * b.length)]
      }  
      function e() {
        return String.fromCharCode(94 * Math.random() + 33)
      }
      function n(r) {
        for (var n = document.createDocumentFragment(), i = 0; r &gt; i; i++) {
          var l = document.createElement(&quot;span&quot;);
          l.textContent = e(), l.style.color = t(), n.appendChild(l)
        }
        return n
      }
      function i() {
        var t = o[c.skillI];
        c.step ? c.step-- : (c.step = g, c.prefixP &lt; l.length ? (c.prefixP &gt;= 0 &amp;&amp; (c.text += l[c.prefixP]), c.prefixP++) : &quot;forward&quot; === c.direction ? c.skillP &lt; t.length ? (c.text += t[c.skillP], c.skillP++) : c.delay ? c.delay-- : (c.direction = &quot;backward&quot;, c.delay = a) : c.skillP &gt; 0 ? (c.text = c.text.slice(0, -1), c.skillP--) : (c.skillI = (c.skillI + 1) % o.length, c.direction = &quot;forward&quot;)), r.textContent = c.text, r.appendChild(n(c.prefixP &lt; l.length ? Math.min(s, s + c.prefixP) : Math.min(s, t.length - c.skillP))), setTimeout(i, d)
      }
      var l = &quot;&quot;,
      o = [&quot;大家好，这是我的第一篇文章，欢迎查看&quot;].map(function (r) {
      return r + &quot;&quot;
      }),
      a = 2,
      g = 1,
      s = 5,
      d = 75,
      b = [&quot;rgb(110,64,170)&quot;, &quot;rgb(150,61,179)&quot;, &quot;rgb(191,60,175)&quot;, &quot;rgb(228,65,157)&quot;, &quot;rgb(254,75,131)&quot;, &quot;rgb(255,94,99)&quot;, &quot;rgb(255,120,71)&quot;, &quot;rgb(251,150,51)&quot;, &quot;rgb(226,183,47)&quot;, &quot;rgb(198,214,60)&quot;, &quot;rgb(175,240,91)&quot;, &quot;rgb(127,246,88)&quot;, &quot;rgb(82,246,103)&quot;, &quot;rgb(48,239,130)&quot;, &quot;rgb(29,223,163)&quot;, &quot;rgb(26,199,194)&quot;, &quot;rgb(35,171,216)&quot;, &quot;rgb(54,140,225)&quot;, &quot;rgb(76,110,219)&quot;, &quot;rgb(96,84,200)&quot;],
      c = {
        text: &quot;&quot;,
        prefixP: -s,
        skillI: 0,
        skillP: 0,
        direction: &quot;forward&quot;,
        delay: a,
        step: g
      };
      i()
      };
      binft(document.getElementById(&#39;binft&#39;));
  &lt;/script&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>什么是Warmup</title>
    <link href="https://du2279664786.github.io/2022/11/16/2022-11-16Warmup/"/>
    <id>https://du2279664786.github.io/2022/11/16/2022-11-16Warmup/</id>
    <published>2022-11-16T14:55:10.000Z</published>
    <updated>2022-11-17T07:34:31.067Z</updated>
    
    <content type="html"><![CDATA[<p>Warmup可以逐渐地将学习率从一个小的值提升到一个大的值</p><span id="more"></span><h1 id="Warmup"><a href="#Warmup" class="headerlink" title="Warmup"></a>Warmup</h1><h3 id="什么是预热学习率-Warmup-？"><a href="#什么是预热学习率-Warmup-？" class="headerlink" title="什么是预热学习率(Warmup)？"></a>什么是预热学习率(Warmup)？</h3><p>先来看一下预热的定义：预热指的是为防止急热，焊接前先对材料预热。</p><p>而预热学习率指的是对学习率进行控制，使学习率缓慢的变化，而不是直接变为设定值。</p><p>下面我们通过一个具体的例子来展示Warmup：</p><p>设置初始化lr &#x3D; 0.1,假设所有步数num_step &#x3D; 20000,warmup_step &#x3D; 5000，即当步数达到5000时，达到所设置的学习率</p><p>实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">warmup_steps = <span class="number">5000</span></span><br><span class="line">init_lr = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 模拟训练20000步</span></span><br><span class="line">max_steps = <span class="number">20000</span></span><br><span class="line">a = []</span><br><span class="line"><span class="keyword">for</span> train_steps <span class="keyword">in</span> <span class="built_in">range</span>(max_steps):</span><br><span class="line">    <span class="keyword">if</span> warmup_steps <span class="keyword">and</span> train_steps &lt; warmup_steps:</span><br><span class="line">        warmup_percent_done = train_steps / warmup_steps</span><br><span class="line">        warmup_learning_rate = init_lr * warmup_percent_done  <span class="comment">#gradual warmup_lr</span></span><br><span class="line">        learning_rate = warmup_learning_rate</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment">#learning_rate = np.sin(learning_rate)  #预热学习率结束后,学习率呈sin衰减</span></span><br><span class="line">        learning_rate = learning_rate**<span class="number">1.0001</span> <span class="comment">#预热学习率结束后,学习率呈指数衰减(近似模拟指数衰减)</span></span><br><span class="line">    <span class="keyword">if</span> (train_steps+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">             <span class="built_in">print</span>(<span class="string">&quot;train_steps:%.3f--warmup_steps:%.3f--learning_rate:%.3f&quot;</span> % (</span><br><span class="line">                 train_steps+<span class="number">1</span>,warmup_steps,learning_rate))</span><br><span class="line">    a.append(learning_rate)</span><br><span class="line">plt.plot(a)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>可得学习率变化如下：</p><p><img src="https://img-blog.csdnimg.cn/ddd9decebde845009dbf4aad2a84ac4a.png"></p><h3 id="为什么要进行预热学习率"><a href="#为什么要进行预热学习率" class="headerlink" title="为什么要进行预热学习率"></a>为什么要进行预热学习率</h3><p>由于刚开始训练时,模型的权重(weights)是随机初始化的，此时若选择一个较大的学习率,可能带来模型的不稳定(振荡)，选择Warmup预热学习率的方式，可以使得开始训练的几个epoches或者一些steps内学习率较小,在预热的小学习率下，模型可以慢慢趋于稳定,等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Warmup可以逐渐地将学习率从一个小的值提升到一个大的值&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中的MASK理解</title>
    <link href="https://du2279664786.github.io/2022/11/15/2022-11-14%E5%AF%B9Transformer%E4%B8%AD%E7%9A%84MASK%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/11/15/2022-11-14%E5%AF%B9Transformer%E4%B8%AD%E7%9A%84MASK%E7%90%86%E8%A7%A3/</id>
    <published>2022-11-15T14:55:10.000Z</published>
    <updated>2022-11-17T06:58:24.681Z</updated>
    
    <content type="html"><![CDATA[<p>Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分</p><span id="more"></span><p>上一篇文章我们介绍了<a href="https://blog.csdn.net/weixin_51756104/article/details/127250190?spm=1001.2014.3001.5501">对Transformer中FeedForward层的理解</a>，今天我们来介绍一下对MASK的理解<br>老规矩，还是先放一张Transformer的图片<br><img src="https://img-blog.csdnimg.cn/de74182b27f24a84a28fdd5f7204f0cd.png" alt="在这里插入图片描述"><br>Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，而所谓的MASK在Encoder和Decoder两个结构中都有使用。</p><p>Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分</p><h1 id="Padding-Masked"><a href="#Padding-Masked" class="headerlink" title="Padding Masked"></a>Padding Masked</h1><p>对于Transformer而言，每次的输入为：[batch_size,seq_length,d_module]结构，由于句子一般是长短不一的，而输入的数据需要是固定的格式，所以要对句子进行处理。<br>通常会把每个句子按照最大长度进行补齐，所以当句子不够长时，需要进行补0操作，以保证输入数据结构的完整性<br>但是在计算注意力机制时的Softmax函数时，就会出现问题，Padding数值为0的话，仍然会影响到Softmax的计算结果，即无效数据参加了运算。<br>为了不让Padding数据产生影响，通常会将Padding数据变为负无穷，这样的话就不会影响Softmax函数了</p><h1 id="Sequence-Masked"><a href="#Sequence-Masked" class="headerlink" title="Sequence Masked"></a>Sequence Masked</h1><p>Sequence Masked只发生在Decoder操作中，在Decoder中，我们的预测是一个一个进行的，即输入一个token，输出下一个token，在网上看到一个很好的解释如下：<br>假设我们当前在进行机器翻译<br>    输入：我很好<br>    输出：I am fine<br>接下来是Decoder执行步骤<br>第一步：<br>    ·初始输入： 起始符</s> + Positional Encoding（位置编码）<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“I”<br>第二步：<br>    ·初始输入：起始符</s> + “I”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“am”<br>第三步：<br>    ·初始输入：起始符</s> + “I”+ “am”+ Positonal Encoding<br>    ·中间输入：（我很好）Encoder Embedding<br>    ·最终输出：产生预测“fine”<br>上面就是Decoder的执行过程，在预测出“I”之前，我们是不可能知道‘am’的，所以要将数进行Mask，防止看到当前值后面的值，如下图所示：当我们仅知道start的时候，后面的关系是不知道的，所以start和“I”以及其它单词的Score都为负无穷，当预测出“I”之后，再去预测“am”，最终得到下面第三个得分矩阵。<br><img src="https://img-blog.csdnimg.cn/8df2f100c0a447909a8f9d99cbf86a1d.png"></p><p>最后经过Softmax处理之后，得到最终的得分矩阵<br><img src="https://img-blog.csdnimg.cn/90d840807bf04b89bf8e32dd6bf19e85.png"></p><p>最后不要忘了Decoder中依然采用的是Masked Multi-Head Attention，即多次进行Mask机制</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Transformer中的MASK主要分为两部分：Padding Mask和Sequence Mask两部分&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="Mask" scheme="https://du2279664786.github.io/tags/Mask/"/>
    
  </entry>
  
  <entry>
    <title>第五届“泰迪杯”数据分析技能赛</title>
    <link href="https://du2279664786.github.io/2022/11/13/2022-11-13%E7%AC%AC%E4%BA%94%E5%B1%8A%E2%80%9C%E6%B3%B0%E8%BF%AA%E6%9D%AF%E2%80%9D%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD%E8%B5%9B/"/>
    <id>https://du2279664786.github.io/2022/11/13/2022-11-13%E7%AC%AC%E4%BA%94%E5%B1%8A%E2%80%9C%E6%B3%B0%E8%BF%AA%E6%9D%AF%E2%80%9D%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%8A%80%E8%83%BD%E8%B5%9B/</id>
    <published>2022-11-13T14:55:10.000Z</published>
    <updated>2022-11-17T06:55:58.174Z</updated>
    
    <content type="html"><![CDATA[<p>第五届“泰迪杯”数据分析技能赛复盘</p><span id="more"></span><p>今天刚结束了第五届“泰迪杯”数据分析技能赛，我们选择的B题做，昨天的A题太伤心了，做不出来A题（时间太短，再加上太麻烦，晚上直接跑路去操场看70周年校庆了）</p><p>先上代码：<a href="https://github.com/du2279664786/2022-taidi-competition">https://github.com/du2279664786/2022-taidi-competition</a>，直接在前面仓库可以看到代码和相关数据</p><h1 id="竞赛日程；"><a href="#竞赛日程；" class="headerlink" title="竞赛日程；"></a>竞赛日程；</h1><p>报名起始时间：2022年9月5日-11月10日<br>赛前指导时间：2022年9月13日-11月10日<br>A题竞赛时间：2022年11月12日 8:00-21:00（8:00:00公布赛题）<br>B题竞赛时间：2022年11月13日 8:00-20:00（8:00:00公布赛题）<br>视频答辩时间：2022年12月上旬，具体时间另行通知<br>成绩公示时间：2022年12月中下旬<br>最终成绩公布时间：2022年12月中下旬</p><h1 id="赛题基本情况"><a href="#赛题基本情况" class="headerlink" title="赛题基本情况"></a>赛题基本情况</h1><p>A题基本如下：<br><img src="https://img-blog.csdnimg.cn/fc0e1942da0840e581af2f1a07f92294.png"><br>B题基本如下：<br><img src="https://img-blog.csdnimg.cn/60fe3adf19504c08858bb6e0335dae6c.png"><br><img src="https://img-blog.csdnimg.cn/0bd891ee6bc648db963bbb5be25ff832.png"><br><img src="https://img-blog.csdnimg.cn/32ce46c165fb4f959267a3ee388aa68e.png"><br><img src="https://img-blog.csdnimg.cn/528ac1e435114bd781e6ad1f5de35bb8.png"></p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>昨天做A题，今天做B题，每个题的做题时间是8:00-20:00（8:00:00公布赛题），共12个小时的竞赛时间，完成一篇论文</p><p>可以看出，A题和我们人工智能专业相关性不大，所以昨天我和两个队友基本出于摆烂状态，也没有提交论文，基本放弃了A题，而转身去看建校70周年的节目去了</p><p>然后今天早晨又看了B题，发现可以冲，于是我们三个就去201教室开干，一共写了20页，差不多从9点开干，一直到晚上8点，难度不大，但也需要一定的思考逻辑，模型部分如下图展示：<br><img src="https://img-blog.csdnimg.cn/492c0617f3ce49c68980a8c3d4e23b36.png"><br><img src="https://img-blog.csdnimg.cn/4b25af9f99db44c195bb9ce3f6079b28.png"><br><img src="https://img-blog.csdnimg.cn/b7c75b831b574d3fb7c78f100c022287.png"></p><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>做的还可以，基本都是按照题目要求做的，美中不足就是正第一页排版不大好，其他的都还可以，希望能够摸一个奖</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;第五届“泰迪杯”数据分析技能赛复盘&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="数据挖掘" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数据分析" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>第三届“大湾区杯”粤港澳金融数学建模竞赛</title>
    <link href="https://du2279664786.github.io/2022/11/08/2022-11-08%E7%AC%AC%E4%B8%89%E5%B1%8A%E2%80%9C%E5%A4%A7%E6%B9%BE%E5%8C%BA%E6%9D%AF%E2%80%9D%E7%B2%A4%E6%B8%AF%E6%BE%B3%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B/"/>
    <id>https://du2279664786.github.io/2022/11/08/2022-11-08%E7%AC%AC%E4%B8%89%E5%B1%8A%E2%80%9C%E5%A4%A7%E6%B9%BE%E5%8C%BA%E6%9D%AF%E2%80%9D%E7%B2%A4%E6%B8%AF%E6%BE%B3%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B/</id>
    <published>2022-11-08T14:55:10.000Z</published>
    <updated>2022-11-14T03:05:46.386Z</updated>
    
    <content type="html"><![CDATA[<p>2022年第三届“大湾区杯”粤港澳金融数学建模竞赛复盘</p><span id="more"></span><p>老规矩先上代码仓库：<a href="https://github.com/du2279664786/2022-JinRong-Mathematics-modeling">https://github.com/du2279664786/2022-JinRong-Mathematics-modeling</a></p><h1 id="竞赛日程"><a href="#竞赛日程" class="headerlink" title="竞赛日程"></a>竞赛日程</h1><p>报名时间：10月12日10:00:00-10月30日22:00:00<br>竞赛时间：11月1日10:00:00-11月8日14:00:00</p><h1 id="赛题基本情况"><a href="#赛题基本情况" class="headerlink" title="赛题基本情况"></a>赛题基本情况</h1><p>A题如下<br><img src="https://img-blog.csdnimg.cn/4d314192ae304e3ab220fddc93379418.png"><br>B题如下<br><img src="https://img-blog.csdnimg.cn/1c6b8f2fcc844fc891edbf488f9078df.png"><br>AB两个题都是比较偏向金融的（题目就叫‘金融’数学建模）</p><h1 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h1><p>我们三个都是计算机专业的，对金融一窍不通，刚开始选择的是A题，因为我们感觉A比较偏代码，实践性更强一些，但是当我们做了两天之后发现：A题根本没有头绪，对他们所给的数据也是不会处理，11月3号，还是没有思路，于是我们果断换成了B题，容易水论文<br><img src="https://img-blog.csdnimg.cn/83574b6bd9624a229fb935074e4c808a.png"></p><h1 id="感悟"><a href="#感悟" class="headerlink" title="感悟"></a>感悟</h1><p>整体还行，至少尽力做了，对金融领域不大了解，这也是我们的短处，希望能摸一个奖！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年第三届“大湾区杯”粤港澳金融数学建模竞赛复盘&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数学建模" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
    <category term="金融" scheme="https://du2279664786.github.io/tags/%E9%87%91%E8%9E%8D/"/>
    
  </entry>
  
  <entry>
    <title>中国大学生计算机设计大赛复盘</title>
    <link href="https://du2279664786.github.io/2022/10/27/2022-10-27%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/"/>
    <id>https://du2279664786.github.io/2022/10/27/2022-10-27%E4%B8%AD%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AE%BE%E8%AE%A1%E5%A4%A7%E8%B5%9B/</id>
    <published>2022-10-27T14:55:10.000Z</published>
    <updated>2022-11-07T05:01:37.083Z</updated>
    
    <content type="html"><![CDATA[<p>2022年中国大学生计算机设计大赛-人工智能挑战赛-国家二等奖</p><span id="more"></span><p>此篇文章写于2022年10月27日，为了复盘、回顾上一次的计算机设计大赛<br>中国大学生计算机设计大赛</p><h1 id="日程"><a href="#日程" class="headerlink" title="日程"></a>日程</h1><p>5-6月份开始初赛省赛，我和我的两位队友努力的写文档，整理代码，提交了相关资料，由于初赛（省赛）没有答辩，所以差不多等到六月多收到获得省二的通知<br>7月份多得之进了国赛<br>7月底-8月底就开始修改文档，修改代码，录制相关视频，等待国赛的答辩</p><h1 id="内容"><a href="#内容" class="headerlink" title="内容"></a>内容</h1><p>项目背景：在数字经济蓬勃发展的时代背景下， “数字化”“智慧化”成为了企业管理转型升级的核心引擎。传统建管理模式已不再符合可持续发展的要求，迫切需要利用以信息技术为代表的现代科技手段，实现中国企业管理转型升级与跨越式发展。为了更好的解决时代问题的痛点，并在一定程度上节约人力成本，本项目设计了一个基于云端和硬件的人工智能多场景实时物体监测平台，将多场景实时监测与报警系统进行了融合创新，做到了监控、检测、分类、识别四位一体的平台建设。<br>整体布局：<br><img src="https://img-blog.csdnimg.cn/d652a2b775ee475f951a36c928d2054e.png" alt="在这里插入图片描述"></p><p>算法介绍：<br>移动物体检测：本项目采用OpenCV（Open Source Computer Vision Library）算法，使用灰度转化、图片缩放和高斯滤波等相关操作，对图像进行预处理，增强了移动物体的可检测性。</p><p>物体识别：为提高精确度并且处理时间尽可能短，本项目采用了运算速度比较快的 Yolov3 算法，它是基于深度学习框架Darknet的目标检测开源项目，不仅可以充分发挥多核处理器和 GPU 并行运算的功能，还可以基于预训练模型进行实时目标检测,预期效果如下：<br><img src="https://img-blog.csdnimg.cn/d1d870e51929499fb0637ed6a3ba3703.png" alt="在这里插入图片描述"><br>以及安全帽检测、疲劳监测、口罩检测<br>具体代码仓库如下：<a href="https://github.com/du2279664786/Chinese-undergraduate-computer-design-contest">https://github.com/du2279664786/Chinese-undergraduate-computer-design-contest</a></p><p>最终获得证书：一个省二证书、一个国二证书、外加一枚金匾<br><img src="https://img-blog.csdnimg.cn/736f6b9f339841fc832f0180d06ab5d8.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/99c107c9a863454e8ee0d84169935b90.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年中国大学生计算机设计大赛-人工智能挑战赛-国家二等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="机器视觉" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>数学建模省一思路及其代码</title>
    <link href="https://du2279664786.github.io/2022/10/14/2022-10-14%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%9C%81%E4%B8%80%E6%80%9D%E8%B7%AF%E5%8F%8A%E5%85%B6%E4%BB%A3%E7%A0%81/"/>
    <id>https://du2279664786.github.io/2022/10/14/2022-10-14%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%9C%81%E4%B8%80%E6%80%9D%E8%B7%AF%E5%8F%8A%E5%85%B6%E4%BB%A3%E7%A0%81/</id>
    <published>2022-10-14T14:55:10.000Z</published>
    <updated>2022-11-14T02:42:57.417Z</updated>
    
    <content type="html"><![CDATA[<p>2022年全国大学生数学建模竞赛山东省一等奖</p><span id="more"></span><p>连肝三天，撸了三天代码，9.17早-9.18晚日夜不停，终于和队友拿下了2022年全国大学生数学建模竞赛山东省一等奖<br>基本思路如下：<br><img src="https://img-blog.csdnimg.cn/c6128ba97db94cf489182b2fe05ee6ec.jpeg"><br>下面是代码部分，仓库连接如下：<a href="https://github.com/du2279664786/CUMCM">https://github.com/du2279664786/CUMCM</a><br><img src="https://img-blog.csdnimg.cn/1b6a9e5ed0054967b790696d20f287ce.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2022年全国大学生数学建模竞赛山东省一等奖&lt;/p&gt;</summary>
    
    
    
    <category term="竞赛" scheme="https://du2279664786.github.io/categories/%E7%AB%9E%E8%B5%9B/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="竞赛" scheme="https://du2279664786.github.io/tags/%E7%AB%9E%E8%B5%9B/"/>
    
    <category term="数学建模" scheme="https://du2279664786.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中FeedForward层的理解</title>
    <link href="https://du2279664786.github.io/2022/10/10/2022-10-10%E5%AF%B9Transformer%E4%B8%ADFeedForward%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/10/2022-10-10%E5%AF%B9Transformer%E4%B8%ADFeedForward%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-10T14:55:10.000Z</published>
    <updated>2022-10-30T03:04:33.367Z</updated>
    
    <content type="html"><![CDATA[<p>在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强</p><span id="more"></span><p>上一篇我们介绍了<a href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/">对Add&amp;Norm层的理解</a>，有不大熟悉的可以看一下上篇文章。</p><p>今天来说一下Transformer中FeedForward层，首先还是先来回顾一下Transformer的基本结构：首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，然后又做了一个ADD&amp;Norm，再通过Feed Forward进行输出。<br><img src="https://img-blog.csdnimg.cn/4af25c021fd14b70aa2648a925fadf54.png"><br>FeedForward的输入是什么呢？是Multi-Head Attention的输出做了残差连接和Norm之后得数据，然后FeedForward做了两次线性线性变换，为的是更加深入的提取特征。<br><img src="https://img-blog.csdnimg.cn/b976d7add795475fac7bbc6c5f01121f.png" alt="在这里插入图片描述"><br>可以看出在每次线性变换都引入了非线性激活函数Relu，在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强，FeedForward的计算公式如下：max相当于Relu<br><img src="https://img-blog.csdnimg.cn/43b6886add22435795f3f8a7a889c58e.png" alt="在这里插入图片描述"></p><p>所以FeedForward的作用是：通过线性变换，先将数据映射到高纬度的空间再映射到低纬度的空间，提取了更深层次的特征</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在Multi-Head Attention中，主要是进行矩阵乘法，即都是线性变换，而线性变换的学习能力不如非线性变换的学习能力强&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Add&amp;Norm层的理解</title>
    <link href="https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&amp;Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/09/2022-10-09%E5%AF%B9Transformer%E4%B8%ADAdd&amp;Norm%E5%B1%82%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-09T14:55:10.000Z</published>
    <updated>2022-10-18T11:40:29.638Z</updated>
    
    <content type="html"><![CDATA[<p>无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><span id="more"></span><h1 id="Add操作"><a href="#Add操作" class="headerlink" title="Add操作"></a>Add操作</h1><p>首先我们还是先来回顾一下Transformer的结构：Transformer结构主要分为两大部分，一是Encoder层结构，另一个则是Decoder层结构，Encoder 的输入由 Input Embedding 和 Positional Embedding 求和输入Multi-Head-Attention，再通过Feed Forward进行输出。</p><p>由下图可以看出：在Encoder层和Decoder层中都用到了Add&amp;Norm操作，即残差连接和层归一化操作。<br><img src="https://img-blog.csdnimg.cn/5ef722ad3c5b407482ac132b0883c59c.png" alt="在这里插入图片描述"><br>什么是残差连接呢？残差连接就是把网络的输入和输出相加，即网络的输出为F(x)+x，在网络结构比较深的时候，网络梯度反向传播更新参数时，容易造成梯度消失的问题，但是如果每层的输出都加上一个x的时候，就变成了F(x)+x，对x求导结果为1，所以就相当于每一层求导时都加上了一个常数项‘1’，有效解决了梯度消失问题。</p><h1 id="Norm操作"><a href="#Norm操作" class="headerlink" title="Norm操作"></a>Norm操作</h1><p>首先要明白Norm做了一件什么事，从刚开始接触Transformer开始，我认为所谓的Norm就是BatchNorm，但是有一天我看到了这篇<a href="https://mp.weixin.qq.com/s/HNCl6MPS_hjTVHNt7UkYyw">文章</a>，才明白了Norm是什么。</p><p>假设我们输入的词向量的形状是（2，3，4），2为批次（batch），3为句子长度，4为词向量的维度，生成以下数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[w11, w12, w13, w14], [w21, w22, w23, w24], [w31, w32, w33, w34]</span><br><span class="line">[w41, w42, w43, w44], [w51, w52, w53, w54], [w61, w62, w63, w64]]</span><br></pre></td></tr></table></figure><p>如果是在做BatchNorm（BN）的话，其计算过程如下：BN1&#x3D;(w11+w12+w13+w14+w41+<br>w42+w43+w44)&#x2F;8，同理会得到BN2和BN3，最终得到[BN1,BN2,BN3] 3个mean</p><p>如果是在做LayerNorm（LN）的话，则会进如下计算：LN1&#x3D;(w11+w12+w13+w14+w21+<br>w22+w23+w24+w31+w32+w33+w34)&#x2F;12，同理会得到LN2，最终得到[LN1,LN2]两个mean</p><p>如果是在做InstanceNorm（IN）的话，则会进如下计算：IN1&#x3D;(w11+w12+w13+w14)&#x2F;4，同理会得到IN2，IN3，IN4，IN5，IN6，六个mean，[[IN1，IN2，IN3],[IN4，IN5，IN6]]<br>下图完美的揭示了，这几种Norm<br><img src="https://img-blog.csdnimg.cn/a143d6b41e654fa1849f44580401110c.png" alt="在这里插入图片描述"><br>接下来我们来看一下Transformer中的Norm：首先生成[2,3,4]形状的数据，使用原始的编码方式进行编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> InstanceNorm2d</span><br><span class="line">random_seed = <span class="number">123</span></span><br><span class="line">torch.manual_seed(random_seed)</span><br><span class="line"></span><br><span class="line">batch_size, seq_size, dim = <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span></span><br><span class="line">embedding = torch.randn(batch_size, seq_size, dim)</span><br><span class="line"></span><br><span class="line">layer_norm = torch.nn.LayerNorm(dim, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y: &quot;</span>, layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y:  tensor([[[ <span class="number">1.5524</span>,  <span class="number">0.0155</span>, -<span class="number">0.3596</span>, -<span class="number">1.2083</span>],</span><br><span class="line">         [ <span class="number">0.5851</span>,  <span class="number">1.3263</span>, -<span class="number">0.7660</span>, -<span class="number">1.1453</span>],</span><br><span class="line">         [ <span class="number">0.2864</span>,  <span class="number">0.0185</span>,  <span class="number">1.2388</span>, -<span class="number">1.5437</span>]],</span><br><span class="line">        [[ <span class="number">1.1119</span>, -<span class="number">0.3988</span>,  <span class="number">0.7275</span>, -<span class="number">1.4406</span>],</span><br><span class="line">         [-<span class="number">0.4144</span>, -<span class="number">1.1914</span>,  <span class="number">0.0548</span>,  <span class="number">1.5510</span>],</span><br><span class="line">         [ <span class="number">0.3914</span>, -<span class="number">0.5591</span>,  <span class="number">1.4105</span>, -<span class="number">1.2428</span>]]])</span><br></pre></td></tr></table></figure><p>接下来手动去进行一下编码:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以发现和LayerNorm的结果是一样的，也就是说明Norm是对d_model进行的Norm，会给我们[batch,sqe_length]形状的平均值。<br>加下来进行batch_norm,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer_norm = torch.nn.LayerNorm([seq_size,dim], elementwise_affine = <span class="literal">False</span>)</span><br><span class="line">eps: <span class="built_in">float</span> = <span class="number">0.00001</span></span><br><span class="line">mean = torch.mean(embedding[:, :, :], dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">var = torch.square(embedding[:, :, :] - mean).mean(dim=(-<span class="number">2</span>,-<span class="number">1</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;mean: &quot;</span>, mean.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;y_custom: &quot;</span>, (embedding[:, :, :] - mean) / torch.sqrt(var + eps))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mean:  torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">y_custom:  tensor([[[ <span class="number">1.1822</span>,  <span class="number">0.4419</span>, -<span class="number">0.3196</span>, -<span class="number">1.9889</span>],</span><br><span class="line">         [-<span class="number">0.6677</span>, -<span class="number">0.2537</span>, -<span class="number">0.8151</span>,  <span class="number">1.5143</span>],</span><br><span class="line">         [ <span class="number">0.7174</span>,  <span class="number">1.2147</span>, -<span class="number">0.0852</span>, -<span class="number">0.9403</span>]],</span><br><span class="line">        [[-<span class="number">0.0138</span>,  <span class="number">1.5666</span>, -<span class="number">2.1726</span>,  <span class="number">1.0590</span>],</span><br><span class="line">         [ <span class="number">0.6646</span>,  <span class="number">0.6852</span>, -<span class="number">0.8706</span>, -<span class="number">0.0442</span>],</span><br><span class="line">         [-<span class="number">0.1163</span>,  <span class="number">0.1389</span>,  <span class="number">0.4454</span>, -<span class="number">1.3423</span>]]])</span><br></pre></td></tr></table></figure><p>可以看到BN的计算的mean形状为[2, 1, 1]，并且Norm结果也和上面的两个不一样，这就充分说明了Norm是在对最后一个维度求平均。<br>那么什么又是Instancenorm呢？接下来再来实现一下instancenorm</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">instance_norm = InstanceNorm2d(<span class="number">3</span>, affine=<span class="literal">False</span>)</span><br><span class="line">output = instance_norm(embedding.reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>)) <span class="comment">#InstanceNorm2D需要(N,C,H,W)的shape作为输入</span></span><br><span class="line">layer_norm = torch.nn.LayerNorm(<span class="number">4</span>, elementwise_affine = <span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(layer_norm(embedding))</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">1.1505</span>,  <span class="number">0.5212</span>, -<span class="number">0.1262</span>, -<span class="number">1.5455</span>],</span><br><span class="line">         [-<span class="number">0.6586</span>, -<span class="number">0.2132</span>, -<span class="number">0.8173</span>,  <span class="number">1.6890</span>],</span><br><span class="line">         [ <span class="number">0.6000</span>,  <span class="number">1.2080</span>, -<span class="number">0.3813</span>, -<span class="number">1.4267</span>]],</span><br><span class="line">        [[-<span class="number">0.0861</span>,  <span class="number">1.0145</span>, -<span class="number">1.5895</span>,  <span class="number">0.6610</span>],</span><br><span class="line">         [ <span class="number">0.8724</span>,  <span class="number">0.9047</span>, -<span class="number">1.5371</span>, -<span class="number">0.2400</span>],</span><br><span class="line">         [ <span class="number">0.1507</span>,  <span class="number">0.5268</span>,  <span class="number">0.9785</span>, -<span class="number">1.6560</span>]]])</span><br></pre></td></tr></table></figure><p>可以看出无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！</p><p>如果喜欢文章请点个赞，笔者也是一个刚入门Transformer的小白，一起学习，共同努力。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;无论是layernorm还是instancenorm，还是我们手动去求平均计算其Norm，结果都是一样的，由此我们可以得出一个结论：Layernorm实际上是在做Instancenorm！&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Add&amp;Norm" scheme="https://du2279664786.github.io/tags/Add-Norm/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中self-attention的理解</title>
    <link href="https://du2279664786.github.io/2022/10/08/2022-10-08%E5%AF%B9Transformer%E4%B8%ADself-attention%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/08/2022-10-08%E5%AF%B9Transformer%E4%B8%ADself-attention%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-08T14:55:10.000Z</published>
    <updated>2022-10-18T11:38:45.094Z</updated>
    
    <content type="html"><![CDATA[<p>Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差</p><span id="more"></span><h1 id="什么是self-attention"><a href="#什么是self-attention" class="headerlink" title="什么是self-attention"></a>什么是self-attention</h1><p>首先我们来看一下Transformer架构：对于input输出，首先进行input embedding，然后再进行positional encoding，将两者相加作为Encoder的输入，也就是输如X<img src="https://img-blog.csdnimg.cn/65ef7c5730514e2cac88d332e2588423.png" alt="在这里插入图片描述"><br>何为self-attention？首先我们要明白什么是attention，对于传统的seq2seq任务，例如中-英文翻译，输入中文，得到英文，即source是中文句子（x1 x2 x3）,英文句子是target（y1 y2 y3）<br><img src="https://img-blog.csdnimg.cn/296c379fd77c475ebf77c06fa8e42e59.png" alt="在这里插入图片描述"><br>attention机制发生在target的元素和source中的所有元素之间。简单的将就是attention机制中的权重计算需要target参与，即在上述Encoder-Decoder模型中，Encoder和Decoder两部分都需要参与运算。</p><p>而对于self-attention，它不需要Decoder的参与，而是source内部元素之间发生的运算，对于输入向量X，对其做线性变换，分别得到Q、K、V矩阵<br><img src="https://img-blog.csdnimg.cn/17358bbb1cf641d78117625fb5a00d31.png" alt="在这里插入图片描述"><br>然后去计算attention，Q、K点乘得到初步的权重因子，并对Q、K点乘结果进行放缩，除以sqrt（dk），Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差，最终再加一个softmax就得到了self attention的输出。<br><img src="https://img-blog.csdnimg.cn/07fdbd802d114b7193c70e9fc451ba22.png" alt="在这里插入图片描述"></p><h1 id="Multi–head-attention"><a href="#Multi–head-attention" class="headerlink" title="Multi–head-attention"></a>Multi–head-attention</h1><p>Multi–head-attention使用了多个头进行运算，捕捉到了更多的信息，多头的数量用h表示，一般h&#x3D;8，表示8个头<br><img src="https://img-blog.csdnimg.cn/7cad1e37120b4d40ad64140677452ec1.png" alt="在这里插入图片描述"><br>在输入每个self-attention之前，我们需将输入X均分的分到h个头中，得到Z1-Z7八个头的输出结果。<br><img src="https://img-blog.csdnimg.cn/d5763f726a5c48f292a140f84c0e5200.png" alt="在这里插入图片描述"><br>对于每个头计算相应的attention score，将其进行拼接，再与W0进行一个线性变换，就得到最终输出的Z。<br><img src="https://img-blog.csdnimg.cn/05bfb9eb87bb4f3b8066c8190c0dff0f.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Q、K点乘之后的方差会随着维度的增大而增大，而大的方差会导致极小的梯度，为了防止梯度消失，所以除以sqrt(dk)来减小方差&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>对Transformer中Positional Encoding的理解</title>
    <link href="https://du2279664786.github.io/2022/10/07/2022-10-07%E5%AF%B9Transformer%E4%B8%ADPositional%20Encoding%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>https://du2279664786.github.io/2022/10/07/2022-10-07%E5%AF%B9Transformer%E4%B8%ADPositional%20Encoding%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2022-10-07T14:55:10.000Z</published>
    <updated>2022-10-19T03:21:15.466Z</updated>
    
    <content type="html"><![CDATA[<p>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值</p><span id="more"></span><p>首先来看一下Transformer结构的结构：<br><img src="https://img-blog.csdnimg.cn/c7ce14349b734567a014c4cf679398fe.png" alt="在这里插入图片描述"><br>Transformer是由Encoder和Decoder两大部分组成，首先对于文本特征，需要进行Embedding，由于transformer抛弃了Rnn的结构，不能捕捉到序列的信息，交换单词位置，得到相应的attention也会发生交换，并不会发生数值上的改变，所以要对input进行Positional Encoding。</p><p>Positional encoding和input embedding是同等维度的，所以可以将两者进行相加，的到输入向量<br><img src="https://img-blog.csdnimg.cn/be30b27838dd411c89d793432ff72582.png" alt="在这里插入图片描述"><br>接下来看一些Positional Encoding的计算公式：<br><img src="https://img-blog.csdnimg.cn/6e9a80e756b94a70aeef8a79097eb7a6.png" alt="在这里插入图片描述"><br>其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值，也就是说：对于单个token的d_model维度的词向量，奇数位置取cos，偶数位置取sin，最终的到一个维度和word embedding维度一样的矩阵，接下来可以看一下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_positional_encoding</span>(<span class="params">max_seq_len, embed_dim</span>):</span><br><span class="line">    <span class="comment"># 初始化一个positional encoding</span></span><br><span class="line">    <span class="comment"># embed_dim: 字嵌入的维度</span></span><br><span class="line">    <span class="comment"># max_seq_len: 最大的序列长度</span></span><br><span class="line">    positional_encoding = np.array([</span><br><span class="line">        [pos / np.power(<span class="number">10000</span>, <span class="number">2</span> * i / embed_dim) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embed_dim)] <span class="keyword">if</span> pos != <span class="number">0</span> <span class="keyword">else</span> np.zeros(embed_dim) <span class="keyword">for</span> pos <span class="keyword">in</span> <span class="built_in">range</span>(max_seq_len)])</span><br><span class="line"></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(positional_encoding[<span class="number">1</span>:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i 偶数</span></span><br><span class="line">    positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(positional_encoding[<span class="number">1</span>:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1 奇数</span></span><br><span class="line">    <span class="keyword">return</span> positional_encoding</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">positional_encoding = get_positional_encoding(max_seq_len=<span class="number">100</span>, embed_dim=<span class="number">16</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">sns.heatmap(positional_encoding)</span><br><span class="line">plt.title(<span class="string">&quot;Sinusoidal Function&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;hidden dimension&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;sequence length&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>首先求初始向量：positional_encoding，然后对其奇数列求sin，偶数列求cos：<br><img src="https://img-blog.csdnimg.cn/2da3445d7953426394ab2e46d8820baf.png" alt="在这里插入图片描述"><br>最终得到positional encoding之后的数据可视化：<br><img src="https://img-blog.csdnimg.cn/507cc7ca0ba34f8fb2ec87216689857c.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;其中pos表示token在sequence中的位置，d_model表示词嵌入的维度，i则是range(d_model)中的数值&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="Positional Encoding" scheme="https://du2279664786.github.io/tags/Positional-Encoding/"/>
    
  </entry>
  
  <entry>
    <title>怎么理解预训练模型？</title>
    <link href="https://du2279664786.github.io/2022/10/06/2022-10-06%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F/"/>
    <id>https://du2279664786.github.io/2022/10/06/2022-10-06%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%9F/</id>
    <published>2022-10-06T14:55:10.000Z</published>
    <updated>2022-10-19T03:22:28.472Z</updated>
    
    <content type="html"><![CDATA[<p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性……</p><span id="more"></span><h1 id="什么是预训练"><a href="#什么是预训练" class="headerlink" title="什么是预训练"></a>什么是预训练</h1><p>“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性，然后将其中的共性“移植”到特定任务的模型中，再使用相关特定领域的少量标注数据进行“微调”，这样的话，模型只需要从”共性“出发，去“学习”该特定任务的“特殊”部分即可。</p><h1 id="预训练的思想"><a href="#预训练的思想" class="headerlink" title="预训练的思想"></a>预训练的思想</h1><p>预训练的思想是：模型的参数不再是随机初始化的，而是通过一些任务进行预先训练，得到一套模型参数，然后用这套参数对模型进行初始化，再进行训练</p><h1 id="CV领域的预训练"><a href="#CV领域的预训练" class="headerlink" title="CV领域的预训练"></a>CV领域的预训练</h1><p>首先对于CV领域图片分类任务，常用的深度学习模型是卷积视神经网络，对于多层的卷积神经网络来说，不同的层学到的特征是不同的，为了捕获更多的特征，浅层的感受野较小，所以浅层学到的特征往往是更加通用的，包含更多的像素点的信息，比如一些细粒度的信息：颜色、纹理、边缘等。<br>通常在大规模图片数据上预先获得‘通用特征’，然后再去做下游任务：<br><img src="https://img-blog.csdnimg.cn/455a209bb08941ff8d390ede716556b4.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“预训练“的做法一般是将大量低成本收集的训练数据放在一起，经过某种预训方法去学习其中的共性……&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
    <category term="预训练" scheme="https://du2279664786.github.io/tags/%E9%A2%84%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>HuggingFace的安装和编码</title>
    <link href="https://du2279664786.github.io/2022/10/05/2022-10-05HuggingFace%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A0%81/"/>
    <id>https://du2279664786.github.io/2022/10/05/2022-10-05HuggingFace%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E7%BC%96%E7%A0%81/</id>
    <published>2022-10-05T14:55:10.000Z</published>
    <updated>2022-10-18T11:36:57.563Z</updated>
    
    <content type="html"><![CDATA[<p>模型的加载和编码以及基本的使用功能</p><span id="more"></span><h1 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h1><p>使用以下命令安装：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></table></figure><h1 id="模型的加载"><a href="#模型的加载" class="headerlink" title="模型的加载"></a>模型的加载</h1><p>导入包：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from transformers import BertTokenizer</span><br></pre></td></tr></table></figure><p>加载预训练模型bert-base-chinese，初次加载可能需要较长的时间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#加载预训练字典和分词方法</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(</span><br><span class="line"># 下载或者从本地加载模型</span><br><span class="line">    pretrained_model_name_or_path=&#x27;bert-base-chinese&#x27;,</span><br><span class="line">    force_download=False,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>接下来就可以看到tokenizer的内容：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokenizer</span><br><span class="line"></span><br><span class="line">#print:(name_or_path=&#x27;bert-base-chinese&#x27;, vocab_size=21128, model_max_len=512, is_fast=False, padding_side=&#x27;right&#x27;, truncation_side=&#x27;right&#x27;, special_tokens=&#123;&#x27;unk_token&#x27;: &#x27;[UNK]&#x27;, &#x27;sep_token&#x27;: &#x27;[SEP]&#x27;, &#x27;pad_token&#x27;: &#x27;[PAD]&#x27;, &#x27;cls_token&#x27;: &#x27;[CLS]&#x27;, &#x27;mask_token&#x27;: &#x27;[MASK]&#x27;&#125;)</span><br></pre></td></tr></table></figure><h1 id="进行编码"><a href="#进行编码" class="headerlink" title="进行编码"></a>进行编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sents = [</span><br><span class="line">    &#x27;我在一所学院上大三。&#x27;,</span><br><span class="line">    &#x27;今天天气好，我来图书馆学习。&#x27;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">res= tokenizer.encode(</span><br><span class="line">    # 可以一次编码两个句子，即text和text_pair</span><br><span class="line">    text = sents[0],</span><br><span class="line">    text_pair=sents[1],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    return_tensors=None,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>可以打印出res的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 101为CLS    102为SEP    0为PAD</span><br><span class="line">#print(res):[101, 2769, 1762, 671, 2792, 2110, 7368, 677, 1920, 676, 511, 102, 791, 1921, 1921, 3698, 1962, 8024, 2769, 3341, 1745, 741, 7667, 2110, 739, 511, 102, 0, 0, 0]</span><br></pre></td></tr></table></figure><p>也可以查看编码之后的结果:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.decode(res)</span><br><span class="line"># [CLS] 我 在 一 所 学 院 上 大 三 。 [SEP] 今 天 天 气 好 ， 我 来 图 书 馆 学 习 。 [SEP] [PAD] [PAD] [PAD]</span><br></pre></td></tr></table></figure><h1 id="多功能编码"><a href="#多功能编码" class="headerlink" title="多功能编码"></a>多功能编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">res = tokenizer.encode_plus(</span><br><span class="line">    # 可以一次编码两个句子</span><br><span class="line">    text = sents[0],</span><br><span class="line">    text_pair=sents[1],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    </span><br><span class="line">    # 返回值的类型：tf  pt  np</span><br><span class="line">    return_tensors=None,</span><br><span class="line">    </span><br><span class="line">    # 返回token_type_ids</span><br><span class="line">    return_token_type_ids=True,</span><br><span class="line">    </span><br><span class="line">    # 返回attention_mask</span><br><span class="line">    return_attention_mask = True,</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # 返回length标识长度</span><br><span class="line">    return_length = True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>查看编码的结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">for k,v in res.items():</span><br><span class="line">    print(k,&#x27;:&#x27;,v)</span><br><span class="line">    </span><br><span class="line">tokenizer.decode(res[&#x27;input_ids&#x27;])</span><br><span class="line"></span><br><span class="line"># input_ids:编码后的句子</span><br><span class="line"># token_type_ids:第一个句子和特殊符号的位置是0，第二个句子的位置是1</span><br><span class="line"># attention_mask:pad的位置是0，其它位置是1</span><br><span class="line"># length:返回句子的长度</span><br></pre></td></tr></table></figure><h1 id="将句子批量进行编码"><a href="#将句子批量进行编码" class="headerlink" title="将句子批量进行编码"></a>将句子批量进行编码</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">out = tokenizer.batch_encode_plus(</span><br><span class="line">    batch_text_or_text_pairs=[sents[0],sents[1]],</span><br><span class="line">    </span><br><span class="line">    # 当句子长度大于max_length是选择截断</span><br><span class="line">    truncation=True,</span><br><span class="line">    </span><br><span class="line">    # 一律补pad到max_length长度</span><br><span class="line">    padding=&#x27;max_length&#x27;,</span><br><span class="line">    add_special_tokens = True,</span><br><span class="line">    max_length=30,</span><br><span class="line">    </span><br><span class="line">    # 返回值的类型：tf  pt  np</span><br><span class="line">    return_tensors=None,</span><br><span class="line">    </span><br><span class="line">    # 返回token_type_ids</span><br><span class="line">    return_token_type_ids=True,</span><br><span class="line">    </span><br><span class="line">    # 返回attention_mask</span><br><span class="line">    return_attention_mask = True,</span><br><span class="line">    </span><br><span class="line">    # 返回length标识长度</span><br><span class="line">    return_length = True,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;模型的加载和编码以及基本的使用功能&lt;/p&gt;</summary>
    
    
    
    <category term="自然语言处理" scheme="https://du2279664786.github.io/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="NLP" scheme="https://du2279664786.github.io/tags/NLP/"/>
    
    <category term="HuggingFace" scheme="https://du2279664786.github.io/tags/HuggingFace/"/>
    
  </entry>
  
  <entry>
    <title>深度学习课程回顾</title>
    <link href="https://du2279664786.github.io/2022/07/10/2022-07-10%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/"/>
    <id>https://du2279664786.github.io/2022/07/10/2022-07-10%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/</id>
    <published>2022-07-10T14:55:10.000Z</published>
    <updated>2022-10-21T01:34:08.778Z</updated>
    
    <content type="html"><![CDATA[<p>大二下学期深度学习课程回顾</p><span id="more"></span><h1 id="第1章：深度学习概述"><a href="#第1章：深度学习概述" class="headerlink" title="第1章：深度学习概述"></a>第1章：深度学习概述</h1><p>BP神经网络的反向传播（《深度学习08_小复习.pptx》，链式法则：损失函数，激活函数、信号强度；每多一层多一个sigma符号：也是网络不能太深的原因之一）<br>    参数确定（权值个数和偏置个数）</p><h1 id="第2章：Pytorch简介"><a href="#第2章：Pytorch简介" class="headerlink" title="第2章：Pytorch简介"></a>第2章：Pytorch简介</h1><p>Pytorch安装要点：装python、装CUDA、装gpu版的torch<br>Linux编译安装Python的主要步骤：下载、解压、配置(.&#x2F;configure)、编译安装(make &amp;&amp; make install)<br>Tensor的基本知识：数据方面就是numpy，计算图方面是张量的本质。</p><h1 id="第3章：Pytorch计算图"><a href="#第3章：Pytorch计算图" class="headerlink" title="第3章：Pytorch计算图"></a>第3章：Pytorch计算图</h1><p>《深度学习08_小复习.pptx》<br>给几个公式，可以画出计算图，给计算图，可以复原出公式。<br>给计算图，可以向上算数据，可以算回传梯度<br>Pytorch的计算图结构：1、是否所有的叶子节点都可以设置为需要计算梯度，是否所有的叶子节点都需要计算梯度。2、是否默认所有的计算图都可以回传多次。3、计算图是否一个有向无环图。4、是否默认可以让中间节点保存梯度。5、非叶子节点的导函数是怎么来的？</p><h1 id="第4章：线性回归"><a href="#第4章：线性回归" class="headerlink" title="第4章：线性回归"></a>第4章：线性回归</h1><p>分类与回归，线性回归的基本概念，可用的解法。<br>小批量随机梯度下降法。<br>看网络写方程，看方程画网络。<br>数据加载、损失函数。</p><h1 id="第5章：Softmax回归"><a href="#第5章：Softmax回归" class="headerlink" title="第5章：Softmax回归"></a>第5章：Softmax回归</h1><p>Softmax和交叉熵，代码、计算、业务场景，<br>权值、偏置初始化<br>view()的业务意义</p><h1 id="第6章：多层感知机"><a href="#第6章：多层感知机" class="headerlink" title="第6章：多层感知机"></a>第6章：多层感知机</h1><p>单层和多层的区别（非线性激活函数的意义）<br>权值、偏置初始化，MLP的完备性<br>Relu激活函数</p><h1 id="第7章：模型训练与深度学习计算"><a href="#第7章：模型训练与深度学习计算" class="headerlink" title="第7章：模型训练与深度学习计算"></a>第7章：模型训练与深度学习计算</h1><p>为什么网络不能过深<br>Wd的原因和表现<br>Dropout的原理和实现<br>模型的读写<br>GPU计算的特点和历史<br>常用的框架代码：比如参数初始化、损失函数定义、优化器定义</p><h1 id="第8章：卷积"><a href="#第8章：卷积" class="headerlink" title="第8章：卷积"></a>第8章：卷积</h1><p>卷积的思想基础，能够解释局部性和平移不变性<br>对于某个像素的信息，应当只与其附近的像素有关系，超出一定的距离以后则无关<br>对某个区域进行的特征提取所得到的输出并不会由于平移操作而发生变化<br>卷积、池化的代码、计算<br>步幅、填充、多通道<br>    卷积和池化是否可以用相同的步幅、填充、输入通道、输出通道<br>1*1卷积</p><h1 id="第9章：机器视觉初步"><a href="#第9章：机器视觉初步" class="headerlink" title="第9章：机器视觉初步"></a>第9章：机器视觉初步</h1><p>LeNet、AlexNet、Vgg、NiN、GoogLeNet、ResNet<br>LeNet、典型的VGG块、inception块、Res块<br>过拟与DA，常用的DA技术<br>每个网络的名字的意义，历史脉络、网络深了是更容易过拟还是会减轻过拟。</p><h1 id="第10章：循环神经网络"><a href="#第10章：循环神经网络" class="headerlink" title="第10章：循环神经网络"></a>第10章：循环神经网络</h1><p>N元语法、词向量，两种常用的向量训练方法。<br>二阶马尔可夫展开<br>RNN、LSTM、GRU</p><h1 id="论述："><a href="#论述：" class="headerlink" title="论述："></a>论述：</h1><p>计算图、框架的作用<br>深度学习、卷积神经网络：简史、意义、动力<br>深度学习与机器学习的联系与区别<br>机器学习的核心理念<br>三个以上的观点，可以是自己的合理的观点，言之有物，自圆其说，字数达标，每个关键观点可酌情给予3-4分。<br>纯网络、教材内容不会得到高分。<br>如果有雷同答案，将判为抄袭，本题不得分，不区分抄袭与被抄袭者，欢迎你和同学讨论，但不要将你的完成文稿给别人，如果你已经给了别人，建议你自己再写一份。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大二下学期深度学习课程回顾&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="总结复盘" scheme="https://du2279664786.github.io/tags/%E6%80%BB%E7%BB%93%E5%A4%8D%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>机器学习课程回顾</title>
    <link href="https://du2279664786.github.io/2022/07/08/2022-07-08%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/"/>
    <id>https://du2279664786.github.io/2022/07/08/2022-07-08%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E5%9B%9E%E9%A1%BE/</id>
    <published>2022-07-08T14:55:10.000Z</published>
    <updated>2022-10-21T01:33:12.143Z</updated>
    
    <content type="html"><![CDATA[<p>大二上学期机器学习课程回顾</p><span id="more"></span><h1 id="1-基本概念部分"><a href="#1-基本概念部分" class="headerlink" title="1.基本概念部分"></a>1.基本概念部分</h1><p>1、统计学习方法可以概括如下：……..<br>2、什么是有监督学习、无监督学习、半监督学习<br>3、有监督的学习的三要素两过程<br>4、生成式模型和判别式模型是什么意思，常见的代表模型有哪几个<br>5、什么叫过拟合、欠拟合，常用的减轻拟合的方法<br>6、如果clf是一个模拟的对象，则一般clf.train(X, y), clf.fit(X, y), clf.predict(test)是什么意思，执行后的结果或改变是什么<br>7、Precision, Recall, F1, Accuracy, AUC of ROC。上面这几个概念的定义、意义、计算。给定正负例的信号强度，能画出ROC<br>8、训练集、验证集、测试集的作用是什么，S折交叉验证是怎么回事<br>9、什么叫回归，什么叫聚类，什么叫分类</p><h1 id="2-Knn"><a href="#2-Knn" class="headerlink" title="2.Knn"></a>2.Knn</h1><p>中英文名字、算法理念、算法过程、算法伪代码，算法代码实现</p><h1 id="3-Kmeans"><a href="#3-Kmeans" class="headerlink" title="3.Kmeans"></a>3.Kmeans</h1><p>中英文名字、算法理念、算法过程、算法伪代码，算法代码实现</p><h1 id="4-最优化问题"><a href="#4-最优化问题" class="headerlink" title="4.最优化问题"></a>4.最优化问题</h1><p>最优化问题，迭代最优化问题，梯度下降法都是什么意思。<br>梯度下降法的算法理念、算法过程、算法伪代码、停机条件<br>给定函数、当前自变量、学习率，可算出下一次迭代的自变量<br>给定函数，能求出argmin和min</p><h1 id="5-感知机"><a href="#5-感知机" class="headerlink" title="5.感知机"></a>5.感知机</h1><p>算法理念、算法过程、算法伪代码，算法代码实现<br>感知机解的情况和业务意义，感知机的局限，感知机在机器学习中的地位</p><h1 id="6-线性回归"><a href="#6-线性回归" class="headerlink" title="6.线性回归"></a>6.线性回归</h1><p>线性回归的定义，解法，解的情况，广义线性回归<br>会手算简单的线性回归(单变量)</p><h1 id="7-逻辑回归"><a href="#7-逻辑回归" class="headerlink" title="7.逻辑回归"></a>7.逻辑回归</h1><p>线性回归的定义，解法，解的情况<br>Sigmoid函数及求导，求解最大似然估计</p><h1 id="8-朴素贝叶斯"><a href="#8-朴素贝叶斯" class="headerlink" title="8.朴素贝叶斯"></a>8.朴素贝叶斯</h1><p>给定一个小规模数据集，可以手算朴素贝叶斯</p><h1 id="9-决策树"><a href="#9-决策树" class="headerlink" title="9.决策树"></a>9.决策树</h1><p>决策树的基本算法<br>熵、基尼、熵增益、固有值、熵增益比的定义和业务意义<br>ID3、C4.5、Cart算法基本思路和伪代码</p><h1 id="10-提升方法"><a href="#10-提升方法" class="headerlink" title="10.提升方法"></a>10.提升方法</h1><p>Bagging和随机森林<br>能说清楚GBDT的脉络即：<br>adaboost的理念，加法模型，前向加法模型，提升树，回归树对残差的拟合，以及对梯度的拟合。</p><h1 id="11-SVM"><a href="#11-SVM" class="headerlink" title="11.SVM"></a>11.SVM</h1><p>线性可分支持向量机的基本脉络<br>松弛变量、核函数的业务背景和操作方法<br>SMO算法的大致过程</p><h1 id="12-NN"><a href="#12-NN" class="headerlink" title="12.NN"></a>12.NN</h1><p>说清楚神经网络学习和预测的过程<br>了解常见的神经网络，及中英文名称<br>对于多层神经网络，可以计算其待定参数的个数，并能说明BP算法如何更新网络参数</p><h1 id="13-Numpy"><a href="#13-Numpy" class="headerlink" title="13.Numpy"></a>13.Numpy</h1><p>基本的向量化运算，使用numpy常见的方法</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;大二上学期机器学习课程回顾&lt;/p&gt;</summary>
    
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://du2279664786.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="总结复盘" scheme="https://du2279664786.github.io/tags/%E6%80%BB%E7%BB%93%E5%A4%8D%E7%9B%98/"/>
    
  </entry>
  
  <entry>
    <title>Python切换源，快速下载文件</title>
    <link href="https://du2279664786.github.io/2022/06/05/2022-06-05Python%E5%88%87%E6%8D%A2%E6%BA%90%EF%BC%8C%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/"/>
    <id>https://du2279664786.github.io/2022/06/05/2022-06-05Python%E5%88%87%E6%8D%A2%E6%BA%90%EF%BC%8C%E5%BF%AB%E9%80%9F%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6/</id>
    <published>2022-06-05T14:55:10.000Z</published>
    <updated>2022-10-21T01:39:16.932Z</updated>
    
    <content type="html"><![CDATA[<p>快速下载文件</p><span id="more"></span><h1 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h1><p>进入电脑路径’C:\Users\user_name\AppData\Roaming\pip’文件夹下，找到pip.ini文件<br><img src="https://img-blog.csdnimg.cn/d02a90ee68bd40248c2b39ea89154681.png" alt="在这里插入图片描述"><br>若没有此文件则可以创建一个pip文件，后缀为ini，，然后打开文件，将文件内容替换为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[<span class="keyword">global</span>]</span><br><span class="line"></span><br><span class="line">index-url=http://pypi.douban.com/simple</span><br><span class="line"></span><br><span class="line">[install]</span><br><span class="line"></span><br><span class="line">trusted-host=pypi.douban.com</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>保存文件即可<br><img src="https://img-blog.csdnimg.cn/435b149734414ae49fd8cb8cfbe6f124.png" alt="在这里插入图片描述"><br>下载速度惊人<br><img src="https://img-blog.csdnimg.cn/e904985a074142faaab9e22a7e273cc5.png" alt="在这里插入图片描述"><br>如果文章对您有帮助，请点赞+收藏</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;快速下载文件&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>SOFTMAX回归模型</title>
    <link href="https://du2279664786.github.io/2022/05/08/2022-05-08SOFTMAX%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/"/>
    <id>https://du2279664786.github.io/2022/05/08/2022-05-08SOFTMAX%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/</id>
    <published>2022-05-08T14:55:10.000Z</published>
    <updated>2022-10-18T09:14:53.067Z</updated>
    
    <content type="html"><![CDATA[<p>SOFTMAX函数的脉络梳理</p><span id="more"></span><p>2022-05-08SOFTMAX回归模型</p><h2 id="什么是SOFTMAX回归函数"><a href="#什么是SOFTMAX回归函数" class="headerlink" title="什么是SOFTMAX回归函数"></a>什么是SOFTMAX回归函数</h2><p>·softmax回归跟线性回归⼀样将输⼊特征与权᯿做线性叠加<br>·与线性回归的⼀个主要不同在于，softmax回归的输出值个数等于标签⾥的类别数<br>·SOFTMAX是一个单层的神经网络<br>结构图如下：<br><img src="https://img-blog.csdnimg.cn/1eef7cdde9b04e1d866471e374b3a80b.png" alt="在这里插入图片描述"><br>运算过程如下：<br><img src="https://img-blog.csdnimg.cn/4e928316894e4e298b9e71c18598fbd1.png" alt="在这里插入图片描述"><br>即我们通过神经网络预测，然后得到相应的一个分数，此时我们希望我们得到的分数是一个概率：<br><img src="https://img-blog.csdnimg.cn/c16934676c254f119005ddefbb0d5164.png" alt="在这里插入图片描述"><br>此时我们就应该选取一个合适的方案，来将预测的分数来转化为标签的概率，那么最合适的肯定是softmax了，softmax公式：<br><img src="https://img-blog.csdnimg.cn/df4b36ec76f44413b348ad2917c4d399.png" alt="在这里插入图片描述"><br>即将得到的分数都进行exp，然后求每个标签占比(概率)，最终我们得到的是预测概率最大的标签：<br><img src="https://img-blog.csdnimg.cn/656cab1ab05b4020ac06c14f0ea28951.png" alt="在这里插入图片描述"><br>很显然，我们只关注预测概率最大的标签（单标签预测）<br>总结了一下特性：<br>    ·结果都为正数：即将得分为负数的进行转化<br>    ·所有求和为1：所有概率相加等于1<br>    ·平移不变性：所有得分平移得到的结果不受影响<br>    ·最大–&gt;最大：预测得分最大的概率也大</p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>在这里我们使用的是交叉熵损失函数（Cross Entropy）<br><img src="https://img-blog.csdnimg.cn/eded5e651bfd471dada59239dc2b6c9d.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/cf23b1b754ab4f3eae57f49f5f12b0f1.png" alt="其中带下标的是向量 中⾮0即1的元素"></p><h2 id="实现代码："><a href="#实现代码：" class="headerlink" title="实现代码："></a>实现代码：</h2><p>·导入包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh_pytorch <span class="keyword">as</span> d2l</span><br></pre></td></tr></table></figure><p>若出现没有‘d2lzh_pytorch’这个包，<a href="https://blog.csdn.net/weixin_51756104/article/details/124626354?spm=1001.2014.3001.5501">点击此处离线安装</a><br>·导入数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure><p>·定义网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">784</span></span><br><span class="line">num_outputs = <span class="number">10</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(num_inputs, num_outputs)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): <span class="comment"># x shape: (batch, 1, 28, 28)</span></span><br><span class="line">        y = self.linear(x.view(x.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"> </span><br><span class="line">net = LinearNet(num_inputs,num_outputs)</span><br></pre></td></tr></table></figure><p>·初始化模型参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init.normal_(net.linear.weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net.linear.bias, val=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>·定义损失函数和优化器：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>·训练：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs,batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;SOFTMAX函数的脉络梳理&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="SOFTMAX" scheme="https://du2279664786.github.io/tags/SOFTMAX/"/>
    
  </entry>
  
  <entry>
    <title>d2lzh_pytorch包离线安装</title>
    <link href="https://du2279664786.github.io/2022/05/07/2022-05-07d2lzh_pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/"/>
    <id>https://du2279664786.github.io/2022/05/07/2022-05-07d2lzh_pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/</id>
    <published>2022-05-07T14:55:10.000Z</published>
    <updated>2022-10-18T09:12:51.207Z</updated>
    
    <content type="html"><![CDATA[<p>线上安装经常出错，所以可以选择离线安装</p><span id="more"></span><p>在导入d2lzh_pytorch包时，一般会报错：<br><img src="https://img-blog.csdnimg.cn/56f5ff1ee876479e9707f06a234a04dd.png" alt="在这里插入图片描述"><br>我们可以离线下载包：<br>链接：<a href="https://pan.baidu.com/share/init?surl=iXyFqY8uM5PGhrthL_-9xQ#list/path=/">点击此处</a>，提取码：1314<br>下载后：进入到我们要使用的环境，按照如下位置安放包<br><img src="https://img-blog.csdnimg.cn/79fcfef8196a43c8bb122b712b2dce9d.png" alt="在这里插入图片描述"><br>最后就可以导入了：<br><img src="https://img-blog.csdnimg.cn/4c3b27ce6aa9488494f97a336101d894.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;线上安装经常出错，所以可以选择离线安装&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="d2lzh_pytorch包离线安装" scheme="https://du2279664786.github.io/tags/d2lzh-pytorch%E5%8C%85%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>线性回归的简洁实现</title>
    <link href="https://du2279664786.github.io/2022/04/28/2022-04-28%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0/"/>
    <id>https://du2279664786.github.io/2022/04/28/2022-04-28%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0/</id>
    <published>2022-04-28T14:55:10.000Z</published>
    <updated>2022-10-18T09:11:02.360Z</updated>
    
    <content type="html"><![CDATA[<p>创建单层神经网络</p><span id="more"></span><p>线性回归详细实现，<a href="https://blog.csdn.net/weixin_51756104/article/details/124334225">请点击此处</a></p><h3 id="导入包："><a href="#导入包：" class="headerlink" title="导入包："></a>导入包：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data    <span class="comment"># 数据读取</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init     <span class="comment"># 初始化</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim   <span class="comment"># 优化器</span></span><br></pre></td></tr></table></figure><h3 id="数据集的生成"><a href="#数据集的生成" class="headerlink" title="数据集的生成"></a>数据集的生成</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_inputs = <span class="number">2</span>     <span class="comment"># 2个维度</span></span><br><span class="line">num_examples = <span class="number">1000</span>     <span class="comment"># 1000调数据</span></span><br><span class="line"><span class="comment"># 标准的参数</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">features = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, (num_examples,num_inputs)), dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">label = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] +true_b</span><br><span class="line">label += torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>,size=label.size()), dtype=torch.<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure><h3 id="数据的读取"><a href="#数据的读取" class="headerlink" title="数据的读取"></a>数据的读取</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">30</span></span><br><span class="line">dataset = Data.TensorDataset(features,label)</span><br><span class="line">data_it = Data.DataLoader(dataset,batch_size,shuffle =<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><p>定义模型有多种方法：<br>方法一：继承nn.Module</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,n_feature</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearNet, self).__init__()</span><br><span class="line">        self.linear = nn.Linear(n_feature,<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        y = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">net = LinearNet(num_inputs)</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br></pre></td></tr></table></figure><p>方法二：nn.Sequential</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">    nn.Linear(num_inputs, <span class="number">1</span>)</span><br><span class="line"> )</span><br></pre></td></tr></table></figure><p>方法三：nn.Sequential()+add_module</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential()</span><br><span class="line">net.add_module(<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>方法四：导入OrderedDict</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br><span class="line">net = nn.Sequential(OrderedDict([</span><br><span class="line">    (<span class="string">&#x27;linear&#x27;</span>, nn.Linear(num_inputs, <span class="number">1</span>))]))</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数"><a href="#初始化模型参数" class="headerlink" title="初始化模型参数"></a>初始化模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init.normal_(net[<span class="number">0</span>].weight, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br><span class="line">init.constant_(net[<span class="number">0</span>].bias,val=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="MSE损失函数"><a href="#MSE损失函数" class="headerlink" title="MSE损失函数"></a>MSE损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br></pre></td></tr></table></figure><h3 id="定义优化算法"><a href="#定义优化算法" class="headerlink" title="定义优化算法"></a>定义优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.03</span>)</span><br></pre></td></tr></table></figure><h3 id="模型的优化"><a href="#模型的优化" class="headerlink" title="模型的优化"></a>模型的优化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_epochs + <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_it:</span><br><span class="line">        output = net(X)</span><br><span class="line">        l = loss(output, y.view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        optimizer.zero_grad() <span class="comment"># 梯度清零，等价于net.zero_grad()</span></span><br><span class="line">        l.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">    <span class="built_in">print</span>(epoch, l.item())</span><br></pre></td></tr></table></figure><p>最后输出epoch和loss：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span> <span class="number">0.3572668433189392</span></span><br><span class="line"><span class="number">2</span> <span class="number">0.005662666633725166</span></span><br><span class="line"><span class="number">3</span> <span class="number">0.00011592111695790663</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;创建单层神经网络&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>史上最详细的Pytorch+CUDA+CUDNN的安装(GPU版)</title>
    <link href="https://du2279664786.github.io/2022/04/27/2022-04-27%E5%8F%B2%E4%B8%8A%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84Pytorch+CUDA+CUDNN%E7%9A%84%E5%AE%89%E8%A3%85(GPU%E7%89%88)/"/>
    <id>https://du2279664786.github.io/2022/04/27/2022-04-27%E5%8F%B2%E4%B8%8A%E6%9C%80%E8%AF%A6%E7%BB%86%E7%9A%84Pytorch+CUDA+CUDNN%E7%9A%84%E5%AE%89%E8%A3%85(GPU%E7%89%88)/</id>
    <published>2022-04-27T14:55:10.000Z</published>
    <updated>2022-10-18T09:09:30.930Z</updated>
    
    <content type="html"><![CDATA[<p>炒鸡详细的pytorch GPU安装版本，从0搭建！</p><span id="more"></span><p>CPU版本的教程<a href="https://blog.csdn.net/weixin_51756104/article/details/124222546">请点击此处查看</a></p><h3 id="首先看一下自己的驱动："><a href="#首先看一下自己的驱动：" class="headerlink" title="首先看一下自己的驱动："></a>首先看一下自己的驱动：</h3><p>·如果驱动不支持CUDA11的话就要先更新驱动<br>·打开命令行win+r，输入cmd，在命令行输入：nvidia-smi   查看信息<br><img src="https://img-blog.csdnimg.cn/f1d12b1f1cb24e11a5e70143ef5c1195.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>这里可以看到我的驱动是512.2，根据下图可以看到驱动只要大于451.22就支持CUDA11，,pytorch最新本已经不支持CUDA10,如果驱动版本低于451,可以升级驱动，<a href="https://www.nvidia.com/en-us/geforce/drivers/">点击此处下载驱动</a>，下面是CUDA和显卡驱动对应的版本：<br><img src="https://img-blog.csdnimg.cn/ea404a8d657e418eaa0ab409ba8386f7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="安装Pytorch"><a href="#安装Pytorch" class="headerlink" title="安装Pytorch"></a>安装Pytorch</h3><p>此处使用的是本地安装(因为pip安装和conda安装本人都没有成功，可能是网络问题),<a href="https://download.pytorch.org/whl/torch_stable.html">点击此处</a>进行Pytorch的下载：可以看到我的CUDA是11.6版本：<br><img src="https://img-blog.csdnimg.cn/06c6e268cdfa4a2da9224cb2ba7f9b48.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>我们进入下载pytorch的网站，发现还没有CUDA11.6版本，我们可以下载CUDA11.5版本，<br>cu115代表CUDA11.5版本，cp38代表python的版本，选择合适的进行下载，我下载的是CUDA11.5版本，Python版本3.8,所以我们选择：<br><img src="https://img-blog.csdnimg.cn/ac64598e145c4b3b943c9516ba7df188.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>同理我们再选择torchvison和torchaudio的下载，下载完成后进行本地安装：使用pip install+安装包的路径安装，我的在D盘：<br><img src="https://img-blog.csdnimg.cn/5e018a995bc3415faf2ccdb8be245aa7.png" alt="："><br>此时我们就可以检测一下是否安装成功：<br><img src="https://img-blog.csdnimg.cn/2c27cd5cb5084ae3a502032a12bd7316.png" alt="在这里插入图片描述">可以看到已经成功了！！！</p><h3 id="CUDA安装"><a href="#CUDA安装" class="headerlink" title="CUDA安装"></a>CUDA安装</h3><p><a href="https://developer.nvidia.com/cuda-toolkit-archive">点击此处</a>，进入下载，选择合适自己的版本：<br><img src="https://img-blog.csdnimg.cn/91fb78d977294a21ae0068b7f1eb8234.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>选择好信息开始下载：<br><img src="https://img-blog.csdnimg.cn/5b0e8251a48640f9b1490f335f916d84.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>点击安装包，一路默认安装就行</p><h3 id="CUDNN安装"><a href="#CUDNN安装" class="headerlink" title="CUDNN安装"></a>CUDNN安装</h3><p><a href="https://developer.nvidia.com/rdp/cudnn-download">点击此处</a>，自行注册账号</p><p><img src="https://img-blog.csdnimg.cn/fcc85c8aeb184e60bf7f1899a3d3e681.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后点击下载：需要填写调查问卷，点击提交<br><img src="https://img-blog.csdnimg.cn/55be6915942542f2b7e61f587a44e4fa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>找到相应的版本：<br><img src="https://img-blog.csdnimg.cn/84f7be9ba3e74a5bb4566b39b57ea958.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>这里我的是windows CUDA11.6，所以我下载windows版本的压缩包<br><img src="https://img-blog.csdnimg.cn/cb5883133ef64418adefffb76c9aebfa.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>下载完进行解压：<br><img src="https://img-blog.csdnimg.cn/69864e29e15c46afa22218a9edb63b6e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后找到我们CUDA11.6的位置，默认安装的在：C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA<br><img src="https://img-blog.csdnimg.cn/70e4ce0c5ba84be48fd10eecd82524af.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>然后我们找到刚刚解压的cudnn文件夹<br><img src="https://img-blog.csdnimg.cn/87f93e9696584e929c591d283fc883c2.png" alt="在这里插入图片描述"><br>将bin，include，lib文件夹下里面的‘文件’分别复制到CUDA相应的文件夹里面（复制的是里面的的文件，不是文件夹）：<br><img src="https://img-blog.csdnimg.cn/69d6624d180c40d999de7c7aa76bc6dc.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="查看是否安装成功"><a href="#查看是否安装成功" class="headerlink" title="查看是否安装成功"></a>查看是否安装成功</h3><p>·查看CUDA，在命令行输入：nvcc -V，出现以下代表成功：<br><img src="https://img-blog.csdnimg.cn/2bf5e7e797224b03aa842149bab8c4cf.png" alt="在这里插入图片描述"><br>·查看cudnn，我们在命令行进入安装cuda的目录，我的是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11<span class="number">.6</span>\extras\demo_suite</span><br></pre></td></tr></table></figure><p>然后在命令行进入文件夹：<br><img src="https://img-blog.csdnimg.cn/3be6ef8c961743acad34f275013dcd1d.png" alt="在这里插入图片描述"><br>输入：bandwidthTest.exe<br><img src="https://img-blog.csdnimg.cn/c5722a063f564e788682cd13491f759a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>输入：deviceQuery.exe<br><img src="https://img-blog.csdnimg.cn/cc1b0f43a5944f03b12627fd3738dc74.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5rGfIOS4nA==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>表示安装成功！！！<br>喜欢文章可以点赞收藏，欢迎关注，如有错误请指正！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;炒鸡详细的pytorch GPU安装版本，从0搭建！&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://du2279664786.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="pytorch" scheme="https://du2279664786.github.io/tags/pytorch/"/>
    
    <category term="深度学习" scheme="https://du2279664786.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="CUDA" scheme="https://du2279664786.github.io/tags/CUDA/"/>
    
  </entry>
  
</feed>
